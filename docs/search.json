[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "",
    "text": "Module Overview\nGeospatial analytics and dashboards are in very high remand among policymakers, NGOs, IGOs, and the private sector. Deploying these systems often requires handling data that exceeds the computational and storage capabilities of personal machines. This module will teach students how to harness and critically interrogate large quantities of geospatial data using cloud computing services, and how to design and build an interactive online application that communicates geospatial insights to wider audiences.\nIn line with this objective, the module is divided into two sections. In the first, database concepts and techniques are introduced, providing the students with the skills required to manipulate and derive meaning from organised datasets. SQL syntax will be taught in depth at this stage, with a strong emphasis on practical application. This will allow students to learn state of the art methods for handling large vector datasets.\nThe second section of the course focuses on the handling of large raster datasets. As geospatial datasets—particularly satellite imagery collections—increase in size, researchers are increasingly relying on cloud computing platforms such as Google Earth Engine (GEE) to analyze vast quantities of data. Despite the fact that it was only released in 2015, the number of geospatial journal articles using Google Earth Engine has outpaced every other major geospatial analysis software, including ArcGIS, Python, and R in just five years. Weeks 6-9 will be co-taught with CASA0023 Remote Sensing.\nThe module therefore spans a full, cloud-based geospatial workflow: from importing and analysing geospatial data, to building and presenting interactive data visualisations. Students will gain proficiency in working with and interrogating large spatial data sets while working towards an interactive group project that will develop their portfolio."
  },
  {
    "objectID": "index.html#what-is-sql",
    "href": "index.html#what-is-sql",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "What is SQL?",
    "text": "What is SQL?\nSQL (Structured Query Language) is a programming language used to communicate with databases. It is the standard language for relational database management systems. SQL statements are used to perform tasks such as update data on a database, or retrieve data from a database. Some common relational database management systems that use SQL are: Oracle, Sybase, Microsoft SQL Server, Access, Ingres, etc. Although most database systems use SQL, most of them also have their own additional proprietary extensions that are usually only used on their system. However, the standard SQL commands such as “Select”, “Insert”, “Update”, “Delete”, “Create”, and “Drop” can be used to accomplish almost everything that one needs to do with a database.\nThe first five weeks of this module will focus on working with large vector datasets. We will use SQL to query and manipulate data stored in a PostgreSQL database. PostgreSQL is a free and open-source relational database management system emphasizing extensibility and SQL compliance. It is the most advanced open-source database system widely used for GIS applications. We will also work with DuckDB, a new, open-source, in-process SQL OLAP database management system. DuckDB is designed to be used as an embedded database library, providing C/C++, Python, R, Java, and Go bindings. It has a built-in SQL engine with support for transactions, a powerful query optimizer, and a columnar storage engine."
  },
  {
    "objectID": "index.html#what-is-google-earth-engine",
    "href": "index.html#what-is-google-earth-engine",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "What is Google Earth Engine?",
    "text": "What is Google Earth Engine?\nAs geospatial datasets—particularly satellite imagery collections—increase in size, researchers are increasingly relying on cloud computing platforms such as Google Earth Engine (GEE) to analyze vast quantities of data.\nGEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geospatial analysis software platforms:\n\nDespite only being released in 2015, the number of geospatial journal articles using Google Earth Engine (shown in red above) has outpaced every other major geospatial analysis software, including ArcGIS, Python, and R in just five years. GEE applications have been developed and used to present interactive geospatial data visualizations by NGOs, Universities, the United Nations, and the European Commission. By storing and running computations on google servers, GEE is far more accessible to those who don’t have significant local computational resources; all you need is an internet connection."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nSQL\n\nFive initial weeks exporing spatial database systems including PostgreSQL and DuckDB.\n\nIntroduction\nSQL\nPostGIS I\nPostGIS II\nDatabase Quiz\n\n\nGoogle Earth Engine\n\nFive weeks on Google Earth Engine, a cloud-based platform for geospatial analysis.\n\nIntroduction to GEE\nClassification I\nClassification II\nSynthetic Aperture Radar\nUser Interface Design\nBonus: Object Detection\n\n\n\nGEE Textbook * Recently, a team of over 100 scientists came together to write a book called “Cloud-Based Remote Sensing with Google Earth Engine: Fundamentals and Applications”. It’s a great resource for learning about remote sensing and Earth Engine. The material in this section is a subset of the book, edited to fit the scope of this guide. If you’re interested in learning more, check out the full book. * Getting Started * Interpreting Images * Image Series * Vectors and Tables"
  },
  {
    "objectID": "W1_intro.html",
    "href": "W1_intro.html",
    "title": "1  Installation",
    "section": "",
    "text": "2 Creating a Spatial Database\nSupported by a wide variety of libraries and applications, PostGIS provides many options for loading data.\nWe will first load our working data from a database backup file, then review some standard ways of loading different GIS data formats using common tools."
  },
  {
    "objectID": "W1_intro.html#postgresql-for-microsoft-windows",
    "href": "W1_intro.html#postgresql-for-microsoft-windows",
    "title": "1  Installation",
    "section": "1.1 PostgreSQL for Microsoft Windows",
    "text": "1.1 PostgreSQL for Microsoft Windows\nFor a Windows install:\n\nGo to the Windows PostgreSQL download page.\nSelect the latest version of PostgreSQL and save the installer to disk.\nRun the installer and accept the defaults.\nFind and run the \"StackBuilder\" program that was installed with the database.\nSelect the \"Spatial Extensions\" section and choose latest \"PostGIS ..Bundle\" option.\n\nAccept the defaults and install."
  },
  {
    "objectID": "W1_intro.html#postgresql-for-apple-macos",
    "href": "W1_intro.html#postgresql-for-apple-macos",
    "title": "1  Installation",
    "section": "1.2 PostgreSQL for Apple MacOS",
    "text": "1.2 PostgreSQL for Apple MacOS\nFor a MacOS install:\n\nGo to the Postgres.app site, and download the latest release.\nOpen the disk image, and drag the Postgres icon into the Applications folder.\n\nIn the Applications folder, double-click the Postgres icon to start the server.\nClick the Initialize button to create a new blank database instance.\n{.inline, .border .inline, .border}\nIn the Applications folder, go to the Utilities folder and open Terminal.\nAdd the command-line utilities to your PATH for convenience.\n\n\nsudo mkdir -p /etc/paths.d\necho /Applications/Postgres.app/Contents/Versions/latest/bin | sudo tee /etc/paths.d/postgresapp"
  },
  {
    "objectID": "W1_intro.html#pgadmin-for-windows-and-macos",
    "href": "W1_intro.html#pgadmin-for-windows-and-macos",
    "title": "1  Installation",
    "section": "1.3 PgAdmin for Windows and MacOS",
    "text": "1.3 PgAdmin for Windows and MacOS\nPgAdmin is available for multiple platforms, at https://www.pgadmin.org/download/.\n\nDownload and install the latest version for your platform.\nStart PgAdmin!"
  },
  {
    "objectID": "W1_intro.html#pgadmin",
    "href": "W1_intro.html#pgadmin",
    "title": "1  Installation",
    "section": "2.1 PgAdmin",
    "text": "2.1 PgAdmin\nPostgreSQL has a number of administrative front-ends. The primary one is psql, a command-line tool for entering SQL queries. Another popular PostgreSQL front-end is the free and open source graphical tool pgAdmin. All queries done in pgAdmin can also be done on the command line with psql. pgAdmin also includes a geometry viewer you can use to spatial view PostGIS queries.\n\nFind pgAdmin and start it up.\n\nIf this is the first time you have run pgAdmin, you probably don't have any servers configured. Right click the Servers item in the Browser panel.\nWe'll name our server PostGIS. In the Connection tab, enter the Host name/address. If you're working with a local PostgreSQL install, you'll be able to use localhost. If you're using a cloud service, you should be able to retrieve the host name from your account.\nLeave Port set at 5432, and both Maintenance database and Username as postgres. The Password should be what you specified with a local install or with your cloud service."
  },
  {
    "objectID": "W1_intro.html#creating-a-database",
    "href": "W1_intro.html#creating-a-database",
    "title": "1  Installation",
    "section": "2.2 Creating a Database",
    "text": "2.2 Creating a Database\n\nOpen the Databases tree item and have a look at the available databases. The postgres database is the user database for the default postgres user and is not too interesting to us.\nRight-click on the Databases item and select New Database.\n\nFill in the Create Database form as shown below and click OK.\n\n\n\nName\nnyc\n\n\nOwner\npostgres\n\n\n\n\nSelect the new nyc database and open it up to display the tree of objects. You'll see the public schema.\n\nClick on the SQL query button indicated below (or go to Tools &gt; Query Tool).\n\nEnter the following query into the query text field to load the PostGIS spatial extension:\nCREATE EXTENSION postgis;\nClick the Play button in the toolbar (or press F5) to \"Execute the query.\"\nNow confirm that PostGIS is installed by running a PostGIS function:\nSELECT postgis_full_version();\n\nYou have successfully created a PostGIS spatial database!!"
  },
  {
    "objectID": "W1_intro.html#function-list",
    "href": "W1_intro.html#function-list",
    "title": "1  Installation",
    "section": "2.3 Function List",
    "text": "2.3 Function List\nPostGIS_Full_Version: Reports full PostGIS version and build configuration info."
  },
  {
    "objectID": "W1_intro.html#loading-the-backup-file",
    "href": "W1_intro.html#loading-the-backup-file",
    "title": "1  Installation",
    "section": "3.1 Loading the Backup File",
    "text": "3.1 Loading the Backup File\n\nIn the PgAdmin browser, right-click on the nyc database icon, and then select the Restore... option.\n{.inline, .border .inline, .border}\nBrowse to the location of your workshop data data directory (available in the workshop data bundle), and select the nyc_data.backup file.\n{.inline, .border .inline, .border}\nClick on the Restore options tab, scroll down to the Do not save section and toggle Owner to Yes.\n{.inline, .border .inline, .border}\nClick the Restore button. The database restore should run to completion without errors.\n{.inline, .border .inline, .border}\nAfter the load is complete, right click the nyc database, and select the Refresh option to update the client information about what tables exist in the database.\n{.inline, .border .inline, .border}\n\n\n\nNote\n\nIf you want to practice loading data from the native spatial formats, instead of using the PostgreSQL db backup files just covered, the next couple of sections will guide you thru loading using various command-line tools and QGIS DbManager. Note you can skip these sections, if you have already loaded the data with pgAdmin."
  },
  {
    "objectID": "W1_intro.html#shapefiles-whats-that",
    "href": "W1_intro.html#shapefiles-whats-that",
    "title": "1  Installation",
    "section": "3.2 Shapefiles? What's that?",
    "text": "3.2 Shapefiles? What's that?\nYou may be asking yourself -- \"What's this shapefile thing?\" A \"shapefile\" commonly refers to a collection of files with .shp, .shx, .dbf, and other extensions on a common prefix name (e.g., nyc_census_blocks). The actual shapefile relates specifically to files with the .shp extension. However, the .shp file alone is incomplete for distribution without the required supporting files.\nMandatory files:\n\n.shp—shape format; the feature geometry itself\n.shx—shape index format; a positional index of the feature geometry\n.dbf—attribute format; columnar attributes for each shape, in dBase III\n\nOptional files include:\n\n.prj—projection format; the coordinate system and projection information, a plain text file describing the projection using well-known text format\n\nThe shp2pgsql utility makes shape data usable in PostGIS by converting it from binary data into a series of SQL commands that are then run in the database to load the data."
  },
  {
    "objectID": "W1_intro.html#loading-with-shp2pgsql",
    "href": "W1_intro.html#loading-with-shp2pgsql",
    "title": "1  Installation",
    "section": "3.3 Loading with shp2pgsql",
    "text": "3.3 Loading with shp2pgsql\nThe shp2pgsql converts Shape files into SQL. It is a conversion utility that is part of the PostGIS code base and ships with PostGIS packages. If you installed PostgreSQL locally on your computer, you may find that shp2pgsql has been installed along with it, and it is available in the executable directory of your installation.\nUnlike ogr2ogr, shp2pgsql does not connect directly to the destination database, it just emits the SQL equivalent to the input shape file. It is up to the user to pass the SQL to the database, either with a \"pipe\" or by saving the SQL to file and then loading it.\nHere is an example invocation, loading the same data as before:\nexport PGPASSWORD=mydatabasepassword\n\nshp2pgsql \\\n  -D \\\n  -I \\\n  -s 26918 \\\n  nyc_census_blocks_2000.shp \\\n  nyc_census_blocks_2000 \\\n  | psql dbname=nyc user=postgres host=localhost\nHere is a line-by-line explanation of the command.\nshp2pgsql \\\nThe executable program! It reads the source data file, and emits SQL which can be directed to a file or piped to psql to load directly into the database.\n-D \\\nThe D flag tells the program to generate \"dump format\" which is much faster to load than the default \"insert format\".\n-I \\\nThe I flag tells the program to create a spatial index on the table after loading is complete.\n-s 26918 \\\nThe s flag tells the program what the \"spatial reference identifier (SRID)\" of the data is. The source data for this workshop is all in \"UTM 18\", for which the SRID is 26918 (see below).\nnyc_census_blocks_2000.shp \\\nThe source shape file to read.\nnyc_census_blocks_2000 \\\nThe table name to use when creating the destination table.\n| psql dbname=nyc user=postgres host=localhost\nThe utility program is generating a stream of SQL. The \"|\" operator takes that stream and uses it as input to the psql database terminal program. The arguments to psql are just the connection string for the destination database."
  },
  {
    "objectID": "W1_intro.html#srid-26918-whats-with-that",
    "href": "W1_intro.html#srid-26918-whats-with-that",
    "title": "1  Installation",
    "section": "3.4 SRID 26918? What's with that?",
    "text": "3.4 SRID 26918? What's with that?\nMost of the import process is self-explanatory, but even experienced GIS professionals can trip over an SRID.\nAn \"SRID\" stands for \"Spatial Reference IDentifier.\" It defines all the parameters of our data's geographic coordinate system and projection. An SRID is convenient because it packs all the information about a map projection (which can be quite complex) into a single number.\nYou can see the definition of our workshop map projection by looking it up either in an online database,\n\nhttps://epsg.io/26918\n\nor directly inside PostGIS with a query to the spatial_ref_sys table.\nSELECT srtext FROM spatial_ref_sys WHERE srid = 26918;\n\n\nNote\n\nThe PostGIS spatial_ref_sys table is an OGC-standard table that defines all the spatial reference systems known to the database. The data shipped with PostGIS, lists over 3000 known spatial reference systems and details needed to transform/re-project between them.\n\nIn both cases, you see a textual representation of the 26918 spatial reference system (pretty-printed here for clarity):\nPROJCS[\"NAD83 / UTM zone 18N\",\n  GEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n      SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],\n      AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4269\"]],\n  UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],\n  PROJECTION[\"Transverse_Mercator\"],\n  PARAMETER[\"latitude_of_origin\",0],\n  PARAMETER[\"central_meridian\",-75],\n  PARAMETER[\"scale_factor\",0.9996],\n  PARAMETER[\"false_easting\",500000],\n  PARAMETER[\"false_northing\",0],\n  AUTHORITY[\"EPSG\",\"26918\"],\n  AXIS[\"Easting\",EAST],\n  AXIS[\"Northing\",NORTH]]\nIf you open up the nyc_neighborhoods.prj file from the data directory, you'll see the same projection definition.\nData you receive from local agencies—such as New York City—will usually be in a local projection noted by \"state plane\" or \"UTM\". Our projection is \"Universal Transverse Mercator (UTM) Zone 18 North\" or EPSG:26918."
  },
  {
    "objectID": "W1_intro.html#things-to-try-view-data-using-qgis",
    "href": "W1_intro.html#things-to-try-view-data-using-qgis",
    "title": "1  Installation",
    "section": "3.5 Things to Try: View data using QGIS",
    "text": "3.5 Things to Try: View data using QGIS\nQGIS, is a desktop GIS viewer/editor for quickly looking at data. You can view a number of data formats including flat shapefiles and a PostGIS database. Its graphical interface allows for easy exploration of your data, as well as simple testing and fast styling.\nTry using this software to connect your PostGIS database. The application can be downloaded from https://qgis.org\nYou'll first want to create a connection to a PostGIS database using menu Layer-&gt;Add Layer-&gt;PostGIS Layers-&gt;New and then filling in the prompts. Once you have a connection, you can add Layers by clicking connect and selecting a table to display."
  },
  {
    "objectID": "W1_intro.html#loading-data-using-qgis-dbmanager",
    "href": "W1_intro.html#loading-data-using-qgis-dbmanager",
    "title": "1  Installation",
    "section": "3.6 Loading data using QGIS DbManager",
    "text": "3.6 Loading data using QGIS DbManager\nQGIS comes with a tool called DbManager that allows you to connect to various different kinds of databases, including a PostGIS enabled one. After you have a PostGIS Database connection configured, go to Database-&gt;DbManager and expand to your database as shown below:\n{.inline, .border .inline, .border}\nFrom there you can use the Import Layer/File menu option to load numerous different spatial formats. In addition to being able to load data from many spatial formats and export data to many formats, you can also add ad-hoc queries to the canvas or define views in your database, using the highlighted wrench icon.\nThis section is based on the PostGIS Intro Workshop, sections 3, 4, 5,and 7"
  },
  {
    "objectID": "W2_SQL.html",
    "href": "W2_SQL.html",
    "title": "2  Introduction to SQL",
    "section": "",
    "text": "3 Getting Started\nThis week we’ll be learning how to use SQL to query data from a database. To get started, work your way through the following two notebooks:\nClicking the link will take you to Google Colab, where you can run the notebook in your browser. You can also download the notebook and run it locally if you prefer.\nOnce you’ve completed these you can test your knowledge by answering the questions in the lab below. You can also access it on Google Colab here.\nDatasets:\nThe following datasets are used in this lab. You don’t need to download them manually, they can be accessed directly from the notebook."
  },
  {
    "objectID": "W2_SQL.html#introduction",
    "href": "W2_SQL.html#introduction",
    "title": "2  Introduction to SQL",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThis notebook shows how to import data into a DuckDB database. It uses the duckdb Python package to connect to a DuckDB database and import data from various formats, including CSV, JSON, DataFrame, parquet, GeoJSON, Shapefile, GeoParquet, and more."
  },
  {
    "objectID": "W2_SQL.html#datasets",
    "href": "W2_SQL.html#datasets",
    "title": "2  Introduction to SQL",
    "section": "3.2 Datasets",
    "text": "3.2 Datasets\nThe following datasets are used in this notebook. You don’t need to download them, they can be accessed directly from the notebook.\n\ncities.csv\ncountries.csv"
  },
  {
    "objectID": "W2_SQL.html#installation",
    "href": "W2_SQL.html#installation",
    "title": "2  Introduction to SQL",
    "section": "3.3 Installation",
    "text": "3.3 Installation\nUncomment the following cell to install the required packages if needed.\npfikbxuesly ipython3 # %pip install duckdb leafmap"
  },
  {
    "objectID": "W2_SQL.html#library-import",
    "href": "W2_SQL.html#library-import",
    "title": "2  Introduction to SQL",
    "section": "3.4 Library Import",
    "text": "3.4 Library Import\npfikbxuesly ipython3 import duckdb import leafmap import pandas as pd"
  },
  {
    "objectID": "W2_SQL.html#installing-extensions",
    "href": "W2_SQL.html#installing-extensions",
    "title": "2  Introduction to SQL",
    "section": "3.5 Installing Extensions",
    "text": "3.5 Installing Extensions\nDuckDB’s Python API provides functions for installing and loading extensions, which perform the equivalent operations to running the INSTALL and LOAD SQL commands, respectively. An example that installs and loads the httpfs extension looks like follows:\npfikbxuesly ipython3 con = duckdb.connect()\npfikbxuesly ipython3 con.install_extension(\"httpfs\") con.load_extension(\"httpfs\")\npfikbxuesly ipython3 con.install_extension(\"spatial\") con.load_extension(\"spatial\")"
  },
  {
    "objectID": "W2_SQL.html#downloading-sample-data",
    "href": "W2_SQL.html#downloading-sample-data",
    "title": "2  Introduction to SQL",
    "section": "3.6 Downloading Sample Data",
    "text": "3.6 Downloading Sample Data\npfikbxuesly ipython3 url = \"https://open.gishub.org/data/duckdb/cities.zip\" leafmap.download_file(url, unzip=True)"
  },
  {
    "objectID": "W2_SQL.html#csv-files",
    "href": "W2_SQL.html#csv-files",
    "title": "2  Introduction to SQL",
    "section": "3.7 CSV Files",
    "text": "3.7 CSV Files\nCSV files can be read using the read_csv function, called either from within Python or directly from within SQL. By default, the read_csv function attempts to auto-detect the CSV settings by sampling from the provided file.\npfikbxuesly ipython3 # read from a file using fully auto-detected settings con.read_csv('cities.csv')\npfikbxuesly ipython3 # specify options on how the CSV is formatted internally con.read_csv('cities.csv', header=True, sep=',')\npfikbxuesly ipython3 # use the (experimental) parallel CSV reader con.read_csv('cities.csv', parallel=True)\npfikbxuesly ipython3 # directly read a CSV file from within SQL con.sql(\"SELECT * FROM 'cities.csv'\")\npfikbxuesly ipython3 # call read_csv from within SQL con.sql(\"SELECT * FROM read_csv_auto('cities.csv')\")"
  },
  {
    "objectID": "W2_SQL.html#json-files",
    "href": "W2_SQL.html#json-files",
    "title": "2  Introduction to SQL",
    "section": "3.8 JSON Files",
    "text": "3.8 JSON Files\nJSON files can be read using the read_json function, called either from within Python or directly from within SQL. By default, the read_json function will automatically detect if a file contains newline-delimited JSON or regular JSON, and will detect the schema of the objects stored within the JSON file.\npfikbxuesly ipython3 # read from a single JSON file con.read_json('cities.json')\npfikbxuesly ipython3 # directly read a JSON file from within SQL con.sql(\"SELECT * FROM 'cities.json'\")\npfikbxuesly ipython3 # call read_json from within SQL con.sql(\"SELECT * FROM read_json_auto('cities.json')\")"
  },
  {
    "objectID": "W2_SQL.html#dataframes",
    "href": "W2_SQL.html#dataframes",
    "title": "2  Introduction to SQL",
    "section": "3.9 DataFrames",
    "text": "3.9 DataFrames\nDuckDB is automatically able to query a Pandas DataFrame.\npfikbxuesly ipython3 df = pd.read_csv('cities.csv') df\npfikbxuesly ipython3 con.sql('SELECT * FROM df').fetchall()"
  },
  {
    "objectID": "W2_SQL.html#parquet-files",
    "href": "W2_SQL.html#parquet-files",
    "title": "2  Introduction to SQL",
    "section": "3.10 Parquet Files",
    "text": "3.10 Parquet Files\nParquet files can be read using the read_parquet function, called either from within Python or directly from within SQL.\npfikbxuesly ipython3 # read from a single Parquet file con.read_parquet('cities.parquet')\npfikbxuesly ipython3 # directly read a Parquet file from within SQL con.sql(\"SELECT * FROM 'cities.parquet'\")\npfikbxuesly ipython3 # call read_parquet from within SQL con.sql(\"SELECT * FROM read_parquet('cities.parquet')\")"
  },
  {
    "objectID": "W2_SQL.html#geojson-files",
    "href": "W2_SQL.html#geojson-files",
    "title": "2  Introduction to SQL",
    "section": "3.11 GeoJSON Files",
    "text": "3.11 GeoJSON Files\npfikbxuesly ipython3 con.sql('SELECT * FROM ST_Drivers()')\npfikbxuesly ipython3 con.sql(\"SELECT * FROM ST_Read('cities.geojson')\")\npfikbxuesly ipython3 con.sql(\"FROM ST_Read('cities.geojson')\")\npfikbxuesly ipython3 con.sql(\"CREATE TABLE cities AS SELECT * FROM ST_Read('cities.geojson')\")\npfikbxuesly ipython3 con.table('cities')\npfikbxuesly ipython3 con.sql(\"SELECT * FROM cities\")"
  },
  {
    "objectID": "W2_SQL.html#shapefiles",
    "href": "W2_SQL.html#shapefiles",
    "title": "2  Introduction to SQL",
    "section": "3.12 Shapefiles",
    "text": "3.12 Shapefiles\npfikbxuesly ipython3 con.sql(\"SELECT * FROM ST_Read('cities.shp')\")\npfikbxuesly ipython3 con.sql(\"FROM ST_Read('cities.shp')\")\npfikbxuesly ipython3 con.sql(     \"\"\"         CREATE TABLE IF NOT EXISTS cities2 AS          SELECT * FROM ST_Read('cities.shp')         \"\"\" )\npfikbxuesly ipython3 con.table('cities2')\npfikbxuesly ipython3 con.sql('SELECT * FROM cities2')"
  },
  {
    "objectID": "W2_SQL.html#geoparquet-files",
    "href": "W2_SQL.html#geoparquet-files",
    "title": "2  Introduction to SQL",
    "section": "3.13 GeoParquet Files",
    "text": "3.13 GeoParquet Files\npfikbxuesly ipython3 con.sql(\"SELECT * FROM 'cities.parquet'\")\npfikbxuesly ipython3 con.sql(     \"\"\" CREATE TABLE IF NOT EXISTS cities3 AS SELECT * EXCLUDE geometry, ST_GeomFromWKB(geometry)  AS geometry FROM 'cities.parquet' \"\"\" )\npfikbxuesly ipython3 con.table('cities3')\npfikbxuesly ipython3 con.sql(     \"\"\" CREATE TABLE IF NOT EXISTS country AS SELECT * EXCLUDE geometry, ST_GeomFromWKB(geometry) FROM         's3://us-west-2.opendata.source.coop/google-research-open-buildings/v2/geoparquet-admin1/country=SSD/*.parquet' \"\"\" )\npfikbxuesly ipython3 con.table('country')\npfikbxuesly ipython3 con.sql('SELECT COUNT(*) FROM country')"
  },
  {
    "objectID": "W2_SQL.html#references",
    "href": "W2_SQL.html#references",
    "title": "2  Introduction to SQL",
    "section": "3.14 References",
    "text": "3.14 References\n\nDuckDB Data Ingestion"
  },
  {
    "objectID": "W2_SQL.html#question-1-creating-tables",
    "href": "W2_SQL.html#question-1-creating-tables",
    "title": "2  Introduction to SQL",
    "section": "4.1 Question 1: Creating Tables",
    "text": "4.1 Question 1: Creating Tables\nCreate a database, then write a SQL query to create a table named nyc_subway_stations and load the data from the file nyc_subway_stations.tsv into it. Similarly, create a table named nyc_neighborhoods and load the data from the file nyc_neighborhoods.tsv into it."
  },
  {
    "objectID": "W2_SQL.html#question-2-column-filtering",
    "href": "W2_SQL.html#question-2-column-filtering",
    "title": "2  Introduction to SQL",
    "section": "4.2 Question 2: Column Filtering",
    "text": "4.2 Question 2: Column Filtering\nWrite a SQL query to display the ID, NAME, and BOROUGH of each subway station in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W2_SQL.html#question-3-row-filtering",
    "href": "W2_SQL.html#question-3-row-filtering",
    "title": "2  Introduction to SQL",
    "section": "4.3 Question 3: Row Filtering",
    "text": "4.3 Question 3: Row Filtering\nWrite a SQL query to find all subway stations in the nyc_subway_stations dataset that are located in the borough of Manhattan."
  },
  {
    "objectID": "W2_SQL.html#question-4-sorting-results",
    "href": "W2_SQL.html#question-4-sorting-results",
    "title": "2  Introduction to SQL",
    "section": "4.4 Question 4: Sorting Results",
    "text": "4.4 Question 4: Sorting Results\nWrite a SQL query to list the subway stations in the nyc_subway_stations dataset in alphabetical order by their names."
  },
  {
    "objectID": "W2_SQL.html#question-5-unique-values",
    "href": "W2_SQL.html#question-5-unique-values",
    "title": "2  Introduction to SQL",
    "section": "4.5 Question 5: Unique Values",
    "text": "4.5 Question 5: Unique Values\nWrite a SQL query to find the distinct boroughs represented in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W2_SQL.html#question-6-counting-rows",
    "href": "W2_SQL.html#question-6-counting-rows",
    "title": "2  Introduction to SQL",
    "section": "4.6 Question 6: Counting Rows",
    "text": "4.6 Question 6: Counting Rows\nWrite a SQL query to count the number of subway stations in each borough in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W2_SQL.html#question-7-aggregating-data",
    "href": "W2_SQL.html#question-7-aggregating-data",
    "title": "2  Introduction to SQL",
    "section": "4.7 Question 7: Aggregating Data",
    "text": "4.7 Question 7: Aggregating Data\nWrite a SQL query to list the number of subway stations in each borough, sorted in descending order by the count."
  },
  {
    "objectID": "W2_SQL.html#question-8-joining-tables",
    "href": "W2_SQL.html#question-8-joining-tables",
    "title": "2  Introduction to SQL",
    "section": "4.8 Question 8: Joining Tables",
    "text": "4.8 Question 8: Joining Tables\nWrite a SQL query to join the nyc_subway_stations and nyc_neighborhoods datasets on the borough name, displaying the subway station name and the neighborhood name."
  },
  {
    "objectID": "W2_SQL.html#question-9-string-manipulation",
    "href": "W2_SQL.html#question-9-string-manipulation",
    "title": "2  Introduction to SQL",
    "section": "4.9 Question 9: String Manipulation",
    "text": "4.9 Question 9: String Manipulation\nWrite a SQL query to display the names of subway stations in the nyc_subway_stations dataset that contain the word “St” in their names."
  },
  {
    "objectID": "W2_SQL.html#question-10-filtering-with-multiple-conditions",
    "href": "W2_SQL.html#question-10-filtering-with-multiple-conditions",
    "title": "2  Introduction to SQL",
    "section": "4.10 Question 10: Filtering with Multiple Conditions",
    "text": "4.10 Question 10: Filtering with Multiple Conditions\nWrite a SQL query to find all subway stations in the nyc_subway_stations dataset that are in the borough of Brooklyn and have routes that include the letter “R”."
  },
  {
    "objectID": "W2_SQL.html#spatial-databases",
    "href": "W2_SQL.html#spatial-databases",
    "title": "2  Introduction to SQL",
    "section": "2.1 Spatial databases",
    "text": "2.1 Spatial databases\nA database management system (DBMS) allows users to store, insert, delete, and update information in a database. Spatial databases go a step further because they record data with geographic coordinates.\nFrom Esri Geodatabase to PostGIS, spatial databases have quickly become the primary method of managing spatial data.\nTo learn more about spatial databases, check out the resources below:\n\nWikipedia: Spatial database\n7 Spatial Databases for Your Enterprise\nGISGeography: Spatial Databases – Build Your Spatial Data Empire\nEsri: What is a geodatabase?\nIntroduction to PostGIS\nPostGEESE? Introducing The DuckDB Spatial Extension"
  },
  {
    "objectID": "W2_SQL.html#duckdb",
    "href": "W2_SQL.html#duckdb",
    "title": "2  Introduction to SQL",
    "section": "2.2 DuckDB",
    "text": "2.2 DuckDB\nDuckDB is an in-process SQL OLAP database management system. It is designed to be used as an embedded database in applications, but it can also be used as a standalone SQL database.\n\nIn-process SQL means that DuckDB’s features run in your application, not an external process to which your application connects. In other words: there is no client sending instructions nor a server to read and process them. SQLite works the same way, while PostgreSQL, MySQL…, do not.\nOLAP stands for OnLine Analytical Processing, and Microsoft defines it as a technology that organizes large business databases and supports complex analysis. It can be used to perform complex analytical queries without negatively affecting transactional systems.\n\nDuckDB is a great option if you’re looking for a serverless data analytics database management system."
  },
  {
    "objectID": "W6_RS.html#active-and-passive-sensors",
    "href": "W6_RS.html#active-and-passive-sensors",
    "title": "6  Remote Sensing",
    "section": "6.1 Active and Passive Sensors",
    "text": "6.1 Active and Passive Sensors\nRemote sensing is the science of obtaining information about an object or phenomenon without making physical contact with the object. Remote sensing can be done with various types of electromagnetic radiation such as visible, infrared, or microwave. The electromagnetic radiation is either emitted or reflected from the object being sensed. The reflected radiation is then collected by a sensor and processed to obtain information about the object.\n\nWhile most satellite imagery is optical, meaning it captures sunlight reflected by the earth’s surface, Synthetic Aperture Radar (SAR) satellites such as Sentinel-1 work by emitting pulses of radio waves and measuring how much of the signal is reflected back. This is similar to the way a bat uses sonar to “see” in the dark: by emitting calls and listening to echoes."
  },
  {
    "objectID": "W6_RS.html#resolution",
    "href": "W6_RS.html#resolution",
    "title": "6  Remote Sensing",
    "section": "6.2 Resolution",
    "text": "6.2 Resolution\nResolution is one of the most important attributes of satellite imagery. There are three types of resolution: spatial, spectral, and temporal. Let’s look at each of these.\n\n6.2.1 Spatial Resolution\nSpatial resolution governs how “sharp” an image looks. The Google Maps satellite basemap, for example, is really sharp Most of the optical imagery that is freely available has relatively low spatial resolution (it looks more grainy than, for example, the Google satellite basemap),\n  \n\n\n6.2.2 Spectral Resolution\nWhat open access imagery lacks in spatial resolution it often makes up for with spectral resolution. Really sharp imagery from MAXAR, for example, mostly collects light in the visible light spectrum, which is what our eyes can see. But there are other parts of the electromagnetic spectrum that we can’t see, but which can be very useful for distinguishing between different materials. Many satellites that have a lower spatial resolution than MAXAR, such as Landsat and Sentinel-2, collect data in a wider range of the electromagnetic spectrum.\nDifferent materials reflect light differently. An apple absorbs shorter wavelengths (e.g. blue and green), and reflects longer wavelengths (red). Our eyes use that information– the color– to distinguish between different objects. Below is a plot of the spectral profiles of different materials:\n\n\n\nThe visible portion of the spectrum is highlighted on the left, ranging from 400nm (violet) to 700nm (red). Our eyes (and satellite imagery in the visible light spectrum) can only see this portion of the light spectrum; we can’t see UV or infrared wavelengths, for example, though the extent to which different materials reflect or absorb these wavelengths is just as useful for distinguishing between them. The European Space Agency’s Sentinel-2 satellite collects spectral information well beyond the visible light spectrum, enabling this sort of analysis. It chops the electromagnetic spectrum up into “bands”, and measures how strongly wavelengths in each of those bands is reflected:\n\nTo illustrate why this is important, consider Astroturf (fake plastic grass). Astroturf and real grass will both look green to us, espeically from a satellite image. But living plants strongly reflect radiation from the sun in a part of the light spectrum that we can’t see (near-infrared). There’s a spectral index called the Normalized Difference Vegetation Index (NDVI) which exploits this fact to isolate vegetation in multispectral satellite imagery. So if we look at Gilette Stadium near Boston, we can tell that the three training fields south of the stadium are real grass (they generate high NDVI values, showing up red), while the pitch in the stadium itself is astroturf (generating low NDVI values, showing up blue).\n\n\n\nVHR image of Gilette Stadium with Sentinel-2 derived NDVI overlay\n\n\nIn other words, even though these fields are all green and indistinguishable to the human eye, their spectral profiles beyond the visible light spectrum differ, and we can use this information to distinguish between them.\nAstroturf is a trivial example. But suppose we were interested in identifying makeshift oil refineries in Northern Syria that constitute a key source of rents for whichever group controls them. As demonstrated in the ‘Refinery Identification’ case study, we can train an algorithm to identify the spectral signatures of oil, and use that to automatically detect them in satellite imagery.\n\n\n6.2.3 Temporal Resolution\nFinally, the frequency with which we can access new imagery is an important consideration. This is called the temporal resolution.\nThe Google Maps basemap is very high resolution, available globally, and is freely available. But it has no temporal dimension: it’s a snapshot from one particular point in time. If the thing we’re interested in involves changes over time, this basemap will be of limited use.\nThe “revisit rate” is the amount of time it takes for the satellite to pass over the same location twice. For example, the Sentinel-2 constellation’s two satellites can achieve a revisit rate of 5 days, as shown in this cool video from the European Space Agency:\n\nSome satellite constellations are able to achieve much higher revisit rates. Sentinel-2 has a revisit rate of 5 days, but SkySat capable of imaging the same point on earth around 12 times per day! How is that possible? Well, as the video above demonstrated, the Sentinel-2 constellation is composed of two satellites that share the same orbit, 180 degrees apart. In contrast, the SkySat constellation comprises 21 satellites, each with its own orbital path:\n\nThis allows SkySat to achieve a revisit rate of 2-3 hours. The catch, however, is that you need to pay for it (and it ain’t cheap). Below is a comparison of revisit rates for various other optical satellites:\n\n\n\nA chart of revisit times for different satellites from Sutlieff et. al.(2021)"
  },
  {
    "objectID": "W6_RS.html#summary",
    "href": "W6_RS.html#summary",
    "title": "6  Remote Sensing",
    "section": "6.3 Summary",
    "text": "6.3 Summary\nYou should hopefully have a better understanding of what satellite imagery is, and how it can be used to answer questions about the world. In the next section, we’ll look at the various types of satellite imagery stored in the Google Earth Engine catalogue."
  },
  {
    "objectID": "W3_postgis1.html",
    "href": "W3_postgis1.html",
    "title": "3  Spatial SQL I",
    "section": "",
    "text": "4 Geometry Exercises\nHere's a reminder of all the functions we have seen so far. They should be useful for the exercises!\nAlso remember the tables we have available:\nHere's a reminder of the functions we saw in the last section. They should be useful for the exercises!\nAlso remember the tables we have available:"
  },
  {
    "objectID": "W3_postgis1.html#introduction",
    "href": "W3_postgis1.html#introduction",
    "title": "3  Spatial SQL I",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this lesson, you will learn how to use SQL to query spatial data. You will learn how to use the duckdb Python library to connect to a DuckDB database and run SQL queries. You will also learn how to use the leafmap Python library to visualize spatial data."
  },
  {
    "objectID": "W3_postgis1.html#learning-objectives",
    "href": "W3_postgis1.html#learning-objectives",
    "title": "3  Spatial SQL I",
    "section": "3.2 Learning Objectives",
    "text": "3.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nConnect to a DuckDB database using Python\nRun SQL queries to query spatial data\nVisualize spatial data using Leafmap\nRun spatial SQL queries using PgAdmin"
  },
  {
    "objectID": "W3_postgis1.html#materials",
    "href": "W3_postgis1.html#materials",
    "title": "3  Spatial SQL I",
    "section": "3.3 Materials",
    "text": "3.3 Materials\nTo get started, work your way through the following two notebooks:\n\nGeometries\nSpatial Relationships\n\nClicking the link will take you to Google Colab, where you can run the notebook in your browser. These workbooks use the duckdb Python library to connect to a DuckDB database and run SQL queries. They also use the leafmap Python library to visualize spatial data. Most of the spatial functions and syntax are the same or very similar to their PostGIS equivalents.\nOnce you’ve completed these you can test your knowledge by answering the questions in the lab below. To complete this lab, open PgAdmin and connect to the nyc database. Then, open a new query window and write your SQL queries there."
  },
  {
    "objectID": "W4_postgis2.html",
    "href": "W4_postgis2.html",
    "title": "4  PostGIS II",
    "section": "",
    "text": "5 Spatial Joins Exercises\nHere's a reminder of some of the functions we have seen. Hint: they should be useful for the exercises!\nAlso remember the tables we have available:\nNow for a less structured exercise. We’re going to look at ship-to-ship transfers. The idea is that two ships meet up in the middle of the ocean, and one ship transfers cargo to the other. This is a common way to avoid sanctions, and is often used to transfer oil from sanctioned countries to other countries. We’re going to look at a few different ways to detect these transfers using AIS data."
  },
  {
    "objectID": "W4_postgis2.html#setup",
    "href": "W4_postgis2.html#setup",
    "title": "4  PostGIS II",
    "section": "5.1 Setup",
    "text": "5.1 Setup\nUncomment and run the following cell to install the required packages.\nnidbblocduu ipython3 # %pip install duckdb leafmap lonboard\nnidbblocduu ipython3 import duckdb import leafmap"
  },
  {
    "objectID": "W4_postgis2.html#question-1",
    "href": "W4_postgis2.html#question-1",
    "title": "4  PostGIS II",
    "section": "5.2 Question 1",
    "text": "5.2 Question 1\nDownload the nyc_data.zip dataset using leafmap. The zip file contains the following datasets. Create a new DuckDB database and import the datasets into the database. Each dataset should be imported into a separate table.\n\nnyc_census_blocks\nnyc_homicides\nnyc_neighborhoods\nnyc_streets\nnyc_subway_stations\n\nnidbblocduu ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-2",
    "href": "W4_postgis2.html#question-2",
    "title": "4  PostGIS II",
    "section": "5.3 Question 2",
    "text": "5.3 Question 2\n+++\nVisualize the nyc_subway_stations and nyc_streets datasets on the same map using leafmap and lonboard.\nnidbblocduu ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-3",
    "href": "W4_postgis2.html#question-3",
    "title": "4  PostGIS II",
    "section": "5.4 Question 3",
    "text": "5.4 Question 3\nFind out what neighborhood the BLUE subway stations are in.\nwgkruzvfmij ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-4",
    "href": "W4_postgis2.html#question-4",
    "title": "4  PostGIS II",
    "section": "5.5 Question 4",
    "text": "5.5 Question 4\nFind out what streets are within 200 meters of the BLUE subway stations.\nwgkruzvfmij ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-5",
    "href": "W4_postgis2.html#question-5",
    "title": "4  PostGIS II",
    "section": "5.6 Question 5",
    "text": "5.6 Question 5\nVisualize the BLUE subway stations and the streets within 200 meters of the BLUE subway stations on the same map using leafmap and lonboard.\nwgkruzvfmij ipython3 # Add your code here"
  },
  {
    "objectID": "W3_postgis1.html#exercises",
    "href": "W3_postgis1.html#exercises",
    "title": "3  Spatial SQL I",
    "section": "4.1 Exercises",
    "text": "4.1 Exercises\n\nWhat is the area of the 'West Village' neighborhood? (Hint: The area is given in square meters. To get an area in hectares, divide by 10000. To get an area in acres, divide by 4047.)\nWhat is the geometry type of ‘Pelham St’? The length?\nWhat is the GeoJSON representation of the 'Broad St' subway station?\nWhat is the total length of streets (in kilometers) in New York City? (Hint: The units of measurement of the spatial data are meters, there are 1000 meters in a kilometer.)\nWhat is the area of Manhattan in acres? (Hint: both nyc_census_blocks and nyc_neighborhoods have a boroname in them.)\nWhat is the most westerly subway station?\nHow long is 'Columbus Cir' (aka Columbus Circle)?\nWhat is the length of streets in New York City, summarized by type?\n\nAnswers (only check after you’ve given it your best shot!)"
  },
  {
    "objectID": "W3_postgis1.html#exercises-1",
    "href": "W3_postgis1.html#exercises-1",
    "title": "3  Spatial SQL I",
    "section": "5.1 Exercises",
    "text": "5.1 Exercises\n\nWhat is the geometry value for the street named 'Atlantic Commons'?\nWhat neighborhood and borough is Atlantic Commons in?\nWhat streets does Atlantic Commons join with?\nApproximately how many people live on (within 50 meters of) Atlantic Commons?\n\nAnswers (only check after you’ve given it your best shot!)"
  },
  {
    "objectID": "W4_postgis2.html#step-1",
    "href": "W4_postgis2.html#step-1",
    "title": "4  PostGIS II",
    "section": "7.1 Step 1",
    "text": "7.1 Step 1\nCreate a spatial database using the following AIS data:\nhttps://storage.googleapis.com/qm2/casa0025_ships.csv\nEach row in this dataset is an AIS ‘ping’ indicating the position of a ship at a particular date/time, alongside vessel-level characteristics.\nIt contains the following columns: * vesselid: A unique numerical identifier for each ship, like a license plate * vessel_name: The ship’s name * vsl_descr: The ship’s type * dwt: The ship’s Deadweight Tonnage (how many tons it can carry) * v_length: The ship’s length in meters * draught: How many meters deep the ship is draughting (how low it sits in the water). Effectively indicates how much cargo the ship is carrying * sog: Speed over Ground (in knots) * date: A timestamp for the AIS signal * lat: The latitude of the AIS signal (EPSG:4326) * lon: The longitude of the AIS signal (EPSG:4326)\nCreate a table called ‘ais’ where each row is a different AIS ping, with no superfluous information. Construct a geometry column.\nCreate a second table called ‘vinfo’ which contains vessel-level information with no superfluous information."
  },
  {
    "objectID": "W4_postgis2.html#step-2",
    "href": "W4_postgis2.html#step-2",
    "title": "4  PostGIS II",
    "section": "7.2 Step 2",
    "text": "7.2 Step 2\nUse a spatial join to identify ship-to-ship transfers in this dataset. Two ships are considered to be conducting a ship to ship transfer IF:\n\nThey are within 500 meters of each other\nFor more than two hours\nAnd their speed is lower than 1 knot\n\nSome things to consider: make sure you’re not joining ships with themselves. Try working with subsets of the data first while you try different things out."
  }
]