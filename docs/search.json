[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "",
    "text": "Module Overview\nGeospatial analytics and dashboards are in very high remand among policymakers, NGOs, IGOs, and the private sector. Deploying these systems often requires handling data that exceeds the computational and storage capabilities of personal machines. This module will teach students how to harness and critically interrogate large quantities of geospatial data using cloud computing services, and how to design and build an interactive online application that communicates geospatial insights to wider audiences.\nIn line with this objective, the module is divided into two sections. In the first, database concepts and techniques are introduced, providing the students with the skills required to manipulate and derive meaning from organised datasets. SQL syntax will be taught in depth at this stage, with a strong emphasis on practical application. This will allow students to learn state of the art methods for handling large vector datasets.\nThe second section of the course focuses on the handling of large raster datasets. As geospatial datasets—particularly satellite imagery collections—increase in size, researchers are increasingly relying on cloud computing platforms such as Google Earth Engine (GEE) to analyze vast quantities of data. Despite the fact that it was only released in 2015, the number of geospatial journal articles using Google Earth Engine has outpaced every other major geospatial analysis software, including ArcGIS, Python, and R in just five years. Weeks 6-9 will be co-taught with CASA0023 Remote Sensing.\nThe module therefore spans a full, cloud-based geospatial workflow: from importing and analysing geospatial data, to building and presenting interactive data visualisations. Students will gain proficiency in working with and interrogating large spatial data sets while working towards an interactive group project that will develop their portfolio."
  },
  {
    "objectID": "index.html#what-is-sql",
    "href": "index.html#what-is-sql",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "What is SQL?",
    "text": "What is SQL?\nSQL (Structured Query Language) is a programming language used to communicate with databases. It is the standard language for relational database management systems. SQL statements are used to perform tasks such as update data on a database, or retrieve data from a database. Some common relational database management systems that use SQL are: Oracle, Sybase, Microsoft SQL Server, Access, Ingres, etc. Although most database systems use SQL, most of them also have their own additional proprietary extensions that are usually only used on their system. However, the standard SQL commands such as “Select”, “Insert”, “Update”, “Delete”, “Create”, and “Drop” can be used to accomplish almost everything that one needs to do with a database.\nThe first five weeks of this module will focus on working with large vector datasets. We will use SQL to query and manipulate data stored in a PostgreSQL database. PostgreSQL is a free and open-source relational database management system emphasizing extensibility and SQL compliance. It is the most advanced open-source database system widely used for GIS applications. We will also work with DuckDB, a new, open-source, in-process SQL OLAP database management system. DuckDB is designed to be used as an embedded database library, providing C/C++, Python, R, Java, and Go bindings. It has a built-in SQL engine with support for transactions, a powerful query optimizer, and a columnar storage engine."
  },
  {
    "objectID": "index.html#what-is-google-earth-engine",
    "href": "index.html#what-is-google-earth-engine",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "What is Google Earth Engine?",
    "text": "What is Google Earth Engine?\nAs geospatial datasets—particularly satellite imagery collections—increase in size, researchers are increasingly relying on cloud computing platforms such as Google Earth Engine (GEE) to analyze vast quantities of data.\nGEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geospatial analysis software platforms:\n\nDespite only being released in 2015, the number of geospatial journal articles using Google Earth Engine (shown in red above) has outpaced every other major geospatial analysis software, including ArcGIS, Python, and R in just five years. GEE applications have been developed and used to present interactive geospatial data visualizations by NGOs, Universities, the United Nations, and the European Commission. By storing and running computations on google servers, GEE is far more accessible to those who don’t have significant local computational resources; all you need is an internet connection."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "CASA0025: Building Spatial Applications with Big Data",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nSQL\n\nFive initial weeks exporing spatial database systems including PostgreSQL and DuckDB.\n\nIntroduction\nSQL\nPostGIS I\nPostGIS II\nDatabase Quiz\n\n\nGoogle Earth Engine\n\nFive weeks on Google Earth Engine, a cloud-based platform for geospatial analysis.\n\nIntroduction to GEE\nClassification I\nClassification II\nSynthetic Aperture Radar\nUser Interface Design\nBonus: Object Detection\n\n\n\nGEE Textbook * Recently, a team of over 100 scientists came together to write a book called “Cloud-Based Remote Sensing with Google Earth Engine: Fundamentals and Applications”. It’s a great resource for learning about remote sensing and Earth Engine. The material in this section is a subset of the book, edited to fit the scope of this guide. If you’re interested in learning more, check out the full book. * Getting Started * Interpreting Images * Image Series * Vectors and Tables"
  },
  {
    "objectID": "W1_intro.html",
    "href": "W1_intro.html",
    "title": "1  Installation",
    "section": "",
    "text": "2 Creating a Spatial Database\nSupported by a wide variety of libraries and applications, PostGIS provides many options for loading data.\nWe will first load our working data from a database backup file, then review some standard ways of loading different GIS data formats using common tools."
  },
  {
    "objectID": "W1_intro.html#postgresql-for-microsoft-windows",
    "href": "W1_intro.html#postgresql-for-microsoft-windows",
    "title": "1  Installation",
    "section": "1.1 PostgreSQL for Microsoft Windows",
    "text": "1.1 PostgreSQL for Microsoft Windows\nFor a Windows install:\n\nGo to the Windows PostgreSQL download page.\nSelect the latest version of PostgreSQL and save the installer to disk.\nRun the installer and accept the defaults.\nFind and run the \"StackBuilder\" program that was installed with the database.\nSelect the \"Spatial Extensions\" section and choose latest \"PostGIS ..Bundle\" option.\n\nAccept the defaults and install."
  },
  {
    "objectID": "W1_intro.html#postgresql-for-apple-macos",
    "href": "W1_intro.html#postgresql-for-apple-macos",
    "title": "1  Installation",
    "section": "1.2 PostgreSQL for Apple MacOS",
    "text": "1.2 PostgreSQL for Apple MacOS\nFor a MacOS install:\n\nGo to the Postgres.app site, and download the latest release.\nOpen the disk image, and drag the Postgres icon into the Applications folder.\n\nIn the Applications folder, double-click the Postgres icon to start the server.\nClick the Initialize button to create a new blank database instance.\n{.inline, .border .inline, .border}\nIn the Applications folder, go to the Utilities folder and open Terminal.\nAdd the command-line utilities to your PATH for convenience.\n\n\nsudo mkdir -p /etc/paths.d\necho /Applications/Postgres.app/Contents/Versions/latest/bin | sudo tee /etc/paths.d/postgresapp"
  },
  {
    "objectID": "W1_intro.html#pgadmin-for-windows-and-macos",
    "href": "W1_intro.html#pgadmin-for-windows-and-macos",
    "title": "1  Installation",
    "section": "1.3 PgAdmin for Windows and MacOS",
    "text": "1.3 PgAdmin for Windows and MacOS\nPgAdmin is available for multiple platforms, at https://www.pgadmin.org/download/.\n\nDownload and install the latest version for your platform.\nStart PgAdmin!"
  },
  {
    "objectID": "W1_intro.html#pgadmin",
    "href": "W1_intro.html#pgadmin",
    "title": "1  Installation",
    "section": "2.1 PgAdmin",
    "text": "2.1 PgAdmin\nPostgreSQL has a number of administrative front-ends. The primary one is psql, a command-line tool for entering SQL queries. Another popular PostgreSQL front-end is the free and open source graphical tool pgAdmin. All queries done in pgAdmin can also be done on the command line with psql. pgAdmin also includes a geometry viewer you can use to spatial view PostGIS queries.\n\nFind pgAdmin and start it up.\n\nIf this is the first time you have run pgAdmin, you probably don't have any servers configured. Right click the Servers item in the Browser panel.\nWe'll name our server PostGIS. In the Connection tab, enter the Host name/address. If you're working with a local PostgreSQL install, you'll be able to use localhost. If you're using a cloud service, you should be able to retrieve the host name from your account.\nLeave Port set at 5432, and both Maintenance database and Username as postgres. The Password should be what you specified with a local install or with your cloud service."
  },
  {
    "objectID": "W1_intro.html#creating-a-database",
    "href": "W1_intro.html#creating-a-database",
    "title": "1  Installation",
    "section": "2.2 Creating a Database",
    "text": "2.2 Creating a Database\n\nOpen the Databases tree item and have a look at the available databases. The postgres database is the user database for the default postgres user and is not too interesting to us.\nRight-click on the Databases item and select New Database.\n\nFill in the Create Database form as shown below and click OK.\n\n\n\nName\nnyc\n\n\nOwner\npostgres\n\n\n\n\nSelect the new nyc database and open it up to display the tree of objects. You'll see the public schema.\n\nClick on the SQL query button indicated below (or go to Tools &gt; Query Tool).\n\nEnter the following query into the query text field to load the PostGIS spatial extension:\nCREATE EXTENSION postgis;\nClick the Play button in the toolbar (or press F5) to \"Execute the query.\"\nNow confirm that PostGIS is installed by running a PostGIS function:\nSELECT postgis_full_version();\n\nYou have successfully created a PostGIS spatial database!!"
  },
  {
    "objectID": "W1_intro.html#function-list",
    "href": "W1_intro.html#function-list",
    "title": "1  Installation",
    "section": "2.3 Function List",
    "text": "2.3 Function List\nPostGIS_Full_Version: Reports full PostGIS version and build configuration info."
  },
  {
    "objectID": "W1_intro.html#loading-the-backup-file",
    "href": "W1_intro.html#loading-the-backup-file",
    "title": "1  Installation",
    "section": "3.1 Loading the Backup File",
    "text": "3.1 Loading the Backup File\n\nIn the PgAdmin browser, right-click on the nyc database icon, and then select the Restore... option.\n{.inline, .border .inline, .border}\nBrowse to the location of your workshop data data directory (available in the workshop data bundle), and select the nyc_data.backup file.\n{.inline, .border .inline, .border}\nClick on the Restore options tab, scroll down to the Do not save section and toggle Owner to Yes.\n{.inline, .border .inline, .border}\nClick the Restore button. The database restore should run to completion without errors.\n{.inline, .border .inline, .border}\nAfter the load is complete, right click the nyc database, and select the Refresh option to update the client information about what tables exist in the database.\n{.inline, .border .inline, .border}\n\n\n\nNote\n\nIf you want to practice loading data from the native spatial formats, instead of using the PostgreSQL db backup files just covered, the next couple of sections will guide you thru loading using various command-line tools and QGIS DbManager. Note you can skip these sections, if you have already loaded the data with pgAdmin."
  },
  {
    "objectID": "W1_intro.html#shapefiles-whats-that",
    "href": "W1_intro.html#shapefiles-whats-that",
    "title": "1  Installation",
    "section": "3.2 Shapefiles? What's that?",
    "text": "3.2 Shapefiles? What's that?\nYou may be asking yourself -- \"What's this shapefile thing?\" A \"shapefile\" commonly refers to a collection of files with .shp, .shx, .dbf, and other extensions on a common prefix name (e.g., nyc_census_blocks). The actual shapefile relates specifically to files with the .shp extension. However, the .shp file alone is incomplete for distribution without the required supporting files.\nMandatory files:\n\n.shp—shape format; the feature geometry itself\n.shx—shape index format; a positional index of the feature geometry\n.dbf—attribute format; columnar attributes for each shape, in dBase III\n\nOptional files include:\n\n.prj—projection format; the coordinate system and projection information, a plain text file describing the projection using well-known text format\n\nThe shp2pgsql utility makes shape data usable in PostGIS by converting it from binary data into a series of SQL commands that are then run in the database to load the data."
  },
  {
    "objectID": "W1_intro.html#loading-with-shp2pgsql",
    "href": "W1_intro.html#loading-with-shp2pgsql",
    "title": "1  Installation",
    "section": "3.3 Loading with shp2pgsql",
    "text": "3.3 Loading with shp2pgsql\nThe shp2pgsql converts Shape files into SQL. It is a conversion utility that is part of the PostGIS code base and ships with PostGIS packages. If you installed PostgreSQL locally on your computer, you may find that shp2pgsql has been installed along with it, and it is available in the executable directory of your installation.\nUnlike ogr2ogr, shp2pgsql does not connect directly to the destination database, it just emits the SQL equivalent to the input shape file. It is up to the user to pass the SQL to the database, either with a \"pipe\" or by saving the SQL to file and then loading it.\nHere is an example invocation, loading the same data as before:\nexport PGPASSWORD=mydatabasepassword\n\nshp2pgsql \\\n  -D \\\n  -I \\\n  -s 26918 \\\n  nyc_census_blocks_2000.shp \\\n  nyc_census_blocks_2000 \\\n  | psql dbname=nyc user=postgres host=localhost\nHere is a line-by-line explanation of the command.\nshp2pgsql \\\nThe executable program! It reads the source data file, and emits SQL which can be directed to a file or piped to psql to load directly into the database.\n-D \\\nThe D flag tells the program to generate \"dump format\" which is much faster to load than the default \"insert format\".\n-I \\\nThe I flag tells the program to create a spatial index on the table after loading is complete.\n-s 26918 \\\nThe s flag tells the program what the \"spatial reference identifier (SRID)\" of the data is. The source data for this workshop is all in \"UTM 18\", for which the SRID is 26918 (see below).\nnyc_census_blocks_2000.shp \\\nThe source shape file to read.\nnyc_census_blocks_2000 \\\nThe table name to use when creating the destination table.\n| psql dbname=nyc user=postgres host=localhost\nThe utility program is generating a stream of SQL. The \"|\" operator takes that stream and uses it as input to the psql database terminal program. The arguments to psql are just the connection string for the destination database."
  },
  {
    "objectID": "W1_intro.html#srid-26918-whats-with-that",
    "href": "W1_intro.html#srid-26918-whats-with-that",
    "title": "1  Installation",
    "section": "3.4 SRID 26918? What's with that?",
    "text": "3.4 SRID 26918? What's with that?\nMost of the import process is self-explanatory, but even experienced GIS professionals can trip over an SRID.\nAn \"SRID\" stands for \"Spatial Reference IDentifier.\" It defines all the parameters of our data's geographic coordinate system and projection. An SRID is convenient because it packs all the information about a map projection (which can be quite complex) into a single number.\nYou can see the definition of our workshop map projection by looking it up either in an online database,\n\nhttps://epsg.io/26918\n\nor directly inside PostGIS with a query to the spatial_ref_sys table.\nSELECT srtext FROM spatial_ref_sys WHERE srid = 26918;\n\n\nNote\n\nThe PostGIS spatial_ref_sys table is an OGC-standard table that defines all the spatial reference systems known to the database. The data shipped with PostGIS, lists over 3000 known spatial reference systems and details needed to transform/re-project between them.\n\nIn both cases, you see a textual representation of the 26918 spatial reference system (pretty-printed here for clarity):\nPROJCS[\"NAD83 / UTM zone 18N\",\n  GEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n      SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],\n      AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4269\"]],\n  UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],\n  PROJECTION[\"Transverse_Mercator\"],\n  PARAMETER[\"latitude_of_origin\",0],\n  PARAMETER[\"central_meridian\",-75],\n  PARAMETER[\"scale_factor\",0.9996],\n  PARAMETER[\"false_easting\",500000],\n  PARAMETER[\"false_northing\",0],\n  AUTHORITY[\"EPSG\",\"26918\"],\n  AXIS[\"Easting\",EAST],\n  AXIS[\"Northing\",NORTH]]\nIf you open up the nyc_neighborhoods.prj file from the data directory, you'll see the same projection definition.\nData you receive from local agencies—such as New York City—will usually be in a local projection noted by \"state plane\" or \"UTM\". Our projection is \"Universal Transverse Mercator (UTM) Zone 18 North\" or EPSG:26918."
  },
  {
    "objectID": "W1_intro.html#things-to-try-view-data-using-qgis",
    "href": "W1_intro.html#things-to-try-view-data-using-qgis",
    "title": "1  Installation",
    "section": "3.5 Things to Try: View data using QGIS",
    "text": "3.5 Things to Try: View data using QGIS\nQGIS, is a desktop GIS viewer/editor for quickly looking at data. You can view a number of data formats including flat shapefiles and a PostGIS database. Its graphical interface allows for easy exploration of your data, as well as simple testing and fast styling.\nTry using this software to connect your PostGIS database. The application can be downloaded from https://qgis.org\nYou'll first want to create a connection to a PostGIS database using menu Layer-&gt;Add Layer-&gt;PostGIS Layers-&gt;New and then filling in the prompts. Once you have a connection, you can add Layers by clicking connect and selecting a table to display."
  },
  {
    "objectID": "W1_intro.html#loading-data-using-qgis-dbmanager",
    "href": "W1_intro.html#loading-data-using-qgis-dbmanager",
    "title": "1  Installation",
    "section": "3.6 Loading data using QGIS DbManager",
    "text": "3.6 Loading data using QGIS DbManager\nQGIS comes with a tool called DbManager that allows you to connect to various different kinds of databases, including a PostGIS enabled one. After you have a PostGIS Database connection configured, go to Database-&gt;DbManager and expand to your database as shown below:\n{.inline, .border .inline, .border}\nFrom there you can use the Import Layer/File menu option to load numerous different spatial formats. In addition to being able to load data from many spatial formats and export data to many formats, you can also add ad-hoc queries to the canvas or define views in your database, using the highlighted wrench icon.\nThis section is based on the PostGIS Intro Workshop, sections 3, 4, 5,and 7"
  },
  {
    "objectID": "W2_SQL.html",
    "href": "W2_SQL.html",
    "title": "2  Introduction to SQL",
    "section": "",
    "text": "3 Getting Started\nThis week we’ll be learning how to use SQL to query data from a database. To get started, work your way through the following two notebooks:\nClicking the link will take you to Google Colab, where you can run the notebook in your browser. You can also download the notebook and run it locally if you prefer.\nOnce you’ve completed these you can test your knowledge by answering the questions in the lab below. You can also access it on Google Colab here.\nDatasets:\nThe following datasets are used in this lab. You don’t need to download them manually, they can be accessed directly from the notebook."
  },
  {
    "objectID": "W2_SQL.html#introduction",
    "href": "W2_SQL.html#introduction",
    "title": "2  Introduction to SQL",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThis notebook shows how to import data into a DuckDB database. It uses the duckdb Python package to connect to a DuckDB database and import data from various formats, including CSV, JSON, DataFrame, parquet, GeoJSON, Shapefile, GeoParquet, and more."
  },
  {
    "objectID": "W2_SQL.html#datasets",
    "href": "W2_SQL.html#datasets",
    "title": "2  Introduction to SQL",
    "section": "3.2 Datasets",
    "text": "3.2 Datasets\nThe following datasets are used in this notebook. You don’t need to download them, they can be accessed directly from the notebook.\n\ncities.csv\ncountries.csv"
  },
  {
    "objectID": "W2_SQL.html#installation",
    "href": "W2_SQL.html#installation",
    "title": "2  Introduction to SQL",
    "section": "3.3 Installation",
    "text": "3.3 Installation\nUncomment the following cell to install the required packages if needed.\npfikbxuesly ipython3 # %pip install duckdb leafmap"
  },
  {
    "objectID": "W2_SQL.html#library-import",
    "href": "W2_SQL.html#library-import",
    "title": "2  Introduction to SQL",
    "section": "3.4 Library Import",
    "text": "3.4 Library Import\npfikbxuesly ipython3 import duckdb import leafmap import pandas as pd"
  },
  {
    "objectID": "W2_SQL.html#installing-extensions",
    "href": "W2_SQL.html#installing-extensions",
    "title": "2  Introduction to SQL",
    "section": "3.5 Installing Extensions",
    "text": "3.5 Installing Extensions\nDuckDB’s Python API provides functions for installing and loading extensions, which perform the equivalent operations to running the INSTALL and LOAD SQL commands, respectively. An example that installs and loads the httpfs extension looks like follows:\npfikbxuesly ipython3 con = duckdb.connect()\npfikbxuesly ipython3 con.install_extension(\"httpfs\") con.load_extension(\"httpfs\")\npfikbxuesly ipython3 con.install_extension(\"spatial\") con.load_extension(\"spatial\")"
  },
  {
    "objectID": "W2_SQL.html#downloading-sample-data",
    "href": "W2_SQL.html#downloading-sample-data",
    "title": "2  Introduction to SQL",
    "section": "3.6 Downloading Sample Data",
    "text": "3.6 Downloading Sample Data\npfikbxuesly ipython3 url = \"https://open.gishub.org/data/duckdb/cities.zip\" leafmap.download_file(url, unzip=True)"
  },
  {
    "objectID": "W2_SQL.html#csv-files",
    "href": "W2_SQL.html#csv-files",
    "title": "2  Introduction to SQL",
    "section": "3.7 CSV Files",
    "text": "3.7 CSV Files\nCSV files can be read using the read_csv function, called either from within Python or directly from within SQL. By default, the read_csv function attempts to auto-detect the CSV settings by sampling from the provided file.\npfikbxuesly ipython3 # read from a file using fully auto-detected settings con.read_csv('cities.csv')\npfikbxuesly ipython3 # specify options on how the CSV is formatted internally con.read_csv('cities.csv', header=True, sep=',')\npfikbxuesly ipython3 # use the (experimental) parallel CSV reader con.read_csv('cities.csv', parallel=True)\npfikbxuesly ipython3 # directly read a CSV file from within SQL con.sql(\"SELECT * FROM 'cities.csv'\")\npfikbxuesly ipython3 # call read_csv from within SQL con.sql(\"SELECT * FROM read_csv_auto('cities.csv')\")"
  },
  {
    "objectID": "W2_SQL.html#json-files",
    "href": "W2_SQL.html#json-files",
    "title": "2  Introduction to SQL",
    "section": "3.8 JSON Files",
    "text": "3.8 JSON Files\nJSON files can be read using the read_json function, called either from within Python or directly from within SQL. By default, the read_json function will automatically detect if a file contains newline-delimited JSON or regular JSON, and will detect the schema of the objects stored within the JSON file.\npfikbxuesly ipython3 # read from a single JSON file con.read_json('cities.json')\npfikbxuesly ipython3 # directly read a JSON file from within SQL con.sql(\"SELECT * FROM 'cities.json'\")\npfikbxuesly ipython3 # call read_json from within SQL con.sql(\"SELECT * FROM read_json_auto('cities.json')\")"
  },
  {
    "objectID": "W2_SQL.html#dataframes",
    "href": "W2_SQL.html#dataframes",
    "title": "2  Introduction to SQL",
    "section": "3.9 DataFrames",
    "text": "3.9 DataFrames\nDuckDB is automatically able to query a Pandas DataFrame.\npfikbxuesly ipython3 df = pd.read_csv('cities.csv') df\npfikbxuesly ipython3 con.sql('SELECT * FROM df').fetchall()"
  },
  {
    "objectID": "W2_SQL.html#parquet-files",
    "href": "W2_SQL.html#parquet-files",
    "title": "2  Introduction to SQL",
    "section": "3.10 Parquet Files",
    "text": "3.10 Parquet Files\nParquet files can be read using the read_parquet function, called either from within Python or directly from within SQL.\npfikbxuesly ipython3 # read from a single Parquet file con.read_parquet('cities.parquet')\npfikbxuesly ipython3 # directly read a Parquet file from within SQL con.sql(\"SELECT * FROM 'cities.parquet'\")\npfikbxuesly ipython3 # call read_parquet from within SQL con.sql(\"SELECT * FROM read_parquet('cities.parquet')\")"
  },
  {
    "objectID": "W2_SQL.html#geojson-files",
    "href": "W2_SQL.html#geojson-files",
    "title": "2  Introduction to SQL",
    "section": "3.11 GeoJSON Files",
    "text": "3.11 GeoJSON Files\npfikbxuesly ipython3 con.sql('SELECT * FROM ST_Drivers()')\npfikbxuesly ipython3 con.sql(\"SELECT * FROM ST_Read('cities.geojson')\")\npfikbxuesly ipython3 con.sql(\"FROM ST_Read('cities.geojson')\")\npfikbxuesly ipython3 con.sql(\"CREATE TABLE cities AS SELECT * FROM ST_Read('cities.geojson')\")\npfikbxuesly ipython3 con.table('cities')\npfikbxuesly ipython3 con.sql(\"SELECT * FROM cities\")"
  },
  {
    "objectID": "W2_SQL.html#shapefiles",
    "href": "W2_SQL.html#shapefiles",
    "title": "2  Introduction to SQL",
    "section": "3.12 Shapefiles",
    "text": "3.12 Shapefiles\npfikbxuesly ipython3 con.sql(\"SELECT * FROM ST_Read('cities.shp')\")\npfikbxuesly ipython3 con.sql(\"FROM ST_Read('cities.shp')\")\npfikbxuesly ipython3 con.sql(     \"\"\"         CREATE TABLE IF NOT EXISTS cities2 AS          SELECT * FROM ST_Read('cities.shp')         \"\"\" )\npfikbxuesly ipython3 con.table('cities2')\npfikbxuesly ipython3 con.sql('SELECT * FROM cities2')"
  },
  {
    "objectID": "W2_SQL.html#geoparquet-files",
    "href": "W2_SQL.html#geoparquet-files",
    "title": "2  Introduction to SQL",
    "section": "3.13 GeoParquet Files",
    "text": "3.13 GeoParquet Files\npfikbxuesly ipython3 con.sql(\"SELECT * FROM 'cities.parquet'\")\npfikbxuesly ipython3 con.sql(     \"\"\" CREATE TABLE IF NOT EXISTS cities3 AS SELECT * EXCLUDE geometry, ST_GeomFromWKB(geometry)  AS geometry FROM 'cities.parquet' \"\"\" )\npfikbxuesly ipython3 con.table('cities3')\npfikbxuesly ipython3 con.sql(     \"\"\" CREATE TABLE IF NOT EXISTS country AS SELECT * EXCLUDE geometry, ST_GeomFromWKB(geometry) FROM         's3://us-west-2.opendata.source.coop/google-research-open-buildings/v2/geoparquet-admin1/country=SSD/*.parquet' \"\"\" )\npfikbxuesly ipython3 con.table('country')\npfikbxuesly ipython3 con.sql('SELECT COUNT(*) FROM country')"
  },
  {
    "objectID": "W2_SQL.html#references",
    "href": "W2_SQL.html#references",
    "title": "2  Introduction to SQL",
    "section": "3.14 References",
    "text": "3.14 References\n\nDuckDB Data Ingestion"
  },
  {
    "objectID": "W2_SQL.html#question-1-creating-tables",
    "href": "W2_SQL.html#question-1-creating-tables",
    "title": "2  Introduction to SQL",
    "section": "4.1 Question 1: Creating Tables",
    "text": "4.1 Question 1: Creating Tables\nCreate a database, then write a SQL query to create a table named nyc_subway_stations and load the data from the file nyc_subway_stations.tsv into it. Similarly, create a table named nyc_neighborhoods and load the data from the file nyc_neighborhoods.tsv into it."
  },
  {
    "objectID": "W2_SQL.html#question-2-column-filtering",
    "href": "W2_SQL.html#question-2-column-filtering",
    "title": "2  Introduction to SQL",
    "section": "4.2 Question 2: Column Filtering",
    "text": "4.2 Question 2: Column Filtering\nWrite a SQL query to display the ID, NAME, and BOROUGH of each subway station in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W2_SQL.html#question-3-row-filtering",
    "href": "W2_SQL.html#question-3-row-filtering",
    "title": "2  Introduction to SQL",
    "section": "4.3 Question 3: Row Filtering",
    "text": "4.3 Question 3: Row Filtering\nWrite a SQL query to find all subway stations in the nyc_subway_stations dataset that are located in the borough of Manhattan."
  },
  {
    "objectID": "W2_SQL.html#question-4-sorting-results",
    "href": "W2_SQL.html#question-4-sorting-results",
    "title": "2  Introduction to SQL",
    "section": "4.4 Question 4: Sorting Results",
    "text": "4.4 Question 4: Sorting Results\nWrite a SQL query to list the subway stations in the nyc_subway_stations dataset in alphabetical order by their names."
  },
  {
    "objectID": "W2_SQL.html#question-5-unique-values",
    "href": "W2_SQL.html#question-5-unique-values",
    "title": "2  Introduction to SQL",
    "section": "4.5 Question 5: Unique Values",
    "text": "4.5 Question 5: Unique Values\nWrite a SQL query to find the distinct boroughs represented in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W2_SQL.html#question-6-counting-rows",
    "href": "W2_SQL.html#question-6-counting-rows",
    "title": "2  Introduction to SQL",
    "section": "4.6 Question 6: Counting Rows",
    "text": "4.6 Question 6: Counting Rows\nWrite a SQL query to count the number of subway stations in each borough in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W2_SQL.html#question-7-aggregating-data",
    "href": "W2_SQL.html#question-7-aggregating-data",
    "title": "2  Introduction to SQL",
    "section": "4.7 Question 7: Aggregating Data",
    "text": "4.7 Question 7: Aggregating Data\nWrite a SQL query to list the number of subway stations in each borough, sorted in descending order by the count."
  },
  {
    "objectID": "W2_SQL.html#question-8-joining-tables",
    "href": "W2_SQL.html#question-8-joining-tables",
    "title": "2  Introduction to SQL",
    "section": "4.8 Question 8: Joining Tables",
    "text": "4.8 Question 8: Joining Tables\nWrite a SQL query to join the nyc_subway_stations and nyc_neighborhoods datasets on the borough name, displaying the subway station name and the neighborhood name."
  },
  {
    "objectID": "W2_SQL.html#question-9-string-manipulation",
    "href": "W2_SQL.html#question-9-string-manipulation",
    "title": "2  Introduction to SQL",
    "section": "4.9 Question 9: String Manipulation",
    "text": "4.9 Question 9: String Manipulation\nWrite a SQL query to display the names of subway stations in the nyc_subway_stations dataset that contain the word “St” in their names."
  },
  {
    "objectID": "W2_SQL.html#question-10-filtering-with-multiple-conditions",
    "href": "W2_SQL.html#question-10-filtering-with-multiple-conditions",
    "title": "2  Introduction to SQL",
    "section": "4.10 Question 10: Filtering with Multiple Conditions",
    "text": "4.10 Question 10: Filtering with Multiple Conditions\nWrite a SQL query to find all subway stations in the nyc_subway_stations dataset that are in the borough of Brooklyn and have routes that include the letter “R”."
  },
  {
    "objectID": "W2_SQL.html#spatial-databases",
    "href": "W2_SQL.html#spatial-databases",
    "title": "2  Introduction to SQL",
    "section": "2.1 Spatial databases",
    "text": "2.1 Spatial databases\nA database management system (DBMS) allows users to store, insert, delete, and update information in a database. Spatial databases go a step further because they record data with geographic coordinates.\nFrom Esri Geodatabase to PostGIS, spatial databases have quickly become the primary method of managing spatial data.\nTo learn more about spatial databases, check out the resources below:\n\nWikipedia: Spatial database\n7 Spatial Databases for Your Enterprise\nGISGeography: Spatial Databases – Build Your Spatial Data Empire\nEsri: What is a geodatabase?\nIntroduction to PostGIS\nPostGEESE? Introducing The DuckDB Spatial Extension"
  },
  {
    "objectID": "W2_SQL.html#duckdb",
    "href": "W2_SQL.html#duckdb",
    "title": "2  Introduction to SQL",
    "section": "2.2 DuckDB",
    "text": "2.2 DuckDB\nDuckDB is an in-process SQL OLAP database management system. It is designed to be used as an embedded database in applications, but it can also be used as a standalone SQL database.\n\nIn-process SQL means that DuckDB’s features run in your application, not an external process to which your application connects. In other words: there is no client sending instructions nor a server to read and process them. SQLite works the same way, while PostgreSQL, MySQL…, do not.\nOLAP stands for OnLine Analytical Processing, and Microsoft defines it as a technology that organizes large business databases and supports complex analysis. It can be used to perform complex analytical queries without negatively affecting transactional systems.\n\nDuckDB is a great option if you’re looking for a serverless data analytics database management system."
  },
  {
    "objectID": "W6_RS.html#active-and-passive-sensors",
    "href": "W6_RS.html#active-and-passive-sensors",
    "title": "6  Remote Sensing",
    "section": "6.1 Active and Passive Sensors",
    "text": "6.1 Active and Passive Sensors\nRemote sensing is the science of obtaining information about an object or phenomenon without making physical contact with the object. Remote sensing can be done with various types of electromagnetic radiation such as visible, infrared, or microwave. The electromagnetic radiation is either emitted or reflected from the object being sensed. The reflected radiation is then collected by a sensor and processed to obtain information about the object.\n\nWhile most satellite imagery is optical, meaning it captures sunlight reflected by the earth’s surface, Synthetic Aperture Radar (SAR) satellites such as Sentinel-1 work by emitting pulses of radio waves and measuring how much of the signal is reflected back. This is similar to the way a bat uses sonar to “see” in the dark: by emitting calls and listening to echoes."
  },
  {
    "objectID": "W6_RS.html#resolution",
    "href": "W6_RS.html#resolution",
    "title": "6  Remote Sensing",
    "section": "6.2 Resolution",
    "text": "6.2 Resolution\nResolution is one of the most important attributes of satellite imagery. There are three types of resolution: spatial, spectral, and temporal. Let’s look at each of these.\n\n6.2.1 Spatial Resolution\nSpatial resolution governs how “sharp” an image looks. The Google Maps satellite basemap, for example, is really sharp Most of the optical imagery that is freely available has relatively low spatial resolution (it looks more grainy than, for example, the Google satellite basemap),\n  \n\n\n6.2.2 Spectral Resolution\nWhat open access imagery lacks in spatial resolution it often makes up for with spectral resolution. Really sharp imagery from MAXAR, for example, mostly collects light in the visible light spectrum, which is what our eyes can see. But there are other parts of the electromagnetic spectrum that we can’t see, but which can be very useful for distinguishing between different materials. Many satellites that have a lower spatial resolution than MAXAR, such as Landsat and Sentinel-2, collect data in a wider range of the electromagnetic spectrum.\nDifferent materials reflect light differently. An apple absorbs shorter wavelengths (e.g. blue and green), and reflects longer wavelengths (red). Our eyes use that information– the color– to distinguish between different objects. Below is a plot of the spectral profiles of different materials:\n\n\n\nThe visible portion of the spectrum is highlighted on the left, ranging from 400nm (violet) to 700nm (red). Our eyes (and satellite imagery in the visible light spectrum) can only see this portion of the light spectrum; we can’t see UV or infrared wavelengths, for example, though the extent to which different materials reflect or absorb these wavelengths is just as useful for distinguishing between them. The European Space Agency’s Sentinel-2 satellite collects spectral information well beyond the visible light spectrum, enabling this sort of analysis. It chops the electromagnetic spectrum up into “bands”, and measures how strongly wavelengths in each of those bands is reflected:\n\nTo illustrate why this is important, consider Astroturf (fake plastic grass). Astroturf and real grass will both look green to us, espeically from a satellite image. But living plants strongly reflect radiation from the sun in a part of the light spectrum that we can’t see (near-infrared). There’s a spectral index called the Normalized Difference Vegetation Index (NDVI) which exploits this fact to isolate vegetation in multispectral satellite imagery. So if we look at Gilette Stadium near Boston, we can tell that the three training fields south of the stadium are real grass (they generate high NDVI values, showing up red), while the pitch in the stadium itself is astroturf (generating low NDVI values, showing up blue).\n\n\n\nVHR image of Gilette Stadium with Sentinel-2 derived NDVI overlay\n\n\nIn other words, even though these fields are all green and indistinguishable to the human eye, their spectral profiles beyond the visible light spectrum differ, and we can use this information to distinguish between them.\nAstroturf is a trivial example. But suppose we were interested in identifying makeshift oil refineries in Northern Syria that constitute a key source of rents for whichever group controls them. As demonstrated in the ‘Refinery Identification’ case study, we can train an algorithm to identify the spectral signatures of oil, and use that to automatically detect them in satellite imagery.\n\n\n6.2.3 Temporal Resolution\nFinally, the frequency with which we can access new imagery is an important consideration. This is called the temporal resolution.\nThe Google Maps basemap is very high resolution, available globally, and is freely available. But it has no temporal dimension: it’s a snapshot from one particular point in time. If the thing we’re interested in involves changes over time, this basemap will be of limited use.\nThe “revisit rate” is the amount of time it takes for the satellite to pass over the same location twice. For example, the Sentinel-2 constellation’s two satellites can achieve a revisit rate of 5 days, as shown in this cool video from the European Space Agency:\n\nSome satellite constellations are able to achieve much higher revisit rates. Sentinel-2 has a revisit rate of 5 days, but SkySat capable of imaging the same point on earth around 12 times per day! How is that possible? Well, as the video above demonstrated, the Sentinel-2 constellation is composed of two satellites that share the same orbit, 180 degrees apart. In contrast, the SkySat constellation comprises 21 satellites, each with its own orbital path:\n\nThis allows SkySat to achieve a revisit rate of 2-3 hours. The catch, however, is that you need to pay for it (and it ain’t cheap). Below is a comparison of revisit rates for various other optical satellites:\n\n\n\nA chart of revisit times for different satellites from Sutlieff et. al.(2021)"
  },
  {
    "objectID": "W6_RS.html#summary",
    "href": "W6_RS.html#summary",
    "title": "6  Remote Sensing",
    "section": "6.3 Summary",
    "text": "6.3 Summary\nYou should hopefully have a better understanding of what satellite imagery is, and how it can be used to answer questions about the world. In the next section, we’ll look at the various types of satellite imagery stored in the Google Earth Engine catalogue."
  },
  {
    "objectID": "W3_postgis1.html",
    "href": "W3_postgis1.html",
    "title": "3  Spatial SQL I",
    "section": "",
    "text": "4 Geometry Exercises\nHere's a reminder of all the functions we have seen so far. They should be useful for the exercises!\nAlso remember the tables we have available:\nHere's a reminder of the functions we saw in the last section. They should be useful for the exercises!\nAlso remember the tables we have available:"
  },
  {
    "objectID": "W3_postgis1.html#introduction",
    "href": "W3_postgis1.html#introduction",
    "title": "3  Spatial SQL I",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this lesson, you will learn how to use SQL to query spatial data. You will learn how to use the duckdb Python library to connect to a DuckDB database and run SQL queries. You will also learn how to use the leafmap Python library to visualize spatial data."
  },
  {
    "objectID": "W3_postgis1.html#learning-objectives",
    "href": "W3_postgis1.html#learning-objectives",
    "title": "3  Spatial SQL I",
    "section": "3.2 Learning Objectives",
    "text": "3.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nConnect to a DuckDB database using Python\nRun SQL queries to query spatial data\nVisualize spatial data using Leafmap\nRun spatial SQL queries using PgAdmin"
  },
  {
    "objectID": "W3_postgis1.html#materials",
    "href": "W3_postgis1.html#materials",
    "title": "3  Spatial SQL I",
    "section": "3.3 Materials",
    "text": "3.3 Materials\nTo get started, work your way through the following two notebooks:\n\nGeometries\nSpatial Relationships\n\nClicking the link will take you to Google Colab, where you can run the notebook in your browser. These workbooks use the duckdb Python library to connect to a DuckDB database and run SQL queries. They also use the leafmap Python library to visualize spatial data. Most of the spatial functions and syntax are the same or very similar to their PostGIS equivalents.\nOnce you’ve completed these you can test your knowledge by answering the questions in the lab below. To complete this lab, open PgAdmin and connect to the nyc database. Then, open a new query window and write your SQL queries there."
  },
  {
    "objectID": "W4_postgis2.html",
    "href": "W4_postgis2.html",
    "title": "4  Spatial SQL II",
    "section": "",
    "text": "5 Spatial Joins Exercises\nHere's a reminder of some of the functions we have seen. Hint: they should be useful for the exercises!\nAlso remember the tables we have available:\nNow for a less structured exercise. We’re going to look at ship-to-ship transfers. The idea is that two ships meet up in the middle of the ocean, and one ship transfers cargo to the other. This is a common way to avoid sanctions, and is often used to transfer oil from sanctioned countries to other countries. We’re going to look at a few different ways to detect these transfers using AIS data."
  },
  {
    "objectID": "W4_postgis2.html#setup",
    "href": "W4_postgis2.html#setup",
    "title": "4  PostGIS II",
    "section": "5.1 Setup",
    "text": "5.1 Setup\nUncomment and run the following cell to install the required packages.\nnidbblocduu ipython3 # %pip install duckdb leafmap lonboard\nnidbblocduu ipython3 import duckdb import leafmap"
  },
  {
    "objectID": "W4_postgis2.html#question-1",
    "href": "W4_postgis2.html#question-1",
    "title": "4  PostGIS II",
    "section": "5.2 Question 1",
    "text": "5.2 Question 1\nDownload the nyc_data.zip dataset using leafmap. The zip file contains the following datasets. Create a new DuckDB database and import the datasets into the database. Each dataset should be imported into a separate table.\n\nnyc_census_blocks\nnyc_homicides\nnyc_neighborhoods\nnyc_streets\nnyc_subway_stations\n\nnidbblocduu ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-2",
    "href": "W4_postgis2.html#question-2",
    "title": "4  PostGIS II",
    "section": "5.3 Question 2",
    "text": "5.3 Question 2\n+++\nVisualize the nyc_subway_stations and nyc_streets datasets on the same map using leafmap and lonboard.\nnidbblocduu ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-3",
    "href": "W4_postgis2.html#question-3",
    "title": "4  PostGIS II",
    "section": "5.4 Question 3",
    "text": "5.4 Question 3\nFind out what neighborhood the BLUE subway stations are in.\nwgkruzvfmij ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-4",
    "href": "W4_postgis2.html#question-4",
    "title": "4  PostGIS II",
    "section": "5.5 Question 4",
    "text": "5.5 Question 4\nFind out what streets are within 200 meters of the BLUE subway stations.\nwgkruzvfmij ipython3 # Add your code here"
  },
  {
    "objectID": "W4_postgis2.html#question-5",
    "href": "W4_postgis2.html#question-5",
    "title": "4  PostGIS II",
    "section": "5.6 Question 5",
    "text": "5.6 Question 5\nVisualize the BLUE subway stations and the streets within 200 meters of the BLUE subway stations on the same map using leafmap and lonboard.\nwgkruzvfmij ipython3 # Add your code here"
  },
  {
    "objectID": "W3_postgis1.html#exercises",
    "href": "W3_postgis1.html#exercises",
    "title": "3  Spatial SQL I",
    "section": "4.1 Exercises",
    "text": "4.1 Exercises\n\nWhat is the area of the 'West Village' neighborhood? (Hint: The area is given in square meters. To get an area in hectares, divide by 10000. To get an area in acres, divide by 4047.)\nWhat is the geometry type of ‘Pelham St’? The length?\nWhat is the GeoJSON representation of the 'Broad St' subway station?\nWhat is the total length of streets (in kilometers) in New York City? (Hint: The units of measurement of the spatial data are meters, there are 1000 meters in a kilometer.)\nWhat is the area of Manhattan in acres? (Hint: both nyc_census_blocks and nyc_neighborhoods have a boroname in them.)\nWhat is the most westerly subway station?\nHow long is 'Columbus Cir' (aka Columbus Circle)?\nWhat is the length of streets in New York City, summarized by type?\n\nAnswers (only check after you’ve given it your best shot!)"
  },
  {
    "objectID": "W3_postgis1.html#exercises-1",
    "href": "W3_postgis1.html#exercises-1",
    "title": "3  Spatial SQL I",
    "section": "5.1 Exercises",
    "text": "5.1 Exercises\n\nWhat is the geometry value for the street named 'Atlantic Commons'?\nWhat neighborhood and borough is Atlantic Commons in?\nWhat streets does Atlantic Commons join with?\nApproximately how many people live on (within 50 meters of) Atlantic Commons?\n\nAnswers (only check after you’ve given it your best shot!)"
  },
  {
    "objectID": "W4_postgis2.html#step-1",
    "href": "W4_postgis2.html#step-1",
    "title": "4  Spatial SQL II",
    "section": "7.1 Step 1",
    "text": "7.1 Step 1\nCreate a spatial database using the following AIS data:\nhttps://storage.googleapis.com/qm2/casa0025_ships.csv\nEach row in this dataset is an AIS ‘ping’ indicating the position of a ship at a particular date/time, alongside vessel-level characteristics.\nIt contains the following columns:\n\nvesselid: A unique numerical identifier for each ship, like a license plate\nvessel_name: The ship’s name\nvsl_descr: The ship’s type\ndwt: The ship’s Deadweight Tonnage (how many tons it can carry)\nv_length: The ship’s length in meters\ndraught: How many meters deep the ship is draughting (how low it sits in the water). Effectively indicates how much cargo the ship is carrying\nsog: Speed over Ground (in knots)\ndate: A timestamp for the AIS signal\nlat: The latitude of the AIS signal (EPSG:4326)\nlon: The longitude of the AIS signal (EPSG:4326)\n\nCreate a table called ‘ais’ where each row is a different AIS ping, with no superfluous information. Construct a geometry column.\nCreate a second table called ‘vinfo’ which contains vessel-level information with no superfluous information."
  },
  {
    "objectID": "W4_postgis2.html#step-2",
    "href": "W4_postgis2.html#step-2",
    "title": "4  Spatial SQL II",
    "section": "7.2 Step 2",
    "text": "7.2 Step 2\nUse a spatial join to identify ship-to-ship transfers in this dataset. Two ships are considered to be conducting a ship to ship transfer IF:\n\nThey are within 500 meters of each other\nFor more than two hours\nAnd their speed is lower than 1 knot\n\nSome things to consider: make sure you’re not joining ships with themselves. Try working with subsets of the data first while you try different things out."
  },
  {
    "objectID": "W7_RS_data.html#optical-imagery",
    "href": "W7_RS_data.html#optical-imagery",
    "title": "7  Data Acquisition",
    "section": "7.1 Optical Imagery",
    "text": "7.1 Optical Imagery\n\n\n\nAutomatic detection of vehicles using artificial intelligence in high resolution optical imagery. See the object detection tutorial.\n\n\nOptical satellite imagery is the bread and butter of many open source investiagtions. It would be tough to list off all of the possible use cases, so here’s a handy flowchart:\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#FFFFFF' ,'primaryBorderColor':'#000000' , 'lineColor':'#009933'}}}%%\n\nflowchart\n  A(Does it happen outside?) \n  A--&gt; B(Yes)\n  A--&gt; C(No)\n  D(Is it very small?)\n  B--&gt;D\n  E(Yes)\n  F(No)\n    D--&gt;F\n    D--&gt;E\nG(Use optical satellite imagery)\nH(Don't use optical satellite imagery)\nE--&gt;H\nF--&gt;G\nC--&gt;H\n\n\n\n\n\nThis is, of course, a bit of an exaggeration. But if you’re interested in a visible phenomenon that happens outdoors and that isn’t very tiny, chances are an earth-observing satellite has taken a picture of it. What that picture can tell you naturally depends on what you’re interested in learning. For a deeper dive into analyzing optical satellite imagery, see the subsection on multispectral remote sensing..\nThere are several different types of optical satellite imagery available in the GEE catalogue. The main collections are the Landsat and Sentinel series of satellites, which are operated by NASA and the European Space Agency, respectively. Landsat satellites have been in orbit since 1972, and Sentinel satellites have been in orbit since 2015. Norway’s International Climate and Forest Initiative (NICFI) has also contributed to the GEE catalogue by providing a collection of optical imagery from Planet’s PlanetScope satellites. These are higher resolution (4.7 meters per pixel) than Landsat (30m/px) and Sentinel-2 (10m/px), but are only available for the tropics. Even higher resolution imagery (60cm/px) is available from the GEE catalogue from the National Agriculture Imagery Program, but it is only available for the United States. For more details, see the “Datasets” section below.\n\nApplications\n\nGeolocating pictures\n\nSome of Bellingcat’s earliest work involved figuring out where a picture was taken by cross-referencing it with optical satellite imagery.\n\nGeneral surveillance\n\nMonitoring Chinese missile silo construction.\nAmassing evidence of genocide in Bucha, Ukraine\n\nDamage detection\n\nUkraine\nMali\nAround the World\n\nVerifying the locations of artillery/missile/drone strikes\n\nThe 2019 attack on Saudi Arabia’s Abqaiq oil processing facility.\n\nMonitoring illegal mining/logging\n\nGlobal Witness investigation into illegal mining by militias in Myanmar.\nTracking illegal logging across the world.\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nLandsat 1-5\n1972–1999\n30m\nGlobal\n\n\nLandsat 7\n1999–2021\n30m\nGlobal\n\n\nLandsat 8\n2013–Present\n30m\nGlobal\n\n\nLandsat 9\n2021–Present\n30m\nGlobal\n\n\nSentinel-2\n2015–Present\n10m\nGlobal\n\n\nNICFI\n2015-Present\n4.7m\nTropics\n\n\nNAIP\n2002-2021\n0.6m\nUSA"
  },
  {
    "objectID": "W7_RS_data.html#radar-imagery",
    "href": "W7_RS_data.html#radar-imagery",
    "title": "7  Data Acquisition",
    "section": "7.2 Radar Imagery",
    "text": "7.2 Radar Imagery\n\n\n\nShips and interference from a radar system are visible in Zhuanghe Wan, near North Korea.\n\n\nSynthetic Aperture Radar imagery (SAR) is a type of remote sensing that uses radio waves to detect objects on the ground. SAR imagery is useful for detecting objects that are small, or that are obscured by clouds or other weather phenomena. SAR imagery is also useful for detecting objects that are moving, such as ships or cars.\n\nApplications\n\nChange/Damage detection\nTracking military radar systems\nMaritime surveillance\nMonitoring illegal mining/logging\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nSentinel 1\n2014-Present\n10m\nGlobal"
  },
  {
    "objectID": "W7_RS_data.html#nighttime-lights",
    "href": "W7_RS_data.html#nighttime-lights",
    "title": "7  Data Acquisition",
    "section": "7.3 Nighttime Lights",
    "text": "7.3 Nighttime Lights\n\n\n\nA timelapse of nighttime lights over Northern Iraq showing the capture and liberation of Mosul by ISIS.\n\n\nSatellite images of the Earth at night a useful proxy for human activity. The brightness of a given area at night is a function of the number of people living there and the nature of their activities. The effects of conflict, natural disasters, and economic development can all be inferred from changes in nighttime lights.\nThe timelapse above reveals a number of interesting things: The capture of Mosul by ISIS in 2014 and the destruction of its infrastructure during the fighting (shown as the city darkening), as well as the liberation of the city by the Iraqi military in 2017 are all visible in nighttime lights. The code to create this gif, as well as a more in-depth tutorial on the uses of nighttime lights, can be found in the “War at Night” case study.\n\nApplications\n\nDamage detection\nIdentifying gas flaring/oil production\nIdentifying urban areas/military bases illuminated at night\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nDMSP-OLS\n1992-2014\n927m\nGlobal\n\n\nVIIRS\n2014-Present\n463m\nGlobal"
  },
  {
    "objectID": "W7_RS_data.html#climate-and-atmospheric-data",
    "href": "W7_RS_data.html#climate-and-atmospheric-data",
    "title": "7  Data Acquisition",
    "section": "7.4 Climate and Atmospheric Data",
    "text": "7.4 Climate and Atmospheric Data\n\n\n\nSulphur Dioxide plume resulting from ISIS attack on the Al-Mishraq Sulphur Plant in Iraq\n\n\nClimate and atmospheric data can be used to track the effects of conflict on the environment. The European Space Agency’s Sentinel-5p satellites measure the concentration of a number of atmospheric gases, including nitrogen dioxide, methane, and ozone. Measurements are available on a daily basis at a fairly high resolution (1km), allowing for the detection of localized sources of pollution such as oil refineries or power plants. For example, see this Bellingcat article in which Wim Zwijnenburg and I trace pollution to specific facilities operated by multinational oil companies in Iraq.\nThe Copernicus Atmosphere Monitoring Service (CAMS) provides similar data at a lower spatial resolution (45km), but measurements are avaialble on an hourly basis. The timelapse above utilizes CAMS data to show a sulphur dioxide plume resulting from an ISIS attack on the Al-Mishraq Sulphur Plant in Iraq. The plant was used to produce sulphuric acid, for use in fertilizers and pesticides. The attack destroyed the plant, causing a fire which burned for a month and released 21 kilotons of sulphur dioxide into the atmosphere per day; the largest human-made release of sulphur dioxide in history.\n\nApplications\n\nMonitoring of airborne pollution\nTracing pollution back to specific facilities and companies\nVisualizing the effects of one-off environmental catastrophes\n\nNordstream 1 leak\nISIS setting Mishraq sulphur plant on fire\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nCAMS NRT\n2016-Present\n44528m\nGlobal\n\n\nSentinel-5p\n2018-Present\n1113m\nGlobal"
  },
  {
    "objectID": "W7_RS_data.html#mineral-deposits",
    "href": "W7_RS_data.html#mineral-deposits",
    "title": "7  Data Acquisition",
    "section": "7.5 Mineral Deposits",
    "text": "7.5 Mineral Deposits\n\n\n\nZinc deposits across Central Africa\n\n\nMining activities often play an important role in conflict. According to an influential study, “the historical rise in mineral prices might explain up to one-fourth of the average level of violence across African countries” between 1997 and 2010. Data on the location of mineral deposits can be used to identify areas where mining activities are likely to be taking place, and several such datasets are available in Google Earth Engine.\n\nApplications\n\nMonitoring mining activity\nIdentifying areas where mining activities are likely to be taking place\nMapping the distribution of resources in rebel held areas in conflicts fueled by resource extraction\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\niSDA\n2001-2017\n30m\nAfrica"
  },
  {
    "objectID": "W7_RS_data.html#fires",
    "href": "W7_RS_data.html#fires",
    "title": "7  Data Acquisition",
    "section": "7.6 Fires",
    "text": "7.6 Fires\n\n\n\nDetected fires over Ukraine since 27/02/2022 showing the frontline of the war\n\n\nEarth-observing satellites can detect “thermal anomalies” (fires) from space. NASA’s Fire Information for Resource Management System (FIRMS) provides daily data on active fires in near real time, going back to the year 2000. Carlos Gonzales wrote a comprehensive Bellingcat article on the use of FIRMS to monitor war zones from Ukraine to Ethiopia. The map above shows that FIRMS detected fires over Eastern Ukraine trace the frontline of the war.\nFIRMS data are derived from the MODIS satellite, but only show the central location and intensity of a detected fire. Another MODIS product (linked in the table below) generates a monthly map of burned areas, which can be used to assess the spatial extent of fires.\n\nApplications\n\nIdentification of possible artillery strikes/fighting in places like Ukraine\nEnvironmental warfare and “scorched earth” policies\nLarge scale arson\n\ne.g. Refugee camps burned down in Myanmar\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nFIRMS\n2000-Present\n1000m\nGlobal\n\n\nMODIS Burned Area\n2000-Present\n500m\nGlobal"
  },
  {
    "objectID": "W7_RS_data.html#population-density-estimates",
    "href": "W7_RS_data.html#population-density-estimates",
    "title": "7  Data Acquisition",
    "section": "7.7 Population Density Estimates",
    "text": "7.7 Population Density Estimates\n\n\n\nPopulation density estimates around Pyongyang, North Korea\n\n\nSometimes, we may want to get an estimate the population in a specific area to ballpark how many people might be affected by a natural disaster, a counteroffensive, or a missile strike. You can’t really google “what is the population in this rectangle i’ve drawn in Northeastern Syria?” and get a good answer. Luckily, there are several spatial population datasets hosted in GEE that let you do just that. Some, such as WorldPop, provide estimated breakdowns by age and sex as well. However, it is extremely important to bear in mind that these are estimates, and will not take into account things like conflict-induced displacement. For example, Oak Ridge National Laboratory’s LandScan program has released high-resolution population data for Ukraine, but this pertains to the pre-war population distribution. The war has radically changed this distribution, so these estimates no longer reflect where people are. Still, this dataset could be used to roughly estimate displacement or the number of people who will need new housing.\n\nApplications:\n\nRough estimates of civilians at risk from conflict or disaster, provided at a high spatial resolution\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nWorldpop\n2000-2021\n92m\nGlobal\n\n\nGPW\n2000-2021\n927m\nGlobal\n\n\nLandScan\n2013–Present\n100m\nUkraine"
  },
  {
    "objectID": "W7_RS_data.html#building-footprints",
    "href": "W7_RS_data.html#building-footprints",
    "title": "7  Data Acquisition",
    "section": "7.8 Building Footprints",
    "text": "7.8 Building Footprints\n\n\n\nBuilding footprints in Mariupol, Ukraine colored by whether the building is damaged\n\n\nA building footprint dataset contains the two dimensional outlines of buildings in a given area. Currently, GEE hosts one building footprint dataset which covers all of Africa. In 2022, Microsoft released a free global building footprint dataset, though to use it in Earth Engine you’ll have to download it from their GitHub page and upload it manually to GEE. The same goes for OpenStreetMap (OSM), a public database of building footprints, roads, and other features that also contains useful annotations for many buildings indicating their use. Benjamin Strick has a great youtube video on conducting investigations using OSM data.\n\nApplications:\n\nJoining damage estimate data with the number of buildings in an area\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nOpen Buildings\n2022\nAfrica"
  },
  {
    "objectID": "W7_RS_data.html#administrative-boundaries",
    "href": "W7_RS_data.html#administrative-boundaries",
    "title": "7  Data Acquisition",
    "section": "7.9 Administrative Boundaries",
    "text": "7.9 Administrative Boundaries\n\n\n\nSecond-level administrative boundaries in Yemen\n\n\nSpatial analysis often have to aggregate information over a defined area; we may want to assess the total burned area by province in Ukraine, or count the number of Saudi airstrikes by district in Yemen. For that, we need data on these administrative boundaries. GEE hosts several such datasets at the country, province, and district (or equivalent) level.\n\nApplications\n\nQuick spatial calculations for different provinces/districts in a country\n\ne.g. counts of conflict events by district over time\n\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nFAO GAUL\n2015\nGlobal"
  },
  {
    "objectID": "W7_RS_data.html#global-power-plant-database",
    "href": "W7_RS_data.html#global-power-plant-database",
    "title": "7  Data Acquisition",
    "section": "7.10 Global Power Plant Database",
    "text": "7.10 Global Power Plant Database\n\n\n\nPower plants in Ukraine colored by type\n\n\nThe Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights. Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. As of June 2018, the database includes around 28,500 power plants from 164 countries. The database is curated by the World Resources Institude (WRI).\n\nApplications:\n\nAnalyzing the impact of conflict on critical infrastructure.\n\ne.g. fighting in Ukraine taking place around nuclear power facilities.\n\nCould be combined with the atmospheric measurements of different pollutants and the population estimates data to assess the impact of various forms of energy generation on air quality and public health.\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nGPPD\n2018\nGlobal"
  },
  {
    "objectID": "W6_RS.html",
    "href": "W6_RS.html",
    "title": "6  Remote Sensing",
    "section": "",
    "text": "7 Data Acquisition\nOne of the main advantages of GEE is that it hosts several Petabytes of satellite imagery and other spatial data sets, all in one place. Among these are a many that could prove useful to those investigating illegal mining and logging, estimating conflict-induced damage, monitoring pollution from extractive industries, conducting maritime surveillance without relying on ship transponders, and many other topics.\nThis section highlights ten categories of geospatial data available natively in the GEE catalogue ranging from optical satellite imagery, to atmospheric data, to building footprints. Each sub-section provides an overview of the given data type, suggests potential applications, and lists the corresponding datasets in the GEE catalogue. The datasets listed under each heading are not an exhaustive list– there are over 500 in the whole catalogue, and the ones listed in this section are simply the ones with the most immediate relevance to open source investigations. If a particular geospatial dataset you want to work with isn’t hosted in the GEE catalog, you can upload your own data. We’ll cover that in the next section."
  },
  {
    "objectID": "W6_RS.html#optical-imagery",
    "href": "W6_RS.html#optical-imagery",
    "title": "6  Remote Sensing",
    "section": "7.1 Optical Imagery",
    "text": "7.1 Optical Imagery\n\n\n\nAutomatic detection of vehicles using artificial intelligence in high resolution optical imagery. See the object detection tutorial.\n\n\nOptical satellite imagery is the bread and butter of many open source investiagtions. It would be tough to list off all of the possible use cases, so here’s a handy flowchart:\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#FFFFFF' ,'primaryBorderColor':'#000000' , 'lineColor':'#009933'}}}%%\n\nflowchart\n  A(Does it happen outside?) \n  A--&gt; B(Yes)\n  A--&gt; C(No)\n  D(Is it very small?)\n  B--&gt;D\n  E(Yes)\n  F(No)\n    D--&gt;F\n    D--&gt;E\nG(Use optical satellite imagery)\nH(Don't use optical satellite imagery)\nE--&gt;H\nF--&gt;G\nC--&gt;H\n\n\n\n\n\nThis is, of course, a bit of an exaggeration. But if you’re interested in a visible phenomenon that happens outdoors and that isn’t very tiny, chances are an earth-observing satellite has taken a picture of it. What that picture can tell you naturally depends on what you’re interested in learning. For a deeper dive into analyzing optical satellite imagery, see the subsection on multispectral remote sensing..\nThere are several different types of optical satellite imagery available in the GEE catalogue. The main collections are the Landsat and Sentinel series of satellites, which are operated by NASA and the European Space Agency, respectively. Landsat satellites have been in orbit since 1972, and Sentinel satellites have been in orbit since 2015. Norway’s International Climate and Forest Initiative (NICFI) has also contributed to the GEE catalogue by providing a collection of optical imagery from Planet’s PlanetScope satellites. These are higher resolution (4.7 meters per pixel) than Landsat (30m/px) and Sentinel-2 (10m/px), but are only available for the tropics. Even higher resolution imagery (60cm/px) is available from the GEE catalogue from the National Agriculture Imagery Program, but it is only available for the United States. For more details, see the “Datasets” section below.\n\nApplications\n\nGeolocating pictures\n\nSome of Bellingcat’s earliest work involved figuring out where a picture was taken by cross-referencing it with optical satellite imagery.\n\nGeneral surveillance\n\nMonitoring Chinese missile silo construction.\nAmassing evidence of genocide in Bucha, Ukraine\n\nDamage detection\n\nUkraine\nMali\nAround the World\n\nVerifying the locations of artillery/missile/drone strikes\n\nThe 2019 attack on Saudi Arabia’s Abqaiq oil processing facility.\n\nMonitoring illegal mining/logging\n\nGlobal Witness investigation into illegal mining by militias in Myanmar.\nTracking illegal logging across the world.\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nLandsat 1-5\n1972–1999\n30m\nGlobal\n\n\nLandsat 7\n1999–2021\n30m\nGlobal\n\n\nLandsat 8\n2013–Present\n30m\nGlobal\n\n\nLandsat 9\n2021–Present\n30m\nGlobal\n\n\nSentinel-2\n2015–Present\n10m\nGlobal\n\n\nNICFI\n2015-Present\n4.7m\nTropics\n\n\nNAIP\n2002-2021\n0.6m\nUSA"
  },
  {
    "objectID": "W6_RS.html#radar-imagery",
    "href": "W6_RS.html#radar-imagery",
    "title": "6  Remote Sensing",
    "section": "7.2 Radar Imagery",
    "text": "7.2 Radar Imagery\n\n\n\nShips and interference from a radar system are visible in Zhuanghe Wan, near North Korea.\n\n\nSynthetic Aperture Radar imagery (SAR) is a type of remote sensing that uses radio waves to detect objects on the ground. SAR imagery is useful for detecting objects that are small, or that are obscured by clouds or other weather phenomena. SAR imagery is also useful for detecting objects that are moving, such as ships or cars.\n\nApplications\n\nChange/Damage detection\nTracking military radar systems\nMaritime surveillance\nMonitoring illegal mining/logging\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nSentinel 1\n2014-Present\n10m\nGlobal"
  },
  {
    "objectID": "W6_RS.html#nighttime-lights",
    "href": "W6_RS.html#nighttime-lights",
    "title": "6  Remote Sensing",
    "section": "7.3 Nighttime Lights",
    "text": "7.3 Nighttime Lights\n\n\n\nA timelapse of nighttime lights over Northern Iraq showing the capture and liberation of Mosul by ISIS.\n\n\nSatellite images of the Earth at night a useful proxy for human activity. The brightness of a given area at night is a function of the number of people living there and the nature of their activities. The effects of conflict, natural disasters, and economic development can all be inferred from changes in nighttime lights.\nThe timelapse above reveals a number of interesting things: The capture of Mosul by ISIS in 2014 and the destruction of its infrastructure during the fighting (shown as the city darkening), as well as the liberation of the city by the Iraqi military in 2017 are all visible in nighttime lights. The code to create this gif, as well as a more in-depth tutorial on the uses of nighttime lights, can be found in the “War at Night” case study.\n\nApplications\n\nDamage detection\nIdentifying gas flaring/oil production\nIdentifying urban areas/military bases illuminated at night\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nDMSP-OLS\n1992-2014\n927m\nGlobal\n\n\nVIIRS\n2014-Present\n463m\nGlobal"
  },
  {
    "objectID": "W6_RS.html#climate-and-atmospheric-data",
    "href": "W6_RS.html#climate-and-atmospheric-data",
    "title": "6  Remote Sensing",
    "section": "7.4 Climate and Atmospheric Data",
    "text": "7.4 Climate and Atmospheric Data\n\n\n\nSulphur Dioxide plume resulting from ISIS attack on the Al-Mishraq Sulphur Plant in Iraq\n\n\nClimate and atmospheric data can be used to track the effects of conflict on the environment. The European Space Agency’s Sentinel-5p satellites measure the concentration of a number of atmospheric gases, including nitrogen dioxide, methane, and ozone. Measurements are available on a daily basis at a fairly high resolution (1km), allowing for the detection of localized sources of pollution such as oil refineries or power plants. For example, see this Bellingcat article in which Wim Zwijnenburg and I trace pollution to specific facilities operated by multinational oil companies in Iraq.\nThe Copernicus Atmosphere Monitoring Service (CAMS) provides similar data at a lower spatial resolution (45km), but measurements are avaialble on an hourly basis. The timelapse above utilizes CAMS data to show a sulphur dioxide plume resulting from an ISIS attack on the Al-Mishraq Sulphur Plant in Iraq. The plant was used to produce sulphuric acid, for use in fertilizers and pesticides. The attack destroyed the plant, causing a fire which burned for a month and released 21 kilotons of sulphur dioxide into the atmosphere per day; the largest human-made release of sulphur dioxide in history.\n\nApplications\n\nMonitoring of airborne pollution\nTracing pollution back to specific facilities and companies\nVisualizing the effects of one-off environmental catastrophes\n\nNordstream 1 leak\nISIS setting Mishraq sulphur plant on fire\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nCAMS NRT\n2016-Present\n44528m\nGlobal\n\n\nSentinel-5p\n2018-Present\n1113m\nGlobal"
  },
  {
    "objectID": "W6_RS.html#mineral-deposits",
    "href": "W6_RS.html#mineral-deposits",
    "title": "6  Remote Sensing",
    "section": "7.5 Mineral Deposits",
    "text": "7.5 Mineral Deposits\n\n\n\nZinc deposits across Central Africa\n\n\nMining activities often play an important role in conflict. According to an influential study, “the historical rise in mineral prices might explain up to one-fourth of the average level of violence across African countries” between 1997 and 2010. Data on the location of mineral deposits can be used to identify areas where mining activities are likely to be taking place, and several such datasets are available in Google Earth Engine.\n\nApplications\n\nMonitoring mining activity\nIdentifying areas where mining activities are likely to be taking place\nMapping the distribution of resources in rebel held areas in conflicts fueled by resource extraction\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\niSDA\n2001-2017\n30m\nAfrica"
  },
  {
    "objectID": "W6_RS.html#fires",
    "href": "W6_RS.html#fires",
    "title": "6  Remote Sensing",
    "section": "7.6 Fires",
    "text": "7.6 Fires\n\n\n\nDetected fires over Ukraine since 27/02/2022 showing the frontline of the war\n\n\nEarth-observing satellites can detect “thermal anomalies” (fires) from space. NASA’s Fire Information for Resource Management System (FIRMS) provides daily data on active fires in near real time, going back to the year 2000. Carlos Gonzales wrote a comprehensive Bellingcat article on the use of FIRMS to monitor war zones from Ukraine to Ethiopia. The map above shows that FIRMS detected fires over Eastern Ukraine trace the frontline of the war.\nFIRMS data are derived from the MODIS satellite, but only show the central location and intensity of a detected fire. Another MODIS product (linked in the table below) generates a monthly map of burned areas, which can be used to assess the spatial extent of fires.\n\nApplications\n\nIdentification of possible artillery strikes/fighting in places like Ukraine\nEnvironmental warfare and “scorched earth” policies\nLarge scale arson\n\ne.g. Refugee camps burned down in Myanmar\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nFIRMS\n2000-Present\n1000m\nGlobal\n\n\nMODIS Burned Area\n2000-Present\n500m\nGlobal"
  },
  {
    "objectID": "W6_RS.html#population-density-estimates",
    "href": "W6_RS.html#population-density-estimates",
    "title": "6  Remote Sensing",
    "section": "7.7 Population Density Estimates",
    "text": "7.7 Population Density Estimates\n\n\n\nPopulation density estimates around Pyongyang, North Korea\n\n\nSometimes, we may want to get an estimate the population in a specific area to ballpark how many people might be affected by a natural disaster, a counteroffensive, or a missile strike. You can’t really google “what is the population in this rectangle i’ve drawn in Northeastern Syria?” and get a good answer. Luckily, there are several spatial population datasets hosted in GEE that let you do just that. Some, such as WorldPop, provide estimated breakdowns by age and sex as well. However, it is extremely important to bear in mind that these are estimates, and will not take into account things like conflict-induced displacement. For example, Oak Ridge National Laboratory’s LandScan program has released high-resolution population data for Ukraine, but this pertains to the pre-war population distribution. The war has radically changed this distribution, so these estimates no longer reflect where people are. Still, this dataset could be used to roughly estimate displacement or the number of people who will need new housing.\n\nApplications:\n\nRough estimates of civilians at risk from conflict or disaster, provided at a high spatial resolution\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nWorldpop\n2000-2021\n92m\nGlobal\n\n\nGPW\n2000-2021\n927m\nGlobal\n\n\nLandScan\n2013–Present\n100m\nUkraine"
  },
  {
    "objectID": "W6_RS.html#building-footprints",
    "href": "W6_RS.html#building-footprints",
    "title": "6  Remote Sensing",
    "section": "7.8 Building Footprints",
    "text": "7.8 Building Footprints\n\n\n\nBuilding footprints in Mariupol, Ukraine colored by whether the building is damaged\n\n\nA building footprint dataset contains the two dimensional outlines of buildings in a given area. Currently, GEE hosts one building footprint dataset which covers all of Africa. In 2022, Microsoft released a free global building footprint dataset, though to use it in Earth Engine you’ll have to download it from their GitHub page and upload it manually to GEE. The same goes for OpenStreetMap (OSM), a public database of building footprints, roads, and other features that also contains useful annotations for many buildings indicating their use. Benjamin Strick has a great youtube video on conducting investigations using OSM data.\n\nApplications:\n\nJoining damage estimate data with the number of buildings in an area\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nOpen Buildings\n2022\nAfrica"
  },
  {
    "objectID": "W6_RS.html#administrative-boundaries",
    "href": "W6_RS.html#administrative-boundaries",
    "title": "6  Remote Sensing",
    "section": "7.9 Administrative Boundaries",
    "text": "7.9 Administrative Boundaries\n\n\n\nSecond-level administrative boundaries in Yemen\n\n\nSpatial analysis often have to aggregate information over a defined area; we may want to assess the total burned area by province in Ukraine, or count the number of Saudi airstrikes by district in Yemen. For that, we need data on these administrative boundaries. GEE hosts several such datasets at the country, province, and district (or equivalent) level.\n\nApplications\n\nQuick spatial calculations for different provinces/districts in a country\n\ne.g. counts of conflict events by district over time\n\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nFAO GAUL\n2015\nGlobal"
  },
  {
    "objectID": "W6_RS.html#global-power-plant-database",
    "href": "W6_RS.html#global-power-plant-database",
    "title": "6  Remote Sensing",
    "section": "7.10 Global Power Plant Database",
    "text": "7.10 Global Power Plant Database\n\n\n\nPower plants in Ukraine colored by type\n\n\nThe Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights. Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. As of June 2018, the database includes around 28,500 power plants from 164 countries. The database is curated by the World Resources Institude (WRI).\n\nApplications:\n\nAnalyzing the impact of conflict on critical infrastructure.\n\ne.g. fighting in Ukraine taking place around nuclear power facilities.\n\nCould be combined with the atmospheric measurements of different pollutants and the population estimates data to assess the impact of various forms of energy generation on air quality and public health.\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nGPPD\n2018\nGlobal"
  },
  {
    "objectID": "F1.html#programming-basics",
    "href": "F1.html#programming-basics",
    "title": "11  Getting Started",
    "section": "11.1 Programming Basics",
    "text": "11.1 Programming Basics\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthor\nUjaval Gandhi\n\n\nOverview\nThis chapter introduces the Google Earth Engine application programming interface (API) and the JavaScript syntax needed to use it. You will learn about the Code Editor environment and get comfortable typing, running, and saving scripts. You will also learn the basics of JavaScript language, such as variables, data structures, and functions.\n\n\nLearning Outcomes\n\nFamiliarity with the Earth Engine Code Editor.\nFamiliarity with the JavaScript syntax.\nAbility to use the Earth Engine API functions from the Code Editor.\n\n\n\nAssumes you know how to:\n\nSign up for an Earth Engine account (See the Google documentation for details).\nAccess the Earth Engine Code Editor (See the Google documentation for details).\n\n\n\n\n\nIntroduction\nIn order to use Earth Engine well, you will need to develop basic skills in remote sensing and programming. The language of this book is JavaScript, and you will begin by learning how to manipulate variables using it. With that base, you’ll learn about viewing individual satellite images, viewing collections of images in Earth Engine, and how common remote sensing terms are referenced and used in Earth Engine.\nGoogle Earth Engine is a cloud-based platform for scientific data analysis. It provides ready-to-use, cloud-hosted datasets and a large pool of servers. One feature that makes Earth Engine particularly attractive is the ability to run large computations very fast by distributing them across a large pool of servers. The ability to efficiently use cloud-hosted datasets and computation is enabled by the Earth Engine API.\nAn API is a way to communicate with Earth Engine servers. It allows you to specify what computation you would like to do, and then to receive the results. The API is designed so that users do not need to worry about how the computation is distributed across a cluster of machines and the results are assembled. Users of the API simply specify what needs to be done. This greatly simplifies the code by hiding the implementation detail from the users. It also makes Earth Engine very approachable for users who are not familiar with writing code.\nThe Earth Engine platform comes with a web-based Code Editor that allows you to start using the Earth Engine JavaScript API without any installation. It also provides additional functionality to display your results on a map, save your scripts, access documentation, manage tasks, and more. It has a one-click mechanism to share your code with other users—allowing for easy reproducibility and collaboration. In addition, the JavaScript API comes with a user interface library, which allows you to create charts and web-based applications with little effort.\n\n\n11.1.1 Getting Started in the Code Editor\nIf you have not already done so, be sure to add the book’s code repository to the Code Editor by entering https://code.earthengine.google.com/?accept_repo=projects/gee-edu/book into your browser. The book’s scripts will then be available in the script manager panel. If you have trouble finding the repo, you can visit this link for help.\nThe Code Editor is an integrated development environment for the Earth Engine JavaScript API. It offers an easy way to type, debug, run, and manage code. Once you have followed Google’s documentation on registering for an Earth Engine account, you should follow the documentation to open the Code Editor. When you first visit the Code Editor, you will see a screen such as the one shown in Fig. F1.0.1.\n\n\n\nFig. F1.0.1 The Earth Engine Code Editor\n\n\nThe Code Editor (Fig. F1.0.1) allows you to type JavaScript code and execute it. When you are first learning a new language and getting used to a new programming environment, it is customary to make a program to display the words “Hello World.” This is a fun way to start coding that shows you how to give input to the program and how to execute it. It also shows where the program displays the output. Doing this in JavaScript is quite simple. Copy the following code into the center panel.\nprint('Hello World');\nThe line of code above uses the JavaScript print function to print the text “Hello World” to the screen. Once you enter the code, click the Run button. The output will be displayed on the upper right-hand panel under the Console tab (Fig. F1.0.2.).\n\n\n\nFig. F1.0.2 Typing and running code\n\n\nYou now know where to type your code, how to run it, and where to look for the output. You just wrote your first Earth Engine script and may want to save it. Click the Save button (Fig. F1.0.3).\n\n\n\nFig. F1.0.3 Saving a script\n\n\nIf this is your first time using the Code Editor, you will be prompted to create a home folder. This is a folder in the cloud where all your code will be saved. You can pick a name of your choice, but remember that it cannot be changed and will forever be associated with your account. A good choice for the name would be your Google Account username (Fig. F1.0.4).\n\n\n\nFig. F1.0.4 Creating a home folder\n\n\nOnce your home folder is created, you will be prompted to enter a new repository. A repository can help you organize and share code. Your account can have multiple repositories and each repository can have multiple scripts inside it. To get started, you can create a repository named “default” (Fig. F1.0.5).\n\n\n\nFig. F1.0.5 Creating a new repository\n\n\nFinally, you will be able to save your script inside the newly created repository. Enter the name “hello_world” and click OK (Fig. F1.0.6).\n\n\n\nFig. F1.0.6 Saving a file\n\n\nOnce the script is saved, it will appear in the script manager panel (Fig. F1.0.7). The scripts are saved in the cloud and will always be available to you when you open the Code Editor.\n\n\n\nFig. F1.0.7 The script manager\n\n\nNow you should be familiar with how to create, run, and save your scripts in the Code Editor. You are ready to start learning the basics of JavaScript.\n\n\n11.1.2 JavaScript Basics\nTo be able to construct a script for your analysis, you will need to use JavaScript. This section covers the JavaScript syntax and basic data structures. In the sections that follow, you will see more JavaScript code, noted in a distinct font and with shaded background. As you encounter code, paste it into the Code Editor and run the script.\n\nVariables\nIn a programming language, variables are used to store data values. In JavaScript, a variable is defined using the var keyword followed by the name of the variable. The code below assigns the text “San Francisco” to the variable named city. Note that the text string in the code should be surrounded by quotes. You are free to use either ’ (single quotes) or “ (double quotes), and they must match at the beginning and end of each string. In your programs, it is advisable to be consistent—use either single quotes or double quotes throughout a given script (the code in this book generally uses single quotes for code). Each statement of your script should typically end with a semicolon, although Earth Engine’s code editor does not require it. \nvar city = 'San Francisco';\nIf you print the variable city, you will get the value stored in the variable (San Francisco) printed in the Console. \nprint(city);\nWhen you assign a text value, the variable is automatically assigned the type string. You can also assign numbers to variables and create variables of type number. The following code creates a new variable called population and assigns a number as its value.\nvar population = 873965;  \nprint(population);\n\n\nLists\nIt is helpful to be able to store multiple values in a single variable. JavaScript provides a data structure called a list that can hold multiple values. We can create a new list using the square brackets [] and adding multiple values separated by a comma.\nvar cities = ['San Francisco', 'Los Angeles', 'New York', 'Atlanta']; \nprint(cities);\nIf you look at the output in the Console, you will see “List” with an expander arrow (▹) next to it. Clicking on the arrow will expand the list and show you its content. You will notice that along with the four items in the list, there is a number next to each value. This is the index of each item. It allows you to refer to each item in the list using a numeric value that indicates its position in the list.\n\n\n\nFig. F1.0.8 A JavaScript list\n\n\n\n\nObjects\nLists allow you to store multiple values in a single container variable. While useful, it is not appropriate to store structured data. It is helpful to be able to refer to each item with its name rather than its position. Objects in JavaScript allow you to store key-value pairs, where each value can be referred to by its key. You can create a dictionary using the curly braces {}. The code below creates an object called cityData with some information about San Francisco.\nvar cityData = {   \n    'city': 'San Francisco',   \n    'coordinates': [-122.4194, 37.7749],   \n    'population': 873965 \n    };  \n\nprint(cityData);\nWe can use multiple lines to define the object. Only when we put in the semicolon (;) is the command considered complete. The object will be printed in the Console. You can see that instead of a numeric index, each item has a label. This is known as the key and can be used to retrieve the value of an item.\n\n\n\nFig. F1.0.9 A JavaScript object\n\n\n\n\nFunctions\nWhile using Earth Engine, you will need to define your own functions. Functions take user inputs, use them to carry out some computation, and send an output back. Functions allow you to group a set of operations together and repeat the same operations with different parameters without having to rewrite them every time. Functions are defined using the function keyword. The code below defines a function called greet that takes an input called name and returns a greeting with Hello prefixed to it. Note that we can call the function with different input and it generates different outputs with the same code.\nvar greet = function(name) {   \n    return 'Hello ' + name;  \n    };\n    \nprint(greet('World'));  \nprint(greet('Readers'));\n\n\n\nFig. F1.0.10 JavaScript function output\n\n\n\n\n11.1.2.1 Comments\nWhile writing code, it is useful to add a bit of text to explain the code or leave a note for yourself. It is a good programming practice to always add comments in the code explaining each step. In JavaScript, you can prefix any line with two forward slashes // to make it a comment. The text in the comment will be ignored by the interpreter and will not be executed.\n// This is a comment!\nCongratulations! You have learned enough JavaScript to be able to use the Earth Engine API. In the next section, you will see how to access and execute Earth Engine API functions using JavaScript.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F10a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n\n11.1.3 Earth Engine API Basics\nThe Earth Engine API is vast and provides objects and methods to do everything from simple math to advanced algorithms for image processing. In the Code Editor, you can switch to the Docs tab to see the API functions grouped by object types. The API functions have the prefix ee (for Earth Engine).\n\n\n\nFig. F1.0.12 Earth Engine API docs\n\n\nLet’s learn to use the API. Suppose you want to add two numbers, represented by the variables a and b, as below. Make a new script and enter the following:\nvar a = 1;  \nvar b = 2;\nIn Sect. 1, you learned how to store numbers in variables, but not how to do any computation. This is because when you use Earth Engine, you do not do addition using JavaScript operators. For example, you would not write “var c = a + b” to add the two numbers. Instead, the Earth Engine API provides you with functions to do this, and it is important that you use the API functions whenever you can. It may seem awkward at first, but using the functions, as we’ll describe below, will help you avoid timeouts and create efficient code.\nLooking at the Docs tab, you will find a group of methods that can be called on an ee.Number. Expand it to see the various functions available to work with numbers. You will see the ee.Number function that creates an Earth Engine number object from a value. In the list of functions, there is an add function for adding two numbers. That’s what you use to add a and b.\n\n\n\nFig. F1.0.13 ee.Number module\n\n\nTo add a and b, we first create an ee.Number object from variable a with ee.Number(a). And then we can use the add(b) call to add the value of b to it. The following code shows the syntax and prints the result which, of course, is the value 3. \nvar result = ee.Number(a).add(b);\nprint(result);\nBy now you may have realized that when learning to program in Earth Engine, you do not need to deeply learn JavaScript or Python—instead, they are ways to access the Earth Engine API. This API is the same whether it is called from JavaScript or Python.\nHere’s another example to drive this point home. Let’s say you are working on a task that requires you to create a list of years from 1980 to 2020 with a five-year interval. If you are faced with this task, the first step is to switch to the Docs tab and open the ee.List module. Browse through the functions and see if there are any functions that can help. You will notice a function ee.List.sequence. Clicking on it will bring up the documentation of the function.\n\n\n\nFig. F1.0.14 The ee.List.sequence function\n\n\nThe function ee.List.sequence is able to generate a sequence of numbers from a given start value to the end value. It also has an optional parameter step to indicate the increment between each number. We can create a ee.List of numbers representing years from 1980 to 2020, counting by 5, by calling this predefined function with the following values: start = 1980, end = 2020, and step = 5.\nvar yearList = ee.List.sequence(1980, 2020, 5);  \nprint(yearList);\nThe output printed in the Console will show that the variable yearList indeed contains the list of years with the correct interval.\n\n\n\nFig. F1.0.15 Output of ee.List.sequence function\n\n\nYou just accomplished a moderately complex programming task with the help of Earth Engine API.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F10b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\nConclusion\nThis chapter introduced the Earth Engine API. You also learned the basics of JavaScript syntax to be able to use the API in the Code Editor environment. We hope you now feel a bit more comfortable starting your journey to become an Earth Engine developer. Regardless of your programming background or familiarity with JavaScript, you have the tools at your disposal to start using the Earth Engine API to build scripts for remote sensing analysis."
  },
  {
    "objectID": "F1.html#exploring-images",
    "href": "F1.html#exploring-images",
    "title": "11  Getting Started",
    "section": "11.2 Exploring Images",
    "text": "11.2 Exploring Images\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthor\nJeff Howarth\n\n\nOverview\nSatellite images are at the heart of Google Earth Engine’s power. This chapter teaches you how to inspect and visualize data stored in image bands. We first visualize individual bands as separate map layers and then explore a method to visualize three different bands in a single composite layer. We compare different kinds of composites for satellite bands that measure electromagnetic radiation in the visible and non-visible spectrum. We then explore images that represent more abstract attributes of locations, and create a composite layer to visualize change over time.  \n\n\nLearning Outcomes\n\nUsing the Code Editor to load an image\nUsing code to select image bands and visualize them as map layers\nUnderstanding true- and false-color composites of images\nConstructing new multiband images.\nUnderstanding how additive color works and how to interpret RGB composites.\n\n\n\nAssumes you know how to:\n\nSign up for an Earth Engine account, open the Code Editor, and save your script (Chap. F1.0).\n\n\n\n\n\n11.2.1 Accessing an Image\nIf you have not already done so, be sure to add the book’s code repository to the Code Editor by entering https://code.earthengine.google.com/?accept_repo=projects/gee-edu/book into your browser. The book’s scripts will then be available in the script manager panel. If you have trouble finding the repo, you can visit this link for help.\nTo begin, you will construct an image with the Code Editor. In the sections that follow, you will see code in a distinct font and with shaded background. As you encounter code, paste it into the center panel of the Code Editor and click Run.\nFirst, copy and paste the following:\nvar first_image = ee.Image('LANDSAT/LT05/C02/T1_L2/LT05_118038_20000606');\nWhen you click Run, Earth Engine will load an image captured by the Landsat 5 satellite on June 6, 2000. You will not yet see any output.\nYou can explore the image in several ways. To start, you can retrieve metadata (descriptive data about the image) by printing the image to the Code Editor’s Console panel:\nprint(first_image);\nIn the Console panel, you may need to click the expander arrows to show the information. You should be able to read that this image consists of 19 different bands. For each band, the metadata lists four properties, but for now let’s simply note that the first property is a name or label for the band enclosed in quotation marks. For example, the name of the first band is “SR_B1” (Fig. F1.1.1).\n\n\n\nFig. F1.1.1 Image metadata printed to Console panel\n\n\nA satellite sensor like Landsat 5 measures the magnitude of radiation in different portions of the electromagnetic spectrum. The first six bands in our image (“SR_B1” through “SR_B7”) contain measurements for six different portions of the spectrum. The first three bands measure visible portions of the spectrum, or quantities of blue, green, and red light. The other three bands measure infrared portions of the spectrum that are not visible to the human eye.\nAn image band is an example of a raster data model, a method of storing geographic data in a two-dimensional grid of pixels, or picture elements.\n\n\n11.2.2 Visualizing an Image\nNow let’s add one of the bands to the map as a layer so that we can see it.  \nMap.addLayer(first_image, //  dataset to display   \n        {bands: ['SR_B1'], //  band to display       \n        min: 8000, //  display range         \n        max: 17000},   \n        'Layer 1' //  name to show in Layer Manager  \n        );\nThe code here uses the addLayer method of the map in the Code Editor. There are four important components of the command above:\n\nfirst_image: This is the dataset to display on the map.\nbands: These are the particular bands from the dataset to display on the map. In our example, we displayed a single band named “SR_B1”.\nmin, max: These represent the lower and upper bounds of values from “SR_B1” to display on the screen. By default, the minimum value provided (8000) is mapped to black, and the maximum value provided (17000) is mapped to white. The values between the minimum and maximum are mapped linearly to grayscale between black and white. Values below 8000 are drawn as black. Values above 17000 are drawn as white. Together, the bands, min, and max parameters define visualization parameters, or instructions for data display.\n‘Layer 1’: This is a label for the map layer to display in the Layer Manager. This label appears in the dropdown menu of layers in the upper right of the map.\n\nWhen you run the code, you might not notice the image displayed unless you pan around and look for it. To do this, click and drag the map towards Shanghai, China. (You can also jump there by typing “Shanghai” into the Search panel at the top of the Code Editor, where the prompt says Search places and datasets…) Over Shanghai, you should see a small, dark, slightly angled square. Use the zoom tool (the + sign, upper left of map) to increase the zoom level and make the square appear larger.  \nCan you recognize any features in the image? By comparing it to the standard Google map that appears under the image (as the base layer), you should be able to distinguish the coastline. The water near the shore generally appears a little lighter than the land, except perhaps for a large, light-colored blob on the land in the bottom of the image.\nLet’s explore this image with the Inspector tool. When you click on the Inspector tab on the right side of the Code Editor (Fig. F1.1.2, area A), your cursor should now look like crosshairs. When you click on a location in the image, the Inspector panel will report data for that location under three categories as follows:  \n\n\n\nFig. F1.1.2 Image data reported through the Inspector panel\n\n\n\nPoint: data about the location on the map. This includes the geographic location (longitude and latitude) and some data about the map display (zoom level and scale).\nPixels: data about the pixel in the layer. If you expand this, you will see the name of the map layer, a description of the data source, and a bar chart. In our example, we see “Layer 1” is drawn from an image dataset that contains 19 bands. Under the layer name, the chart displays the pixel value at the location that you clicked for each band in the dataset. When you hover your cursor over a bar, a panel will pop up to display the band name and “band value” (pixel value). To find the pixel value for “SR_B1”, hover the cursor over the first bar on the left. Alternatively, by clicking on the little blue icon to the right of “Layer 1” (Fig. F1.1.2, area B), you will change the display from a bar chart to a dictionary that reports the pixel value for each band.    \nObjects: data about the source dataset. Here you will find metadata about the image that looks very similar to what you retrieved earlier when you directed Earth Engine to print the image to the Console.  \n\nLet’s add two more bands to the map.\nMap.addLayer(  \n   first_image,  \n   {bands: ['SR_B2'],  \n       min: 8000,  \n       max: 17000},   \n    'Layer 2',   \n    0, //  shown   \n    1 //  opacity  \n);  \n  \nMap.addLayer(  \n   first_image,  \n   {bands: ['SR_B3'],  \n       min: 8000,  \n       max: 17000},   \n    'Layer 3',   \n    1, //  shown   \n    0 //  opacity  \n);\nIn the code above, notice that we included two additional parameters to the Map.addLayer call. One parameter controls whether or not the layer is shown on the screen when the layer is drawn. It may be either 1 (shown) or 0 (not shown). The other parameter defines the opacity of the layer, or your ability to “see through” the map layer. The opacity value can range between 0 (transparent) and 1 (opaque).\n\n\n\nFig. F1.1.3 Three bands from the Landsat image, drawn as three different grayscale layers\n\n\nDo you see how these new parameters influence the map layer displays (Fig. F1.1.3)? For Layer 2, we set the shown parameter as 0. For Layer 3, we set the opacity parameter as 0. As a result, neither layer is visible to us when we first run the code. We can make each layer visible with controls in the Layers manager checklist on the map (at top right). Expand this list and you should see the names that we gave each layer when we added them to the map. Each name sits between a checkbox and an opacity slider. To make Layer 2 visible, click the checkbox (Fig. F1.1.3, area A). To make Layer 3 visible, move the opacity slider to the right (Fig. F1.1.3, area B).\nBy manipulating these controls, you should notice that these layers are displayed as a stack, meaning one on top of the other. For example, set the opacity for each layer to be 1 by pushing the opacity sliders all the way to the right. Then make sure each box is checked next to each layer so that all the layers are shown. Now you can identify which layer is on top of the stack by checking and unchecking each layer. If a layer is on top of another, unchecking the top layer will reveal the layer underneath. If a layer is under another layer in the stack, then unchecking the bottom layer will not alter the display (because the top layer will remain visible). If you try this on our stack, you should see that the list order reflects the stack order, meaning that the layer at the top of the layer list appears on the top of the stack. Now compare the order of the layers in the list to the sequence of operations in your script. What layer did your script add first and where does this appear in the layering order on the map?\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F11a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n11.2.3 True-Color Composites\nUsing the controls in the Layers manager, explore these layers and examine how the pixel values in each band differ. Does Layer 2 (displaying pixel values from the “SR_B2” band) appear generally brighter than Layer 1 (the “SR_B1” band)? Compared with Layer 2, do the ocean waters in Layer 3 (the “SR_B3” band) appear a little darker in the north, but a little lighter in the south?  \nWe can use color to compare these visual differences in the pixel values of each band layer all at once as an RGB composite. This method uses the three primary colors (red, green, and blue) to display each pixel’s values across three bands.\nTo try this, add this code and run it.\nMap.addLayer(  \n   first_image,  \n   {bands: ['SR_B3', 'SR_B2', 'SR_B1'],  \n       min: 8000,  \n       max: 17000},   \n    'Natural Color');\nThe result (Fig. F1.1.4) looks like the world we see, and is referred to as a natural-color composite, because it naturally pairs the spectral ranges of the image bands to display colors. Also called a true-color composite, this image shows the red spectral band with shades of red, the green band with shades of green, and the blue band with shades of blue. We specified the pairing simply through the order of the bands in the list: B3, B2, B1. Because bands 3, 2, and 1 of Landsat 5 correspond to the real-world colors of red, green, and blue, the image resembles the world that we would see outside the window of a plane or with a low-flying drone.  \n\n\n\nFig. F1.1.4 True-color composite\n\n\n\n\n11.2.4 False-Color Composites\nAs you saw when you printed the band list (Fig. F1.1.1), a Landsat image contains many more bands than just the three true-color bands. We can make RGB composites to show combinations of any of the bands—even those outside what the human eye can see. For example, band 4 represents the near-infrared band, just outside the range of human vision. Because of its value in distinguishing environmental conditions, this band was included on even the earliest 1970s Landsats. It has different values in coniferous and deciduous forests, for example, and can indicate crop health. To see an example of this, add this code to your script and run it.  \nMap.addLayer(  \n   first_image,  \n   {bands: ['SR_B4', 'SR_B3', 'SR_B2'],  \n       min: 8000,  \n       max: 17000},   \n    'False Color');\nIn this false-color composite (Fig. F1.1.5), the display colors no longer pair naturally with the bands. This particular example, which is more precisely referred to as a color-infrared composite, is a scene that we could not observe with our eyes, but that you can learn to read and interpret. Its meaning can be deciphered logically by thinking through what is passed to the red, green, and blue color channels.\n\n\n\nFig. F1.1.5 Color-infrared image (a false-color composite)\n\n\nNotice how the land on the northern peninsula appears bright red (Fig. F1.1.5, area A). This is because for that area, the pixel value of the first band (which is drawing the near-infrared brightness) is much higher relative to the pixel value of the other two bands. You can check this by using the Inspector tool. Try zooming into a part of the image with a red patch (Fig. F1.1.5, area B) and clicking on a pixel that appears red. Then expand the “False Color” layer in the Inspector panel (Fig. F1.1.6, area A), click the blue icon next to the layer name (Fig. F1.1.6, area B), and read the pixel value for the three bands of the composite (Fig. F1.1.6, area C). The pixel value for B4 should be much greater than for B3 or B2. \n\n\n\nFig. F1.1.6 Values of B4, B3, B2 bands for a pixel that appears bright red\n\n\nIn the bottom left corner of the image (Fig. F1.1.5, area C), rivers and lakes appear very dark, which means that the pixel value in all three bands is low. However, sediment plumes fanning from the river into the sea appear with blue and cyan tints (Fig. F1.1.5, area D). If they look like primary blue, then the pixel value for the second band (B3) is likely higher than the first (B4) and third (B2) bands. If they appear more like cyan, an additive color, it means that the pixel values of the second and third bands are both greater than the first.\nIn total, the false-color composite provides more contrast than the true-color image for understanding differences across the scene. This suggests that other bands might contain more useful information as well. We saw earlier that our satellite image consisted of 19 bands. Six of these represent different portions of the electromagnetic spectrum, including three beyond the visible spectrum, that can be used to make different false-color composites. Use the code below to explore a composite that shows shortwave infrared, near infrared, and visible green (Fig. F1.1.7).  \nMap.addLayer(  \n   first_image,  \n   {bands: ['SR_B5', 'SR_B4', 'SR_B2'],  \n       min: 8000,  \n       max: 17000},   'Short wave false color');\n\n\n\nFig. F1.1.7 Shortwave infrared false-color composite\n\n\nTo compare the two false-color composites, zoom into the area shown in the two pictures of Fig. F1.1.8. You should notice that bright red locations in the left composite appear bright green in the right composite. Why do you think that is? Does the image on the right show new distinctions not seen in the image on the left? If so, what do you think they are? \n\n\n\n\nFig. F1.1.8 Near-infrared versus shortwave infrared false-color composites\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F11b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n11.2.5 Attributes of Locations\nSo far, we have explored bands as a method for storing data about slices of the electromagnetic spectrum that can be measured by satellites. Now we will work towards applying the additive color system to bands that store non-optical and more abstract attributes of geographic locations.  \nTo begin, add this code to your script and run it.  \nvar lights93 = ee.Image('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS/F101993');  \nprint('Nighttime lights', lights93);  \n  \nMap.addLayer(  \n   lights93,  \n   {  \n       bands: ['stable_lights'],  \n       min: 0,  \n       max: 63   },   'Lights');\nThis code loads an image of global nighttime lights and adds a new layer to the map. Please look at the metadata that we printed to the Console panel. You should see that the image consists of four bands. The code selects the “stable_lights” band to display as a layer to the map. The range of values for display (0–63) represent the minimum and maximum pixel values in this image. As mentioned earlier, you can find this range in the Earth Engine Data Datalog or with other Earth Engine methods. These will be described in more detail in the next few chapters.\nThe global nighttime lights image represents the average brightness of nighttime lights at each pixel for a calendar year. For those of us who have sat by a window in an airplane as it descends to a destination at night, the scene may look vaguely familiar. But the image is very much an abstraction. It provides us a view of the planet that we would never be able to see from an airplane or even from space. Night blankets the entire planet in darkness. There are no clouds. In the “stable lights” band, there are no ephemeral sources of light. Lightning strikes, wildfires, and other transient lights have been removed. It is a layer that aims to answer one question about our planet at one point in time: In 1993, how bright were Earth’s stable, artificial sources of light?\nWith the zoom controls on the map, you can zoom out to see the bright spot of Shanghai, the large blob of Seoul to the north and east, the darkness of North Korea except for the small dot of Pyongyang, and the dense strips of lights of Japan and the west coast of Taiwan (Fig. F1.1.10).  \n\n\n\nFig. F1.1.10 Stable nighttime lights in 1993\n\n\n\n\n11.2.6 Abstract RGB Composites  \nNow we can use the additive color system to make an RGB composite that compares stable nighttime lights at three different slices of time. Add the code below to your script and run it.  \nvar lights03 = ee.Image('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS/F152003')  \n   .select('stable_lights').rename('2003');  \n  \nvar lights13 = ee.Image('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS/F182013')  \n   .select('stable_lights').rename('2013');  \n  \nvar changeImage = lights13.addBands(lights03)  \n   .addBands(lights93.select('stable_lights').rename('1993'));  \n  \nprint('change image', changeImage);  \n  \nMap.addLayer(  \n   changeImage,  \n   {min: 0,  \n    max: 63},\n    'Change composite');\nThis code does a few things. First, it creates two new images, each representing a different slice of time. For both, we use the select method to select a band (“stable_lights”) and the rename method to change the band name to indicate the year it represents.  \nNext, the code uses the addBands method to create a new, three-band image that we name “changeImage”. It does this by taking one image (lights13) as the first band, using another image (lights03) as the second band, and the lights93 image seen earlier as the third band. The third band is given the name “1993” as it is placed into the image.\nFinally, the code prints metadata to the Console and adds the layer to the map as an RGB composite using Map.addLayer. If you look at the printed metadata, you should see under the label “change image” that our image is composed of three bands, with each band named after a year. You should also notice the order of the bands in the image: 2013, 2003, 1993. This order determines the color channels used to represent each slice of time in the composite: 2013 as red, 2003 as green, and 1993 as blue (Fig. F1.1.11).\n\n\n\nFig. F1.1.11 RGB composite of stable nighttime lights (2013, 2003, 1993)\n\n\nWe can now read the colors displayed on the layer to interpret different kinds of changes in nighttime lights across the planet over two decades. Pixels that appear white have high brightness in all three years. You can use the Inspector panel to confirm this. Click on the Inspector panel to change the cursor to a crosshair and then click on a pixel that appears white. Look under the Pixel category of the Inspector panel for the “Change composite” layer. The pixel value for each band should be high (at or near 63).  \nMany clumps of white pixels represent urban cores. If you zoom into Shanghai, you will notice that the periphery of the white-colored core appears yellowish and the terminal edges appear reddish. Yellow represents locations that were bright in 2013 and 2003 but dark in 1993. Red represents locations that appear bright in 2013 but dark in 2003 and 1993. If you zoom out, you will see this gradient of white core to yellow periphery to red edge occurs around many cities across the planet, and shows the global pattern of urban sprawl over the 20-year period. \nWhen you zoom out from Shanghai, you will likely notice that each map layer redraws every time you change the zoom level. In order to explore the change composite layer more efficiently, use the Layer manager panel to not show (uncheck) all of the layers except for “Change composite.” Now the map will respond faster when you zoom and pan because it will only refresh the single displayed shown layer.\nIn addition to urban change, the layer also shows changes in resource extraction activities that produce bright lights. Often, these activities produce lights that are stable over the span of a year (and therefore included in the “stable lights” band), but are not sustained over the span of a decade or more. For example, in the Korea Strait (between South Korea and Japan), you can see geographic shifts of fishing fleets that use bright halogen lights to attract squid and other sea creatures towards the water surface and into their nets. Bluish pixels were likely fished more heavily in 1993 and became used less frequently by 2003, while greenish pixels were likely fished more heavily in 2003 and less frequently by 2013 (Fig. F1.1.11).\n\n\n\nFig. F1.1.12 Large red blobs in North Dakota and Texas from fossil fuel extraction in specific years\n\n\nSimilarly, fossil fuel extraction produces nighttime lights through gas flaring. If you pan to North America (Fig. F1.1.12), red blobs in Alberta and North Dakota and a red swath in southeastern Texas all represent places where oil and gas extraction were absent in 1993 and 2003 but booming by 2013. Pan over to the Persian Gulf and you will see changes that look like holiday lights with dots of white, red, green, and blue appearing near each other; these distinguish stable and shifting locations of oil production. Blue lights in Syria near the border with Iraq signify the abandonment of oil fields after 1993 (Fig. F1.1.13). Pan further north and you will see another “holiday lights” display from oil and gas extraction around Surgut, Russia. In many of these places, you can check for oil and gas infrastructure by zooming in to a colored spot, making the lights layer not visible, and selecting the Satellite base layer (upper right).\n\n\n\nFig. F1.1.13 Nighttime light changes in the Middle East\n\n\nAs you explore this image, remember to check your interpretations with the Inspector panel by clicking on a pixel and reading the pixel value for each band. Refer back to the additive color figure to remember how the color system works. If you practice this, you should be able to read any RGB composite by knowing how colors relate to the relative pixel value of each band. This will empower you to employ false-color composites as a flexible and powerful method to explore and interpret geographic patterns and changes on Earth’s surface. \n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F11c. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\nConclusion\nIn this chapter, we looked at how an image is composed of one or more bands, where each band stores data about geographic locations as pixel values. We explored different ways of visualizing these pixel values as map layers, including a grayscale display of single bands and RGB composites of three bands. We created natural and false-color composites that use additive color to display information in visible and non-visible portions of the spectrum. We examined additive color as a general system for visualizing pixel values across multiple bands. We then explored how bands and RGB composites can be used to represent more abstract phenomena, including different kinds of change over time."
  },
  {
    "objectID": "F1.html#survey-of-raster-datasets",
    "href": "F1.html#survey-of-raster-datasets",
    "title": "11  Getting Started",
    "section": "11.3 Survey of Raster Datasets",
    "text": "11.3 Survey of Raster Datasets\nThe previous chapter introduced you to images, one of the core building blocks of remotely sensed imagery in Earth Engine. In this chapter, we will expand on this concept of images by introducing image collections. Image collections in Earth Engine organize many different images into one larger data storage structure. Image collections include information about the location, date collected, and other properties of each image, allowing you to sift through the ImageCollection for the exact image characteristics needed for your analysis.\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthors\nAndréa Puzzi Nicolau, Karen Dyson, David Saah, Nicholas Clinton\n\n\nOverview\nThe purpose of this chapter is to introduce you to the many types of collections of images available in Google Earth Engine. These include sets of individual satellite images, pre-made composites (which merge multiple individual satellite images into one composite image), classified land use and land cover (LULC) maps, weather data, and other types of datasets. If you are new to JavaScript or programming, work through Chaps. F1.0 and F1.1 first.  \n\n\nLearning Outcomes\n\nAccessing and viewing sets of images in Earth Engine.\nExtracting single scenes from collections of images.\nApplying visualization parameters in Earth Engine to visualize an image.\n\n\n\nAssumes you know how to:\n\nSign up for an Earth Engine account, open the Code Editor, and save your script. (Chap. F1.0)\nLocate the Earth Engine Inspector and Console tabs and understand their purposes (Chap. F1.0).\nUse the Inspector tab to assess pixel values (Chap. F1.1).\n\n\n\n\n\n11.3.1 Image Collections: An Organized Set of Images\nThere are many different types of image collections available in Earth Engine. These include collections of individual satellite images, pre-made composites that combine multiple images into one blended image, classified LULC maps, weather data, and other non-optical data sets. Each one of these is useful for different types of analyses. For example, one recent study examined the drivers of wildfires in Australia (Sulova and Jokar 2021). The research team used the European Center for Medium-Range Weather Forecast Reanalysis (ERA5) dataset produced by the European Center for Medium-Range Weather Forecasts (ECMWF) and is freely available in Earth Engine. We will look at this dataset later in the chapter.\nYou saw some of the basic ways to interact with an individual ee.Image in the previous chapter. However, depending on how long a remote sensing platform has been in operation, there may be thousands or millions of images collected of Earth. In Earth Engine, these are organized into an ImageCollection, a specialized data type that has specific operations available in the Earth Engine API. Like individual images, they can be viewed with Map.addLayer.\n\n\n11.3.2 View an Image Collection\nThe Landsat program from NASA and the United States Geological Survey (USGS) has launched a sequence of Earth observation satellites, named Landsat 1, 2, etc. Landsats have been returning images since 1972, making that collection of images the longest continuous satellite-based observation of the Earth’s surface. We will now view images and basic information about one of the image collections that is still growing: collections of scenes taken by the Operational Land Imager aboard Landsat 8, which was launched in 2013. Copy and paste the following code into the center panel and click Run. While the enormous image catalog is accessed, it could take a couple of minutes to see the result in the Map area. If it takes more than a couple of minutes to see the images, try zooming in to a specific area to speed up the process.\n/////  \n// View an Image Collection  \n/////  \n  \n// Import the Landsat 8 Raw Collection.  \nvar landsat8 = ee.ImageCollection('LANDSAT/LC08/C02/T1');  \n  \n// Print the size of the Landsat 8 dataset.  \nprint(\n    'The size of the Landsat 8 image collection is:', \n    landsat8.size()\n    );  \n  \n// Try to print the image collection.  \n// WARNING! Running the print code immediately below produces an error because  \n// the Console can not print more than 5000 elements.  \nprint(landsat8);  \n  \n// Add the Landsat 8 dataset to the map as a mosaic. The collection is  \n// already chronologically sorted, so the most recent pixel is displayed.  \nMap.addLayer(landsat8,  \n   {bands: ['B4', 'B3', 'B2'],  \n       min: 5000,  \n       max: 15000},   \n    'Landsat 8 Image Collection'\n    );\nFirst, let’s examine the map output (Fig. F1.2.1).\n\n\n\nFig. F1.2.1 USGS Landsat 8 Collection 2 Tier 1 Raw Scenes collection\n\n\nNotice the high amount of cloud cover, and the “layered” look. Zoom out if needed. This is because Earth Engine is drawing each of the images that make up the ImageCollection one on top of the other. The striped look is the result of how the satellite collects imagery. The overlaps between images and the individual nature of the images mean that these are not quite ready for analysis; we will address this issue in future chapters.\nNow examine the printed size on the Console. It will indicate that there are more than a million images in the dataset (Fig. F1.2.2). If you return to this lab in the future, the number will be even larger, since this active collection is continually growing as the satellite gathers more imagery. For the same reason, Fig. F1.2.1 might look slightly different on your map because of this.\n\n\n\nFig. F1.2.2  Size of the entire Landsat 8 collection. Note that this number is constantly growing.\n\n\nNote that printing the ImageCollection returned an error message (Fig. F1.2.3), because calling print on an ImageCollection will write the name of every image in the collection to the Console. This is the result of an intentional safeguard within Earth Engine. We don’t want to see a million image names printed to the Console!\n\n\n\nFig. F1.2.3. Error encountered when trying to print the names and information to the screen for too many elements\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F12a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\nEdit your code to comment out the last two code commands you have written. This will remove the call to Map.addLayer that drew every image, and will remove the print statement that demanded more than 5000 elements. This will speed up your code in subsequent sections. Placing two forward slashes (//) at the beginning of a line will make it into a comment, and any commands on that line will not be executed.\n\n\n11.3.3 Filtering Image Collections\nThe ImageCollection data type in Earth Engine has multiple approaches to filtering, which helps to pinpoint the exact images you want to view or analyze from the larger collection.\n\n11.3.3.1 Filter by Date\nOne of the filters is filterDate, which allows us to narrow down the date range of the ImageCollection. Copy the following code to the center panel (paste it after the previous code you had):\n/////  \n// Filter an Image Collection  \n/////  \n  \n// Filter the collection by date.  \nvar landsatWinter = landsat8.filterDate('2020-12-01', '2021-03-01');  \n  \nMap.addLayer(landsatWinter,  \n   {bands: ['B4', 'B3', 'B2'],  \n       min: 5000,  \n       max: 15000},   \n    'Winter Landsat 8');  \n  \nprint('The size of the Winter Landsat 8 image collection is:',  \n   landsatWinter.size());\nExamine the mapped landsatWinter (Fig. F1.2.4). As described in the previous chaper, the 5000 and the 15000 values in the visualization parameters of the Map.addLayer function of the code above refer to the minimum and maximum of the range of display values.\n\n\n\nFig. F1.2.4 Landsat 8 Winter Collection\n\n\nNow look at the size of the winter Landsat 8 collection. The number is significantly lower than the number of images in the entire collection. This is the result of filtering the dates to three months in the winter of 2020–2021.\n\n\n11.3.3.2 Filter by Location\nA second frequently used filtering tool is filterBounds. This filter is based on a location—for example, a point, polygon, or other geometry. Copy and paste the code below to filter and add to the map the winter images from the Landsat 8 Image Collection to a point in Minneapolis, Minnesota, USA. Note below the Map.addLayer function to add the pointMN to the map with an empty dictionary {} for the visParams argument. This only means that we are not specifying visualization parameters for this element, and it is being added to the map with the default parameters.\n// Create an Earth Engine Point object.  \nvar pointMN = ee.Geometry.Point([-93.79, 45.05]);  \n  \n// Filter the collection by location using the point.  \nvar landsatMN = landsatWinter.filterBounds(pointMN);  \nMap.addLayer(\n    landsatMN,  \n   {bands: ['B4', 'B3', 'B2'],  \n       min: 5000,  \n       max: 15000},   \n    'MN Landsat 8');  \n  \n// Add the point to the map to see where it is.  \nMap.addLayer(pointMN, {}, 'Point MN');  \n  \nprint('The size of the Minneapolis Winter Landsat 8 image collection is: ',  \n   landsatMN.size());\nIf we uncheck the Winter Landsat 8 layer under Layers, we can see that only images that intersect our point have been selected (Fig. F1.2.5). Zoom in or out as needed. Note the printed size of the Minneapolis winter collection—we only have seven images.\n\n\n\nFig. F1.2.5 Minneapolis Winter Collection filtered by bounds.\n\n\nThe first still represents the map without zoom applied. The collection is shown inside the red circle. The second still represents the map after zoom was applied to the region. The red arrow indicates the point (in black) used to filter by bounds.\n\n\n11.3.3.3 Selecting the First Image\nThe final operation we will explore is the first function. This selects the first image in an ImageCollection. This allows us to place a single image on the screen for inspection. Copy and paste the code below to select and view the first image of the Minneapolis Winter Landsat 8 Image Collection. In this case, because the images are stored in time order in the ImageCollection, it will select the earliest image in the set.\n// Select the first image in the filtered collection.  \nvar landsatFirst = landsatMN.first();  \n  \n// Display the first image in the filtered collection.  \nMap.centerObject(landsatFirst, 7);  \nMap.addLayer(\n    landsatFirst,  \n   {bands: ['B4', 'B3', 'B2'],  \n       min: 5000,  \n       max: 15000},   \n    'First Landsat 8');\nThe first command takes our stack of location-filtered images and selects the first image. When the layer is added to the Map area, you can see that only one image is returned—remember to uncheck the other layers to be able to visualize the full image (Fig. F1.2.6). We used the Map.centerObject to center the map on the landsatFirst image with a zoom level of 7 (zoom levels go from 0 to 24).\n\n\n\nFig. F1.2.6 First Landsat image from the filtered set\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F12b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\nNow that we have the tools to examine different image collections, we will explore other datasets.\n\n\n\n11.3.4 Collections of Single Images\nWhen learning about image collections in the previous section, you worked with the Landsat 8 raw image dataset. These raw images have some important corrections already done for you. However, the raw images are only one of several image collections produced for Landsat 8. The remote sensing community has developed additional imagery corrections that help increase the accuracy and consistency of analyses. The results of each of these different imagery processing paths is stored in a distinct ImageCollection in Earth Engine.\nAmong the most prominent of these is the ImageCollection meant to minimize the effect of the atmosphere between Earth’s surface and the satellite. The view from satellites is made imprecise by the need for light rays to pass through the atmosphere, even on the clearest day. There are two important ways the atmosphere obscures a satellite’s view: by affecting the amount of sunlight that strikes the Earth, and by altering electromagnetic energy on its trip from its reflection at Earth’s surface to the satellite’s receptors.\nUnraveling those effects is called atmospheric correction, a highly complex process whose details are beyond the scope of this book. Thankfully, in addition to the raw images from the satellite, each image for Landsat and certain other sensors is automatically treated with the most up-to-date atmospheric correction algorithms, producing a product referred to as a “surface reflectance” ImageCollection. The surface reflectance estimates the ratio of upward radiance at the Earth’s surface to downward radiance at the Earth’s surface, imitating what the sensor would have seen if it were hovering a few feet above the ground.  \nLet’s examine one of these datasets meant to minimize the effects of the atmosphere between Earth’s surface and the satellite. Copy and paste the code below to import and filter the Landsat 8 surface reflectance data (landsat8SR) by date and to a point over San Francisco, California, USA (pointSF). We use the first function to select the first image—a single image from March 18, 2014. By printing the landsat8SRimage image on the Console, and accessing its metadata (see Chap. F1.1), we see that the band names differ from those in the raw image (Fig. F1.2.7). Here, they have the form  “SR_B” as in “Surface Reflectance Band ”, where * is the band number. We can also check the date of the image by looking at the image “id” (Fig. F1.2.7). This has the value “20140318”, a string indicating that the image was from March 18, 2014.\n/////  \n// Collections of single images - Landsat 8 Surface Reflectance  \n/////  \n  \n// Create and Earth Engine Point object over San Francisco.  \nvar pointSF = ee.Geometry.Point([-122.44, 37.76]);  \n  \n// Import the Landsat 8 Surface Reflectance collection.  \nvar landsat8SR = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2');  \n  \n// Filter the collection and select the first image.  \nvar landsat8SRimage = landsat8SR.filterDate('2014-03-18', '2014-03-19')  \n   .filterBounds(pointSF)  \n   .first();  \n  \nprint('Landsat 8 Surface Reflectance image', landsat8SRimage);\n\n\n\nFig. F1.2.7 Landsat 8 Surface Reflectance image bands and date\n\n\nCopy and paste the code below to add this image to the map with adjusted R,G, and B bands in the “bands” parameter for true-color display (see previous chapter).\n// Center map to the first image.  \nMap.centerObject(landsat8SRimage, 8);  \n  \n// Add first image to the map.  \nMap.addLayer(landsat8SRimage,  \n   {bands: ['SR_B4', 'SR_B3', 'SR_B2'],  \n       min: 7000,  \n       max: 13000},   \n    'Landsat 8 SR');\n\n\n\nFig. F1.2.8 Landsat 8 Surface Reflectance scene from March 18, 2014\n\n\nCompare this image (Fig. F1.2.8) with the raw Landsat 8 images from the previous section (Fig. F1.2.6). Zoom in and out and pan the screen as needed. What do you notice? Save your script but don’t start a new one—we will keep adding code to this script.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F12c. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n11.3.5 MODIS Monthly Burned Areas\nWe’ll explore two examples of composites made with data from the MODIS sensors, a pair of sensors aboard the Terra and Aqua satellites. On these complex sensors, different MODIS bands produce data at different spatial resolutions. For the visible bands, the lowest common resolution is 500 m (red and NIR are 250 m).\nSome of the MODIS bands have proven useful in determining where fires are burning and what areas they have burned. A monthly composite product for burned areas is available in Earth Engine. Copy and paste the code below.\n// Import the MODIS monthly burned areas dataset.  \nvar modisMonthly = ee.ImageCollection('MODIS/006/MCD64A1');  \n  \n// Filter the dataset to a recent month during fire season.  \nvar modisMonthlyRecent = modisMonthly.filterDate('2021-08-01');  \n  \n// Add the dataset to the map.  \nMap.addLayer(modisMonthlyRecent, {}, 'MODIS Monthly Burn');\nUncheck the other layers, and then pan and zoom around the map. Areas that have burned in the past month will show up as red (Fig. F1.2.11). Can you see where fires burned areas of California, USA? In Southern and Central Africa? Northern Australia?\n\n\n\nFig. F1.2.11. MODIS Monthly Burn image over California\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F12d. The book’s repository contains a script that shows what your code should look like at this point.\n\n\nSave your script and start a new one by refreshing the page.\n\n\n11.3.6 Methane\nSatellites can also collect information about the climate, weather, and various compounds present in the atmosphere. These satellites leverage portions of the electromagnetic spectrum and how different objects and compounds reflect when hit with sunlight in various wavelengths. For example, methane (CH4) reflects the 760 nm portion of the spectrum. Let’s take a closer look at a few of these datasets.\nThe European Space Agency makes available a methane dataset from Sentinel-5 in Earth Engine. Copy and paste the code below to add to the map methane data from the first time of collection on November 28, 2018. We use the select function (See Chap. F1.1) to select the methane-specific band of the dataset. We also introduce values for a new argument for the visualization parameters of Map.addLayer: We use a color palette to display a single band of an image in color. Here, we chose varying colors from black for the minimum value to red for the maximum value. Values in\nbetween will have the color in the order outlined by the palette parameter (a list of string colors: blue, purple, cyan, green, yellow, red).\n/////  \n// Other satellite products  \n/////  \n  \n// Import a Sentinel-5 methane dataset.  \nvar methane = ee.ImageCollection('COPERNICUS/S5P/OFFL/L3_CH4');  \n  \n// Filter the methane dataset.  \nvar methane2018 = methane.select(       'CH4_column_volume_mixing_ratio_dry_air')  \n   .filterDate('2018-11-28', '2018-11-29')  \n   .first();  \n  \n// Make a visualization for the methane data.  \nvar methaneVis = {  \n   palette: ['black', 'blue', 'purple', 'cyan', 'green',       'yellow', 'red'   ],  \n   min: 1770,  \n   max: 1920  \n};  \n  \n// Center the Map.  \nMap.centerObject(methane2018, 3);  \n  \n// Add the methane dataset to the map.  \nMap.addLayer(methane2018, methaneVis, 'Methane');\nNotice the different levels of methane over the African continent (Fig. F1.2.12).\n\n\n\nFig. F1.2.12. Methane levels over the African continent on November 28, 2018\n\n\n\n\n11.3.7 Global Forest Change\nAnother useful land cover product that has been pre-classified for you and is available in Earth Engine is the Global Forest Change dataset. This analysis was conducted between 2000 and 2020. Unlike the WorldCover dataset, this dataset focuses on the percent of tree cover across the Earth’s surface in a base year of 2000, and how that has changed over time. Copy and paste the code below to visualize the tree cover in 2000. Note that in the code below we define the visualization parameters as a variable treeCoverViz instead of having its calculation done within the Map.addLayer function.\n// Import the Hansen Global Forest Change dataset.  \nvar globalForest = ee.Image(   'UMD/hansen/global_forest_change_2020_v1_8');  \n  \n// Create a visualization for tree cover in 2000.  \nvar treeCoverViz = {  \n   bands: ['treecover2000'],  \n   min: 0,  \n   max: 100,  \n   palette: ['black', 'green']  \n};  \n  \n// Add the 2000 tree cover image to the map.  \nMap.addLayer(globalForest, treeCoverViz, 'Hansen 2000 Tree Cover');\nNotice how areas with high tree cover (e.g., the Amazon) are greener and areas with low tree cover are darker (Fig. F1.2.15). In case you see an error on the Console such as “Cannot read properties of null,” don’t worry. Sometimes Earth Engine will show these transient errors, but they won’t affect the script in any way.\n\n\n\nFig. F1.2.15 Global Forest Change 2000 tree cover layer\n\n\nCopy and paste the code below to visualize the tree cover loss over the past 20 years.\n// Create a visualization for the year of tree loss over the past 20 years.  \nvar treeLossYearViz = {  \n   bands: ['lossyear'],  \n   min: 0,  \n   max: 20,  \n   palette: ['yellow', 'red']  \n};  \n  \n// Add the 2000-2020 tree cover loss image to the map.  \nMap.addLayer(globalForest, treeLossYearViz, '2000-2020 Year of Loss');\nLeave the previous 2000 tree cover layer checked and analyze the loss layer on top of it—yellow, orange, and red areas (Fig. F1.2.16). Pan and zoom around the map. Where has there been recent forest loss (which is shown in red)?\n\n\n\nFig. F1.2.16 Global Forest Change 2000–2020 tree cover loss (yellow-red) and 2000 tree cover (black-green)\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F12f. The book’s repository contains a script that shows what your code should look like at this point.\n\n\nSave your script and start a new one.\n\n\n11.3.8 Digital Elevation Models\nDigital elevation models (DEMs) use airborne and satellite instruments to estimate the elevation of each location. Earth Engine has both local and global DEMs available. One of the global DEMs available is the NASADEM dataset, a DEM produced from a NASA mission. Copy and paste the code below to import the dataset and visualize the elevation band.\n// Import the NASA DEM Dataset.  \nvar nasaDEM = ee.Image('NASA/NASADEM_HGT/001');  \n  \n// Add the elevation layer to the map.  \nMap.addLayer(nasaDEM, {  \n   bands: ['elevation'],  \n   min: 0,  \n   max: 3000}, 'NASA DEM');\nUncheck the population layer and zoom in to examine the patterns of topography (Fig. F1.2.18). Can you see where a mountain range is located? Where is a river located? Try changing the minimum and maximum in order to make these features more visible. Save your script.\n\nFig. F1.2.18. NASADEM elevation\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F12g. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\nConclusion\nIn this chapter, we introduced image collections in Earth Engine and learned how to apply multiple types of filters to image collections to identify multiple or a single image for use. We also explored a few of the many different image collections available in the Earth Engine Data Catalog. Understanding how to find, access, and filter image collections is an important step in learning how to perform spatial analyses in Earth Engine.\n\n\nReferences\nChander G, Huang C, Yang L, et al (2009) Developing consistent Landsat data sets for large area applications: The MRLC 2001 protocol. IEEE Geosci Remote Sens Lett 6:777–781. https://doi.org/10.1109/LGRS.2009.2025244\nChander G, Markham BL, Helder DL (2009) Summary of current radiometric calibration coefficients for Landsat MSS, TM, ETM+, and EO-1 ALI sensors. Remote Sens Environ 113:893–903. https://doi.org/10.1016/j.rse.2009.01.007\nHansen MC, Potapov PV, Moore R, et al (2013) High-resolution global maps of 21st-century forest cover change. Science 342:850–853. https://doi.org/10.1126/science.1244693\nSulova A, Arsanjani JJ (2021) Exploratory analysis of driving force of wildfires in Australia: An application of machine learning within Google Earth Engine. Remote Sens 13:1–23. https://doi.org/10.3390/rs13010010"
  },
  {
    "objectID": "F1.html#the-remote-sensing-vocabulary",
    "href": "F1.html#the-remote-sensing-vocabulary",
    "title": "11  Getting Started",
    "section": "11.4 The Remote Sensing Vocabulary",
    "text": "11.4 The Remote Sensing Vocabulary\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthors\nKaren Dyson, Andréa Puzzi Nicolau, David Saah, Nicholas Clinton\n\n\nOverview\nThe purpose of this chapter is to introduce some of the principal characteristics of remotely sensed images and how they can be examined in Earth Engine. We discuss spatial resolution, temporal resolution, and spectral resolution, along with how to access important image metadata. You will be introduced to image data from several sensors aboard various satellite platforms. At the completion of the chapter, you will be able to understand the difference between remotely sensed datasets based on these characteristics, and how to choose an appropriate dataset for your analysis based on these concepts.  \n\n\nLearning Outcomes\n\nUnderstanding spatial, temporal, and spectral resolution.\nNavigating the Earth Engine Console to gather information about a digital image, including resolution and other data documentation.\n\n\n\nAssumes you know how to:\n\nNavigate among Earth Engine result tabs (Chap. F1.0).\nVisualize images with a variety of false-color band combinations (Chap. F1.1).\n\n\n\n\n\nIntroduction\nImages and image collections form the basis of many remote sensing analyses in Earth Engine. There are many different types of satellite imagery available to use in these analyses, but not every dataset is appropriate for every analysis. To choose the most appropriate dataset for your analysis, you should consider multiple factors. Among these are the resolution of the dataset—including the spatial, temporal, and spectral resolutions—as well as how the dataset was created and its quality.\n\n\n11.4.1 Searching for and Viewing Image Collection Information\nEarth Engine’s search bar can be used to find imagery and to locate important information about datasets in Earth Engine. Let’s use the search bar, located above the Earth Engine code, to find out information about the Landsat 7 Collection 2 Raw Scenes. First, type “landsat 7 collection 2” into the search bar (Fig. F1.3.1). Without hitting Enter, matches to that search term will appear.\n\n\n\nFig. F1.3.1 Searching for Landsat 7 in the search bar\n\n\nNow, click on USGS Landsat 7 Collection 2 Tier 1 Raw Scenes. A new inset window will appear (Fig. F1.3.2).\n\n\n\nFig. F1.3.2 Inset window with information about the Landsat 7 dataset\n\n\nThe inset window has information about the dataset, including a description, bands that are available, image properties, and terms of use for the data across the top. Click on each of these tabs and read the information provided. While you may not understand all of the information right now, it will set you up for success in future chapters.\nOn the left-hand side of this window, you will see a range of dates when the data is available, a link to the dataset provider’s webpage, and a collection snippet. This collection snippet can be used to import the dataset by pasting it into your script, as you did in previous chapters. You can also use the large Import button to import the dataset into your current workspace. In addition, if you click on the See example link, Earth Engine will open a new code window with a snippet of code that shows code using the dataset. Code snippets like this can be very helpful when learning how to use a dataset that is new to you.\nFor now, click on the small “pop out” button in the upper right corner of the window. This will open a new window with the same information (Fig. F1.3.3); you can keep this new window open and use it as a reference as you proceed.\n\n\n\nFig. F1.3.3 The Data Catalog page for Landsat 7 with information about the dataset\n\n\nSwitch back to your code window. Your “landsat 7 collection 2” search term should still be in the search bar. This time, click the “Enter” key or click on the search magnifying glass icon. This will open a Search results inset window (Fig. F1.3.4).\n\n\n\nFig. F1.3.4 Search results matching “landsat 7 collection 2”\n\n\nThis more complete search results inset window contains short descriptions about each of the datasets matching your search, to help you choose which dataset you want to use. Click on the Open in Catalog button to view these search results in the Earth Engine Data Catalog (Fig. F1.3.5). Note that you may need to click Enter in the data catalog search bar with your phrase to bring up the results in this new window.\n\n\n\nFig. F1.3.5 Earth Engine Data Catalog results for the “landsat 7 collection 2” search term\n\n\nNow that we know how to view this information, let’s dive into some important remote sensing terminology.\n\n\n11.4.2 Spatial Resolution\nSpatial resolution relates to the amount of Earth’s surface area covered by a single pixel. For example, we typically say that Landsat 7 has “30 m” color imagery. This means that each pixel is 30 m to a side, covering a total area of 900 square meters of the Earth’s surface. The spatial resolution of a given data set greatly affects the appearance of images, and the information in them, when you are viewing them on Earth’s surface.\nNext, we will visualize data from multiple sensors that capture data at different spatial resolutions, to compare the effect of different pixel sizes on the information and detail in an image. We’ll be selecting a single image from each ImageCollection to visualize. To view the image, we will draw them each as a color-IR image, a type of false-color image (described in detail in Chap. F1.1) that uses the infrared, red, and green bands. As you move through this portion of the course, zoom in and out to see differences in the pixel size and the image size.\n\n11.4.2.1 Landsat Thematic Mapper\nThematic Mapper (TM) sensors were flown aboard Landsat 4 and 5. TM data have been processed to a spatial resolution of 30m, and were active from 1982 to 2012. Search for “Landsat 5 TM” and import the result called “USGS Landsat 5 TM Collection 2 Tier 1 Raw Scenes”. In this dataset, the three bands for a color-IR image are called “B4” (infrared), “B3” (red), and “B2” (green). Let’s now visualize TM data over San Francisco airport. Note that we can either define the visualization parameters as a variable (as in the previous code snippet) or place them in curly braces in the Map.addLayer function (as in this code snippet).\nWhen you run this code, the TM image will display. Notice how many more pixels are displayed on your screen when compared to the MODIS image.\n// TM  \n// Filter TM imagery by location and date.  \nvar tmImage = tm   .filterBounds(Map.getCenter())  \n   .filterDate('1987-03-01', '1987-08-01')  \n   .first();  \n  \n// Display the TM image as a false color composite.  \nMap.addLayer(tmImage, {  \n   bands: ['B4', 'B3', 'B2'],   min: 0,  \n   max: 100}, 'TM');\n\n\n\nFig. F1.3.10 Visualizing the TM imagery from the Landsat 5 satellite\n\n\n\n\n11.4.2.2 Sentinel-2 MultiSpectral Instrument\nThe MultiSpectral Instrument (MSI) flies aboard the Sentinel-2 satellites, which are operated by the European Space Agency. The red, green, blue, and near-infrared bands are captured at 10m resolution, while other bands are captured at 20m and 30m. The Sentinel-2A satellite was launched in 2015 and the 2B satellite was launched in 2017.\nSearch for “Sentinel 2 MSI” in the search bar, and add the “Sentinel-2 MSI: MultiSpectral Instrument, Level-1C” dataset to your workspace. Name it msi. In this dataset, the three bands for a color-IR image are called “B8” (infrared), “B4” (red), and “B3” (green).\n// MSI  \n// Filter MSI imagery by location and date.  \nvar msiImage = msi   .filterBounds(Map.getCenter())  \n   .filterDate('2020-02-01', '2020-04-01')  \n   .first();  \n  \n// Display the MSI image as a false color composite.  \nMap.addLayer(msiImage, {  \n   bands: ['B8', 'B4', 'B3'],  \n   min: 0,  \n   max: 2000}, 'MSI');\nCompare the Sentinel imagery with the Landsat imagery, using the opacity slider. Notice how much more detail you can see on the airport terminal and surrounding landscape. The 10 m spatial resolution means that each pixel covers approximately 100 m2 of the Earth’s surface, a much smaller area than the TM imagery (900 m2).\n\n\n\nFig. F1.3.11 Visualizing the MSI imagery\n\n\n\n\n11.4.2.3 National Agriculture Imagery Program (NAIP)\nThe National Agriculture Imagery Program (NAIP) is a U.S. government program to acquire imagery over the continental United States using airborne sensors. Data is collected for each state approximately every three years. The imagery has a spatial resolution of 0.5–2 m, depending on the state and the date collected.  \nSearch for “naip” and import the data set for “NAIP: National Agriculture Imagery Program”.  Name the import naip. In this dataset, the three bands for a color-IR image are called “N” (infrared), “R” (red), and “G” (green).\n// NAIP  \n// Get NAIP images for the study period and region of interest.  \nvar naipImage = naip.filterBounds(Map.getCenter())  \n   .filterDate('2018-01-01', '2018-12-31')  \n   .first();  \n  \n// Display the NAIP mosaic as a color-IR composite.  \nMap.addLayer(naipImage, {  \n   bands: ['N', 'R', 'G']  \n}, 'NAIP');\nThe NAIP imagery is even more spatially detailed than the Sentinel-2 MSI imagery. However, we can see that our one NAIP image doesn’t totally cover the San Francisco airport. If you like, zoom out to see the boundaries of the NAIP image as we did for the Sentinel-2 MSI imagery.\n\n\n\nFig. F1.3.13 NAIP color-IR composite over the San Francisco airport\n\n\nEach of the datasets we’ve examined has a different spatial resolution. By comparing the different images over the same location in space, you have seen the differences between the large pixels of Landsat 5, the medium pixels of Sentinel-2, and the small pixels of the NAIP. Datasets with large-sized pixels are also called “coarse resolution,” those with medium-sized pixels are also called “moderate resolution,” and those with small-sized pixels are also called “fine resolution.”\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F13a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n\n11.4.3 Temporal Resolution\nTemporal resolution refers to the revisit time, or temporal cadence of a particular sensor’s image stream. Revisit time is the number of days between sequential visits of the satellite to the same location on the Earth’s surface. Think of this as the frequency of pixels in a time series at a given location.\n\n11.4.3.1 Landsat\nThe Landsat satellites 5 and later are able to image a given location every 16 days. Let’s use our existing tm dataset from Landsat 5. To see the time series of images at a location, you can filter an ImageCollection to an area and date range of interest and then print it. For example, to see the Landsat 5 images for three months in 1987, run the following code:\n/////  \n// Explore Temporal Resolution  \n/////  \n// Use Print to see Landsat revisit time  \nprint('Landsat-5 series:', tm   .filterBounds(Map.getCenter())  \n   .filterDate('1987-06-01', '1987-09-01'));  \n  \n// Create a chart to see Landsat 5's 16 day revisit time.  \nvar tmChart = ui.Chart.image.series({  \n   imageCollection: tm.select('B4').filterDate('1987-06-01',       '1987-09-01'),  \n   region: sfoPoint  \n}).setSeriesNames(['NIR']);\nExpand the features property of the printed ImageCollection in the Console output to see a List of all the images in the collection. Observe that the date of each image is part of the filename (e.g., LANDSAT/LT05/C02/T1/LT05_044034_19870628).\n\n\n\nFig. F1.3.14 Landsat image name and feature properties\n\n\nHowever, viewing this list doesn’t make it easy to see the temporal resolution of the dataset. We can use Earth Engine’s plotting functionality to visualize the temporal resolution of different datasets. For each of the different temporal resolutions, we will create a per-pixel chart of the NIR band that we mapped previously. To do this, we will use the ui.Chart.image.series function.\nThe ui.Chart.image.series function requires you to specify a few things in order to calculate the point to chart for each time step. First, we filter the ImageCollection (you can also do this outside the function and then specify the ImageCollection directly). We select the B4 (near infrared) band and then select three months by using filterDate on the ImageCollection. Next, we need to specify the location to chart; this is the region argument. We’ll use the sfoPoint variable we defined earlier.\n// Create a chart to see Landsat 5's 16 day revisit time.  \nvar tmChart = ui.Chart.image.series({  \n   imageCollection: tm.select('B4').filterDate('1987-06-01', '1987-09-01'),  \n   region: sfoPoint  \n}).setSeriesNames(['NIR']);\nBy default, this function creates a trend line. It’s difficult to see precisely when each image was collected, so let’s create a specialized chart style that adds points for each observation.\n// Define a chart style that will let us see the individual dates.  \nvar chartStyle = {  \n   hAxis: {  \n       title: 'Date'   },  \n   vAxis: {  \n       title: 'NIR Mean'   },  \n   series: {       0: {  \n           lineWidth: 3,  \n           pointSize: 6       }  \n   },  \n};// Apply custom style properties to the chart.  \ntmChart.setOptions(chartStyle);  \n  \n// Print the chart.  \nprint('TM Chart', tmChart);\nWhen you print the chart, it will have a point each time an image was collected by the TM instrument (Fig. F1.3.15). In the Console, you can move the mouse over the different points and see more information. Also note that you can expand the chart using the button in the upper right-hand corner. We will see many more examples of charts, particularly in the chapters in Part F4.\n\n\n\nFig. F1.3.15 A chart showing the temporal cadence, or temporal resolution of the Landsat 5 TM instrument at the San Francisco airport\n\n\n\n\n11.4.3.2 Sentinel-2\nThe Sentinel-2 program’s two satellites are in coordinated orbits, so that each spot on Earth gets visited about every 5 days. Within Earth Engine, images from these two sensors are pooled in the same dataset. Let’s create a chart using the MSI instrument dataset we have already imported.\n// Sentinel-2 has a 5 day revisit time.  \nvar msiChart = ui.Chart.image.series({  \n   imageCollection: msi.select('B8').filterDate('2020-06-01',       '2020-09-01'),  \n   region: sfoPoint  \n}).setSeriesNames(['NIR']);  \n  \n// Apply the previously defined custom style properties to the chart.  \nmsiChart.setOptions(chartStyle);  \n  \n// Print the chart.  \nprint('MSI Chart', msiChart);\n\n\n\nFig. F1.3.16 A chart showing the t temporal resolution of the Sentinel-2 MSI instrument at the San Francisco airport\n\n\nCompare this Sentinel-2 graph (Fig. F1.3.16) with the Landsat graph you just produced (Fig. F1.3.15). Both cover a period of six months, yet there are many more points through time for the Sentinel-2 satellite, reflecting the greater temporal resolution.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F13b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n\n11.4.4 Spectral Resolution\nSpectral resolution refers to the number and width of spectral bands in which the sensor takes measurements. You can think of the width of spectral bands as the wavelength intervals for each band. A sensor that measures radiance in multiple bands is called a multispectral sensor (generally 3–10 bands), while a sensor with many bands (possibly hundreds) is called a hyperspectral sensor.\nLet’s compare the multispectral MODIS instrument with the hyperspectral Hyperion sensor aboard the EO-1 satellite, which is also available in Earth Engine.\n\n11.4.4.1 MODIS\nThere is an easy way to check the number of bands in an image:\n/////  \n// Explore spectral resolution  \n/////  \n  \n// Get the MODIS band names as an ee.List  \nvar modisBands = modisImage.bandNames();  \n  \n// Print the list.  \nprint('MODIS bands:', modisBands);  \n  \n// Print the length of the list.  \nprint('Length of the bands list:', modisBands.length());\nNote that not all of the bands are spectral bands. As we did with the temporal resolution, let’s graph the spectral bands to examine the spectral resolution. If you ever have questions about what the different bands in the band list are, remember that you can find this information by visiting the dataset information page in Earth Engine or the data or satellite’s webpage.\n// Graph the MODIS spectral bands (bands 11-17).  \n  \n// Select only the reflectance bands of interest.  \nvar reflectanceImage = modisImage.select(   'sur_refl_b01',   'sur_refl_b02',   'sur_refl_b03',   'sur_refl_b04',   'sur_refl_b05',   'sur_refl_b06',   'sur_refl_b07'  \n);\nAs before, we’ll customize the chart to make it easier to read.\n// Define an object of customization parameters for the chart.  \nvar options = {  \n   title: 'MODIS spectrum at SFO',  \n   hAxis: {  \n       title: 'Band'   },  \n   vAxis: {  \n       title: 'Reflectance'   },  \n   legend: {  \n       position: 'none'   },  \n   pointSize: 3  \n};\nAnd create a chart using the ui.Chart.image.regions function.\n// Make the chart.  \nvar modisReflectanceChart = ui.Chart.image.regions({  \n   image: reflectanceImage,  \n   regions: sfoPoint  \n}).setOptions(options);  \n  \n// Display the chart.  \nprint(modisReflectanceChart);\nThe resulting chart is shown in Fig. F1.3.17. Use the expand button in the upper right to see a larger version of the chart than the one printed to the Console.\n\n\n\nFig. F1.3.17 Plot of TOA reflectance for MODIS\n\n\n\n\n11.4.4.2 EO-1\nNow let’s compare MODIS with the EO-1 satellite’s hyperspectral sensor. Search for “eo-1” and import the “EO-1 Hyperion Hyperspectral Imager” dataset. Name it eo1. We can look at the number of bands from the EO-1 sensor.\n// Get the EO-1 band names as a ee.List  \nvar eo1Image = eo1   .filterDate('2015-01-01', '2016-01-01')  \n   .first();  \n  \n// Extract the EO-1 band names.  \nvar eo1Bands = eo1Image.bandNames();  \n  \n// Print the list of band names.  \nprint('EO-1 bands:', eo1Bands);\nExamine the list of bands that are printed in the Console. Notice how many more bands the hyperspectral instrument provides.\nNow let’s create a reflectance chart as we did with the MODIS data.\n// Create an options object for our chart.  \nvar optionsEO1 = {  \n   title: 'EO1 spectrum',  \n   hAxis: {  \n       title: 'Band'   },  \n   vAxis: {  \n       title: 'Reflectance'   },  \n   legend: {  \n       position: 'none'   },  \n   pointSize: 3  \n};  \n  \n// Make the chart and set the options.  \nvar eo1Chart = ui.Chart.image.regions({  \n   image: eo1Image,  \n   regions: ee.Geometry.Point([6.10, 81.12])  \n}).setOptions(optionsEO1);  \n  \n// Display the chart.  \nprint(eo1Chart);\nThe resulting chart is seen in Fig. F1.3.18. There are so many bands that their names only appear as “…”!\n\n\n\nFig. F1.3.18 Plot of TOA reflectance for EO-1 as displayed in the Console. Note the button to expand the plot in the upper right hand corner.\n\n\nIf we click on the expand icon in the top right corner of the chart, it’s a little easier to see the band identifiers, as shown in Fig. F1.3.19. \n\n\n\nFig. F1.3.19 Expanded plot of TOA reflectance for EO-1\n\n\nCompare this hyperspectral instrument chart with the multispectral chart we plotted above for MODIS.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F13c. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n\n11.4.5 Per-Pixel Quality\nAs you saw above, an image consists of many bands. Some of these bands contain spectral responses of Earth’s surface, including the NIR, red, and green bands we examined in the Spectral Resolution section. What about the other bands? Some of these other bands contain valuable information, like pixel-by-pixel quality-control data.\nFor example, Sentinel-2 has a QA60 band, which contains the surface reflectance quality assurance information. Let’s map it to inspect the values.\n/////  \n// Examine pixel quality  \n/////  \n  \n// Sentinel Quality Visualization.  \nvar msiCloud = msi   .filterBounds(Map.getCenter())  \n   .filterDate('2019-12-31', '2020-02-01')  \n   .first();  \n  \n// Display the MSI image as a false color composite.  \nMap.addLayer(msiCloud,  \n   {  \n       bands: ['B8', 'B4', 'B3'],  \n       min: 0,  \n       max: 2000   },   'MSI Quality Image');  \n  \nMap.addLayer(msiCloud,  \n   {  \n       bands: ['QA60'],  \n       min: 0,  \n       max: 2000   },   'Sentinel Quality Visualization');\nUse the Inspector tool to examine some of the values. You may see values of 0 (black), 1024 (gray), and 2048 (white). The QA60 band has values of 1024 for opaque clouds, and 2048 for cirrus clouds. Compare the false-color image with the QA60 band to see these values. More information about how to interpret these complex values is given in Chap. F4.3, which explains the treatment of clouds.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F13d. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n11.4.6 Metadata\nIn addition to band imagery and per-pixel quality flags, Earth Engine allows you to access substantial amounts of metadata associated with an image. This can all be easily printed to the Console for a single image.\nLet’s examine the metadata for the Sentinel-2 MSI.\n/////  \n// Metadata  \n/////  \nprint('MSI Image Metadata', msiImage);\nExamine the object you’ve created in the Console (Fig. F1.3.20). Expand the image name, then the properties object.\n\nFig. F1.3.20 Checking the “CLOUDY_PIXEL_PERCENTAGE” property in the metadata for Sentinel-2\nThe first entry is the CLOUDY_PIXEL_PERCENTAGE information. Distinct from the cloudiness flag attached to every pixel, this is an image-level summary assessment of the overall cloudiness in the image. In addition to viewing the value, you might find it useful to print it to the screen, for example, or to record a list of cloudiness values in a set of images. Metadata properties can be extracted from an image’s properties using the get function, and printed to the Console.\n// Image-level Cloud info  \nvar msiCloudiness = msiImage.get('CLOUDY_PIXEL_PERCENTAGE');  \n  \nprint('MSI CLOUDY_PIXEL_PERCENTAGE:', msiCloudiness);\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F13e. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\nConclusion\nA good understanding of the characteristics of your images is critical to your work in Earth Engine and the chapters going forward. You now know how to observe and query a variety of remote sensing datasets, and can choose among them for your work. For example, if you are interested in change detection, you might require a dataset with spectral resolution including near-infrared imagery and a fine temporal resolution. For analyses at a continental scale, you may prefer data with a coarse spatial scale, while analyses for specific forest stands may benefit from a very fine spatial scale.\n\n\nReferences\nFisher JRB, Acosta EA, Dennedy-Frank PJ, et al (2018) Impact of satellite imagery spatial resolution on land use classification accuracy and modeled water quality. Remote Sens Ecol Conserv 4:137–149. https://doi.org/10.1002/rse2.61"
  },
  {
    "objectID": "F2.html#image-manipulation-bands-arithmetic-thresholds-and-masks",
    "href": "F2.html#image-manipulation-bands-arithmetic-thresholds-and-masks",
    "title": "12  Interpreting Images",
    "section": "12.1 Image Manipulation: Bands, Arithmetic, Thresholds, and Masks",
    "text": "12.1 Image Manipulation: Bands, Arithmetic, Thresholds, and Masks\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthor\nKaren Dyson, Andréa Puzzi Nicolau, David Saah, and Nicholas Clinton\n\n\nOverview\nOnce images have been identified in Earth Engine, they can be viewed in a wide array of band combinations for targeted purposes. For users who are already versed in remote sensing concepts, this chapter shows how to do familiar tasks on this platform; for those who are entirely new to such concepts, it introduces the idea of band combinations.\n\n\nLearning Outcomes\n\nUnderstanding what spectral indices are and why they are useful.\nBeing introduced to a range of example spectral indices used for a variety of purposes.\n\n\n\nAssumes you know how to:\n\nImport images and image collections, filter, and visualize (Part F1).\n\n\n\n\n\nIntroduction\nSpectral indices are based on the fact that different objects and land covers on the Earth’s surface reflect different amounts of light from the Sun at different wavelengths. In the visible part of the spectrum, for example, a healthy green plant reflects a large amount of green light while absorbing blue and red light—which is why it appears green to our eyes. Light also arrives from the Sun at wavelengths outside what the human eye can see, and there are large differences in reflectances between living and nonliving land covers, and between different types of vegetation, both in the visible and outside the visible wavelengths. We visualized this earlier, in Chaps. F1.1 and F1.3 when we mapped color-infrared images (Fig. F2.0.1).\n\n\n\nFig. F2.0.1 Mapped color-IR images from multiple satellite sensors that we mapped in Chap. F1.3. The near infrared spectrum is mapped as red, showing where there are high amounts of healthy vegetation.\n\n\nIf we graph the amount of light (reflectance) at different wavelengths that an object or land cover reflects, we can visualize this more easily (Fig. F2.0.2). For example, look at the reflectance curves for soil and water in the graph below. Soil and water both have relatively low reflectance at wavelengths around 300 nm (ultraviolet and violet light). Conversely, at wavelengths above 700 nm (red and infrared light) soil has relatively high reflectance, while water has very low reflectance. Vegetation, meanwhile, generally reflects large amounts of near infrared light, relative to other land covers.\n\n\n\nFig. F2.0.2 A graph of the amount of reflectance for different objects on the Earth’s surface at different wavelengths in the visible and infrared portions of the electromagnetic spectrum. 1 micrometer (µm) = 1,000 nanometers (nm).\n\n\nSpectral indices use math to express how objects reflect light across multiple portions of the spectrum as a single number. Indices combine multiple bands, often with simple operations of subtraction and division, to create a single value across an image that is intended to help to distinguish particular land uses or land covers of interest. Using Fig. F2.0.2, you can imagine which wavelengths might be the most informative for distinguishing among a variety of land covers. We will explore a variety of calculations made from combinations of bands in the following sections.\nIndices derived from satellite imagery are used as the basis of many remote-sensing analyses. Indices have been used in thousands of applications, from detecting anthropogenic deforestation to examining crop health. For example, the growth of economically important crops such as wheat and cotton can be monitored throughout the growing season: Bare soil reflects more red wavelengths, whereas growing crops reflect more of the near-infrared (NIR) wavelengths. Thus, calculating a ratio of these two bands can help monitor how well crops are growing (Jackson and Huete 1991).\n\n\n12.1.1 Band Arithmetic in Earth Engine\nIf you have not already done so, be sure to add the book’s code repository to the Code Editor by entering https://code.earthengine.google.com/?accept_repo=projects/gee-edu/book into your browser. The book’s scripts will then be available in the script manager panel. If you have trouble finding the repo, you can visit this link for help.\nMany indices can be calculated using band arithmetic in Earth Engine. Band arithmetic is the process of adding, subtracting, multiplying, or dividing two or more bands from an image. Here we’ll first do this manually, and then show you some more efficient ways to perform band arithmetic in Earth Engine.\n\n12.1.1.1 Arithmetic Calculation of NDVI\nThe red and near-infrared bands provide a lot of information about vegetation due to vegetation’s high reflectance in these wavelengths. Take a look at Fig. F2.0.2 and note, in particular, that vegetation curves (graphed in green) have relatively high reflectance in the NIR range (approximately 750–900 nm). Also note that vegetation has low reflectance in the red range (approximately 630–690 nm), where sunlight is absorbed by chlorophyll. This suggests that if the red and near-infrared bands could be combined, they would provide substantial information about vegetation.\nSoon after the launch of Landsat 1 in 1972, analysts worked to devise a robust single value that would convey the health of vegetation along a scale of −1 to 1. This yielded the NDVI, using the formula:\n (F2.0.1)\nwhere NIR and red refer to the brightness of each of those two bands. As seen in Chaps. F1.1 and F1.2, this brightness might be conveyed in units of reflectance, radiance, or digital number (DN); the NDVI is intended to give nearly equivalent values across platforms that use these wavelengths. The general form of this equation is called a “normalized difference”—the numerator is the “difference” and the denominator “normalizes” the value. Outputs for NDVI vary between −1 and 1. High amounts of green vegetation have values around 0.8–0.9. Absence of green leaves gives values near 0, and water gives values near −1.\nTo compute the NDVI, we will introduce Earth Engine’s implementation of band arithmetic. Cloud-based band arithmetic is one of the most powerful aspects of Earth Engine, because the platform’s computers are optimized for this type of heavy processing. Arithmetic on bands can be done even at planetary scale very quickly—an idea that was out of reach before the advent of cloud-based remote sensing. Earth Engine automatically partitions calculations across a large number of computers as needed, and assembles the answer for display.\nAs an example, let’s examine an image of San Francisco (​​Fig. F2.0.3).\n/////  \n// Band Arithmetic  \n/////  \n  \n// Calculate NDVI using Sentinel 2  \n  \n// Import and filter imagery by location and date.  \nvar sfoPoint = ee.Geometry.Point(-122.3774, 37.6194);  \nvar sfoImage = ee.ImageCollection('COPERNICUS/S2')  \n   .filterBounds(sfoPoint)  \n   .filterDate('2020-02-01', '2020-04-01')  \n   .first();  \n  \n// Display the image as a false color composite.  \nMap.centerObject(sfoImage, 11);  \nMap.addLayer(sfoImage, {  \n   bands: ['B8', 'B4', 'B3'],  \n   min: 0,  \n   max: 2000}, 'False color');\n\n\n\nFig. F2.0.3 False color Sentinel-2 imagery of San Francisco and surroundings\n\n\nThe simplest mathematical operations in Earth Engine are the add, subtract, multiply, and divide methods. Let’s select the near-infrared and red bands and use these operations to calculate NDVI for our image.\n// Extract the near infrared and red bands.  \nvar nir = sfoImage.select('B8');  \nvar red = sfoImage.select('B4');  \n  \n// Calculate the numerator and the denominator using subtraction and addition respectively.  \nvar numerator = nir.subtract(red);  \nvar denominator = nir.add(red);  \n  \n// Now calculate NDVI.  \nvar ndvi = numerator.divide(denominator);  \n  \n// Add the layer to our map with a palette.  \nvar vegPalette = ['red', 'white', 'green'];  \nMap.addLayer(ndvi, {  \n   min: -1,  \n   max: 1,  \n   palette: vegPalette  \n}, 'NDVI Manual');\nExamine the resulting index, using the Inspector to pick out pixel values in areas of vegetation and non-vegetation if desired.\n\n\n\nFig. F2.0.4 NDVI calculated using Sentinel-2. Remember that outputs for NDVI vary between −1 and 1. High amounts of green vegetation have values around 0.8–0.9. Absence of green leaves gives values near 0, and water gives values near −1.\n\n\nUsing these simple arithmetic tools, you can build almost any index, or develop and visualize your own. Earth Engine allows you to quickly and easily calculate and display the index across a large area.\n\n\n12.1.1.2 Single-Operation Computation of Normalized Difference for NDVI\nNormalized differences like NDVI are so common in remote sensing that Earth Engine provides the ability to do that particular sequence of subtraction, addition, and division in a single step, using the normalizedDifference method. This method takes an input image, along with bands you specify, and creates a normalized difference of those two bands. The NDVI computation previously created with band arithmetic can be replaced with one line of code:\n// Now use the built-in normalizedDifference function to achieve the same outcome.  \nvar ndviND = sfoImage.normalizedDifference(['B8', 'B4']);  \nMap.addLayer(ndviND, {  \n   min: -1,  \n   max: 1,  \n   palette: vegPalette  \n}, 'NDVI normalizedDiff');\nNote that the order in which you provide the two bands to normalizedDifference is important. We use B8, the near-infrared band, as the first parameter, and the red band B4 as the second. If your two computations of NDVI do not look identical when drawn to the screen, check to make sure that the order you have for the NIR and red bands is correct.\n\n\n12.1.1.3 Using Normalized Difference for NDWI\nAs mentioned, the normalized difference approach is used for many different indices. Let’s apply the same normalizedDifference method to another index.\nThe Normalized Difference Water Index (NDWI) was developed by Gao (1996) as an index of vegetation water content. The index is sensitive to changes in the liquid content of vegetation canopies. This means that the index can be used, for example, to detect vegetation experiencing drought conditions or differentiate crop irrigation levels. In dry areas, crops that are irrigated can be differentiated from natural vegetation. It is also sometimes called the Normalized Difference Moisture Index (NDMI). NDWI is formulated as follows:\n\n\n\n(F2.0.2)\n\n\nwhere NIR is near-infrared, centered near 860 nm (0.86 μm), and SWIR is short-wave infrared, centered near 1,240 nm (1.24 μm).\nCompute and display NDWI in Earth Engine using the normalizedDifference method. Remember that for Sentinel-2, B8 is the NIR band and B11 is the SWIR band (refer to Chaps. F1.1 and F1.3 to find information about imagery bands).\n// Use normalizedDifference to calculate NDWI  \nvar ndwi = sfoImage.normalizedDifference(['B8', 'B11']);  \nvar waterPalette = ['white', 'blue'];  \nMap.addLayer(ndwi, {  \n   min: -0.5,  \n   max: 1,  \n   palette: waterPalette  \n}, 'NDWI');\nExamine the areas of the map that NDVI identified as having a lot of vegetation. Notice which are more blue. This is vegetation that has higher water content.\n\n\n\nFig. F2.0.5 NDWI displayed for Sentinel-2 over San Francisco\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F20a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n\n12.1.2 Thresholding, Masking, and Remapping Images\nThe previous section in this chapter discussed how to use band arithmetic to manipulate images. Those methods created new continuous values by combining bands within an image. This section uses logical operators to categorize band or index values to create a categorized image.\n\n12.1.2.1 Implementing a Threshold\nImplementing a threshold uses a number (the threshold value) and logical operators to help us partition the variability of images into categories. For example, recall our map of NDVI. High amounts of vegetation have NDVI values near 1 and non-vegetated areas are near 0. If we want to see what areas of the map have vegetation, we can use a threshold to generalize the NDVI value in each pixel as being either “no vegetation” or “vegetation”. That is a substantial simplification, to be sure, but can help us to better comprehend the rich variation on the Earth’s surface. This type of categorization may be useful if, for example, we want to look at the proportion of a city that is vegetated. Let’s create a Sentinel-2 map of NDVI near Seattle, Washington, USA. Enter the code below in a new script.\n// Create an NDVI image using Sentinel 2.  \nvar seaPoint = ee.Geometry.Point(-122.2040, 47.6221);  \nvar seaImage = ee.ImageCollection('COPERNICUS/S2')  \n   .filterBounds(seaPoint)  \n   .filterDate('2020-08-15', '2020-10-01')  \n   .first();  \n  \nvar seaNDVI = seaImage.normalizedDifference(['B8', 'B4']);  \n  \n// And map it.  \nMap.centerObject(seaPoint, 10);  \nvar vegPalette = ['red', 'white', 'green'];  \nMap.addLayer(seaNDVI,  \n   {  \n       min: -1,  \n       max: 1,  \n       palette: vegPalette  \n   },   'NDVI Seattle');\n\n\n\nFig. F2.0.6 NDVI image of Sentinel-2 imagery over Seattle, Washington, USA\n\n\nInspect the image. We can see that vegetated areas are darker green while non-vegetated locations are white and water is pink. If we use the Inspector to query our image, we can see that parks and other forested areas have an NDVI over about 0.5. Thus, it would make sense to define areas with NDVI values greater than 0.5 as forested, and those below that threshold as not forested.\nNow let’s define that value as a threshold and use it to threshold our vegetated areas.\n// Implement a threshold.  \nvar seaVeg = seaNDVI.gt(0.5);  \n  \n// Map the threshold.  \nMap.addLayer(seaVeg,  \n   {  \n       min: 0,  \n       max: 1,  \n       palette: ['white', 'green']  \n   },   'Non-forest vs. Forest');\nThe gt method is from the family of Boolean operators—that is, gt is a function that performs a test in each pixel and returns the value 1 if the test evaluates to true, and 0 otherwise. Here, for every pixel in the image, it tests whether the NDVI value is greater than 0.5. When this condition is met, the layer seaVeg gets the value 1. When the condition is false, it receives the value 0.\n\n\n\nFig. F2.0.7 Thresholded forest and non-forest image based on NDVI for Seattle, Washington, USA\n\n\nUse the Inspector tool to explore this new layer. If you click on a green location, that NDVI should be greater than 0.5. If you click on a white pixel, the NDVI value should be equal to or less than 0.5.\nOther operators in this Boolean family include less than (lt), less than or equal to (lte), equal to (eq), not equal to (neq), and greater than or equal to (gte) and more.\n\n\n12.1.2.2 Building Complex Categorizations with .where\nA binary map classifying NDVI is very useful. However, there are situations where you may want to split your image into more than two bins. Earth Engine provides a tool, the where method, that conditionally evaluates to true or false within each pixel depending on the outcome of a test. This is analogous to an if statement seen commonly in other languages. However, to perform this logic when programming for Earth Engine, we avoid using the JavaScript if statement. Importantly, JavaScript if commands are not calculated on Google’s servers, and can create serious problems when running your code—in effect, the servers try to ship all of the information to be executed to your own computer’s browser, which is very underequipped for such enormous tasks. Instead, we use the where clause for conditional logic.\nSuppose instead of just splitting the forested areas from the non-forested areas in our NDVI, we want to split the image into likely water, non-forested, and forested areas. We can use where and thresholds of -0.1 and 0.5. We will start by creating an image using ee.Image. We then clip the new image so that it covers the same area as our seaNDVI layer.\n// Implement .where.  \n// Create a starting image with all values = 1.  \nvar seaWhere = ee.Image(1)   // Use clip to constrain the size of the new image.   .clip(seaNDVI.geometry());  \n  \n// Make all NDVI values less than -0.1 equal 0.  \nseaWhere = seaWhere.where(seaNDVI.lte(-0.1), 0);  \n  \n// Make all NDVI values greater than 0.5 equal 2.  \nseaWhere = seaWhere.where(seaNDVI.gte(0.5), 2);  \n  \n// Map our layer that has been divided into three classes.  \nMap.addLayer(seaWhere,  \n   {  \n       min: 0,  \n       max: 2,  \n       palette: ['blue', 'white', 'green']  \n   },   'Water, Non-forest, Forest');\nThere are a few interesting things to note about this code that you may not have seen before. First, we’re not defining a new variable for each where call. As a result, we can perform many where calls without creating a new variable each time and needing to keep track of them. Second, when we created the starting image, we set the value to 1. This means that we could easily set the bottom and top values with one where clause each. Finally, while we did not do it here, we can combine multiple where clauses using and and or. For example, we could identify pixels with an intermediate level of NDVI using seaNDVI.gte(-0.1).and(seaNDVI.lt(0.5)).\n\n\n\nFig. F2.0.8 Thresholded water, forest, and non-forest image based on NDVI for Seattle, Washington, USA.\n\n\n\n\n12.1.2.3 Masking Specific Values in an Image\nMasking an image is a technique that removes specific areas of an image—those covered by the mask—from being displayed or analyzed. Earth Engine allows you to both view the current mask and update the mask.\n// Implement masking.  \n// View the seaVeg layer's current mask.  \nMap.centerObject(seaPoint, 9);  \nMap.addLayer(seaVeg.mask(), {}, 'seaVeg Mask');\n\n\n\nFig. F2.0.9 The existing mask for the seaVeg layer we created previously\n\n\nYou can use the Inspector to see that the black area is masked and the white area has a constant value of 1. This means that data values are mapped and available for analysis within the white area only.\nNow suppose we only want to display and conduct analyses in the forested areas. Let’s mask out the non-forested areas from our image. First, we create a binary mask using the equals (eq) method.\n// Create a binary mask of non-forest.  \nvar vegMask = seaVeg.eq(1);\nIn making a mask, you set the values you want to see and analyze to be a number greater than 0. The idea is to set unwanted values to get the value of 0. Pixels that had 0 values become masked out (in practice, they do not appear on the screen at all) once we use the updateMask method to add these values to the existing mask.\n// Update the seaVeg mask with the non-forest mask.  \nvar maskedVeg = seaVeg.updateMask(vegMask);  \n  \n// Map the updated Veg layer  \nMap.addLayer(maskedVeg,  \n   {  \n       min: 0,  \n       max: 1,  \n       palette: ['green']  \n   },   'Masked Forest Layer');\nTurn off all of the other layers. You can see how the maskedVeg layer now has masked out all non-forested areas.\n\n\n\nFig. F2.0.10 An updated mask now displays only the forested areas. Non-forested areas are masked out and transparent.\n\n\nMap the updated mask for the layer and you can see why this is.\n// Map the updated mask  \nMap.addLayer(maskedVeg.mask(), {}, 'maskedVeg Mask');\n\n\n\nFig. F2.0.11 The updated mask. Areas of non-forest are now masked out as well (black areas of the image).\n\n\n\n\n12.1.2.4 Remapping Values in an Image\nRemapping takes specific values in an image and assigns them a different value. This is particularly useful for categorical datasets, including those you read about in Chap. F1.2 and those we have created earlier in this chapter.\nLet’s use the remap method to change the values for our seaWhere layer. Note that since we’re changing the middle value to be the largest, we’ll need to adjust our palette as well.\n// Implement remapping.  \n// Remap the values from the seaWhere layer.  \nvar seaRemap = seaWhere.remap([0, 1, 2], // Existing values.   [9, 11, 10]); // Remapped values.  \n  \nMap.addLayer(seaRemap,  \n   {  \n       min: 9,  \n       max: 11,  \n       palette: ['blue', 'green', 'white']  \n   },   'Remapped Values');\nUse the inspector to compare values between our original seaWhere (displayed as Water, Non-Forest, Forest) and the seaRemap, marked as “Remapped Values.” Click on a forested area and you should see that the Remapped Values should be 10, instead of 2 (Fig. F2.0.12).\n\n\n\nFig. F2.0.12 For forested areas, the remapped layer has a value of 10, compared with the original layer, which has a value of 2. You may have more layers in your Inspector.\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F20b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n\nConclusion\nIn this chapter, you learned how to select multiple bands from an image and calculate indices. You also learned about thresholding values in an image, slicing them into multiple categories using thresholds. It is also possible to work with one set of class numbers and remap them quickly to another set. Using these techniques, you have some of the basic tools of image manipulation. In subsequent chapters you will encounter more complex and specialized image manipulation techniques, including pixel-based image transformations (Chap. F3.1), neighborhood-based image transformations (Chap. F3.2), and object-based image analysis (Chap. F3.3).\n\n\nReferences\nBaig MHA, Zhang L, Shuai T, Tong Q (2014) Derivation of a tasselled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sens Lett 5:423–431. https://doi.org/10.1080/2150704X.2014.915434\nCrist EP (1985) A TM tasseled cap equivalent transformation for reflectance factor data. Remote Sens Environ 17:301–306. https://doi.org/10.1016/0034-4257(85)90102-6\nDrury SA (1987) Image interpretation in geology. Geocarto Int 2:48. https://doi.org/10.1080/10106048709354098\nGao BC (1996) NDWI - A normalized difference water index for remote sensing of vegetation liquid water from space. Remote Sens Environ 58:257–266. https://doi.org/10.1016/S0034-4257(96)00067-3\nHuang C, Wylie B, Yang L, et al (2002) Derivation of a tasselled cap transformation based on Landsat 7 at-satellite reflectance. Int J Remote Sens 23:1741–1748. https://doi.org/10.1080/01431160110106113\nJackson RD, Huete AR (1991) Interpreting vegetation indices. Prev Vet Med 11:185–200. https://doi.org/10.1016/S0167-5877(05)80004-2\nMartín MP (1998) Cartografía e inventario de incendios forestales en la Península Ibérica a partir de imágenes NOAA-AVHRR. Universidad de Alcalá\nMcFeeters SK (1996) The use of the Normalized Difference Water Index (NDWI) in the delineation of open water features. Int J Remote Sens 17:1425–1432. https://doi.org/10.1080/01431169608948714\nNath B, Niu Z, Mitra AK (2019) Observation of short-term variations in the clay minerals ratio after the 2015 Chile great earthquake (8.3 Mw) using Landsat 8 OLI data. J Earth Syst Sci 128:1–21. https://doi.org/10.1007/s12040-019-1129-2\nSchultz M, Clevers JGPW, Carter S, et al (2016) Performance of vegetation indices from Landsat time series in deforestation monitoring. Int J Appl Earth Obs Geoinf 52:318–327. https://doi.org/10.1016/j.jag.2016.06.020\nSegal D (1982) Theoretical basis for differentiation of ferric-iron bearing minerals, using Landsat MSS data. In: Proceedings of Symposium for Remote Sensing of Environment, 2nd Thematic Conference on Remote Sensing for Exploratory Geology, Fort Worth, TX. pp 949–951\nSouza Jr CM, Roberts DA, Cochrane MA (2005) Combining spectral and spatial information to map canopy damage from selective logging and forest fires. Remote Sens Environ 98:329–343. https://doi.org/10.1016/j.rse.2005.07.013\nSouza Jr CM, Siqueira JV, Sales MH, et al (2013) Ten-year Landsat classification of deforestation and forest degradation in the Brazilian Amazon. Remote Sens 5:5493–5513. https://doi.org/10.3390/rs5115493"
  },
  {
    "objectID": "F2.html#interpreting-an-image-classification",
    "href": "F2.html#interpreting-an-image-classification",
    "title": "12  Interpreting Images",
    "section": "12.2 Interpreting an Image: Classification",
    "text": "12.2 Interpreting an Image: Classification\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthor\nAndréa Puzzi Nicolau, Karen Dyson, David Saah, Nicholas Clinton\n\n\nOverview\nImage classification is a fundamental goal of remote sensing. It takes the user from viewing an image to labeling its contents. This chapter introduces readers to the concept of classification and walks users through the many options for image classification in Earth Engine. You will explore the processes of training data collection, classifier selection, classifier training, and image classification.\n\n\nLearning Outcomes\n\nRunning a classification in Earth Engine.\nUnderstanding the difference between supervised and unsupervised classification.\nLearning how to use Earth Engine geometry drawing tools.\nLearning how to collect sample data in Earth Engine.\nLearning the basics of the hexadecimal numbering system.\n\n\n\nAssumes you know how to:\n\nImport images and image collections, filter, and visualize (Part F1).\nUnderstand bands and how to select them (Chap. F1.2, Chap. F2.0).\n\n\n\n\n\nIntroduction\nClassification is addressed in a broad range of fields, including mathematics, statistics, data mining, machine learning, and more. For a deeper treatment of classification, interested readers may see some of the following suggestions: Witten et al. (2011), Hastie et al. (2009), Goodfellow et al. (2016), Gareth et al. (2013), Géron (2019), Müller et al. (2016), or Witten et al. (2005). Unlike regression, which predicts continuous variables, classification predicts categorical, or discrete, variables—variables with a finite number of categories (e.g., age range).\nIn remote sensing, image classification is an attempt to categorize all pixels in an image into a finite number of labeled land cover and/or land use classes. The resulting classified image is a simplified thematic map derived from the original image (Fig. F2.1.1). Land cover and land use information is essential for many environmental and socioeconomic applications, including natural resource management, urban planning, biodiversity conservation, agricultural monitoring, and carbon accounting.\n\n\n\nFig. F2.1.1 Image classification concept\n\n\nImage classification techniques for generating land cover and land use information have been in use since the 1980s (Li et al. 2014). Here, we will cover the concepts of pixel-based supervised and unsupervised classifications, testing out different classifiers. Chapter F3.3 covers the concept and application of object-based classification.\nIt is important to define land use and land cover. Land cover relates to the physical characteristics of the surface: simply put, it documents whether an area of the Earth’s surface is covered by forests, water, impervious surfaces, etc. Land use refers to how this land is being used by people. For example, herbaceous vegetation is considered a land cover but can indicate different land uses: the grass in a pasture is an agricultural land use, whereas the grass in an urban area can be classified as a park.\n\n\n12.2.1 Supervised Classification\nIf you have not already done so, be sure to add the book’s code repository to the Code Editor by entering https://code.earthengine.google.com/?accept_repo=projects/gee-edu/book into your browser. The book’s scripts will then be available in the script manager panel. If you have trouble finding the repo, you can visit this link for help.\nSupervised classification uses a training dataset with known labels and representing the spectral characteristics of each land cover class of interest to “supervise” the classification. The overall approach of a supervised classification in Earth Engine is summarized as follows:\n\nGet a scene.\nCollect training data.\nSelect and train a classifier using the training data.\nClassify the image using the selected classifier.\n\nWe will begin by creating training data manually, based on a clear Landsat image (Fig. F2.1.2). Copy the code block below to define your Landsat 8 scene variable and add it to the map. We will use a point in Milan, Italy, as the center of the area for our image classification.\n// Create an Earth Engine Point object over Milan.  \nvar pt = ee.Geometry.Point([9.453, 45.424]);  \n  \n// Filter the Landsat 8 collection and select the least cloudy image.  \nvar landsat = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')  \n   .filterBounds(pt)  \n   .filterDate('2019-01-01', '2020-01-01')  \n   .sort('CLOUD_COVER')  \n   .first();  \n  \n// Center the map on that image.  \nMap.centerObject(landsat, 8);  \n  \n// Add Landsat image to the map.  \nvar visParams = {  \n   bands: ['SR_B4', 'SR_B3', 'SR_B2'],  \n   min: 7000,  \n   max: 12000  \n};  \nMap.addLayer(landsat, visParams, 'Landsat 8 image');\n\n\n\nFig. F2.1.2 Landsat image\n\n\nUsing the Geometry Tools, we will create points on the Landsat image that represent land cover classes of interest to use as our training data. We’ll need to do two things: (1) identify where each land cover occurs on the ground, and (2) label the points with the proper class number. For this exercise, we will use the classes and codes shown below:\n\nForest: 0\nDeveloped: 1\nWater: 2\nHerbaceous: 3\n\nIn the Geometry Tools, click on the marker option (Fig. F2.1.3). This will create a point geometry which will show up as an import named “geometry”. Click on the gear icon to configure this import.\n\n\n\nFig. F2.1.3 Creating a new layer in the Geometry Imports\n\n\nWe will start by collecting forest points, so name the import forest. Import it as a FeatureCollection, and then click + Property. Name the new property “class” and give it a value of 0 (Fig. F2.1.4). We can also choose a color to represent this class. For a forest class, it is natural to choose a green color. You can choose the color you prefer by clicking on it, or, for more control, you can use a hexadecimal value.\nHexadecimal values are used throughout the digital world to represent specific colors across computers and operating systems. They are specified by six values arranged in three pairs, with one pair each for the red, green, and blue brightness values. If you’re unfamiliar with hexadecimal values, imagine for a moment that colors were specified in pairs of base 10 numbers instead of pairs of base 16. In that case, a bright pure red value would be “990000”; a bright pure green value would be “009900”; and a bright pure blue value would be “000099”. A value like “501263” would be a mixture of the three colors, not especially bright, having roughly equal amounts of blue and red, and much less green: a color that would be a shade of purple. To create numbers in the hexadecimal system, which might feel entirely natural if humans had evolved to have 16 fingers, sixteen “digits” are needed: a base 16 counter goes 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, then 10, 11, and so on. Given that counting framework, the number “FF” is like “99” in base 10: the largest two-digit number. The hexadecimal color used for coloring the letters of the word FeatureCollection in this book, a color with roughly equal amounts of blue and red, and much less green, is “7F1FA2”\nReturning to the coloring of the forest points, the hexadecimal value “589400” is a little bit of red, about twice as much green, and no blue: the deep green seen in Figure F2.1.4. Enter that value, with or without the “#” in front, and click OK after finishing the configuration.\n\n\n\nFig. F2.1.4 Edit geometry layer properties\n\n\nNow, in the Geometry Imports, we will see that the import has been renamed forest. Click on it to activate the drawing mode (Fig. F2.1.5) in order to start collecting forest points.\n\n\n\nFig. F2.1.5 Activate forest layer to start collection\n\n\nNow, start collecting points over forested areas (Fig. F2.1.6). Zoom in and out as needed. You can use the satellite basemap to assist you, but the basis of your collection should be the Landsat image. Remember that the more points you collect, the more the classifier will learn from the information you provide. For now, let’s set a goal to collect 25 points per class. Click Exit next to Point drawing (Fig. F2.1.5) when finished.\n\n\n\nFig. F2.1.6 Forest points\n\n\nRepeat the same process for the other classes by creating new layers (Fig. F2.1.7). Don’t forget to import using the FeatureCollection option as mentioned above. For the developed class, collect points over urban areas. For the water class, collect points over the Ligurian Sea, and also look for other bodies of water, like rivers. For the herbaceous class, collect points over agricultural fields. Remember to set the “class” property for each class to its corresponding code (see Table 2.1.1) and click Exit once you finalize collecting points for each class as mentioned above. We will be using the following hexadecimal colors for the other classes: #FF0000 for developed, #1A11FF for water, and #D0741E for herbaceous.\n\n\n\nFig. F2.1.7 New layer option in Geometry Imports\n\n\nYou should now have four FeatureCollection imports named forest, developed, water, and herbaceous (Fig. F2.1.8).\n\n\n\nFig. F2.1.8 Example of training points\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F21a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\nIf you wish to have the exact same results demonstrated in this chapter from now on, continue beginning with this Code Checkpoint. If you use the points collected yourself, the results may vary from this point forward.\nThe next step is to combine all the training feature collections into one. Copy and paste the code below to combine them into one FeatureCollection called trainingFeatures. Here, we use the flatten method to avoid having a collection of feature collections—we want individual features within our FeatureCollection.\n// Combine training feature collections.  \nvar trainingFeatures = ee.FeatureCollection([  \n   forest, developed, water, herbaceous  \n]).flatten();\nNote: Alternatively, you could use an existing set of reference data. For example, the European Space Agency (ESA) WorldCover dataset is a global map of land use and land cover derived from ESA’s Sentinel-2 imagery at 10 m resolution. With existing datasets, we can randomly place points on pixels classified as the classes of interest (if you are curious, you can explore the Earth Engine documentation to learn about the ee.Image.stratifiedSample and the ee.FeatureCollection.randomPoints methods). The drawback is that these global datasets will not always contain the specific classes of interest for your region, or may not be entirely accurate at the local scale. Another option is to use samples that were collected in the field (e.g., GPS points). In Chap. F5.0, you will see how to upload your own data as Earth Engine assets.\nIn the combined FeatureCollection, each Feature point should have a property called “class”. The class values are consecutive integers from 0 to 3 (you could verify that this is true by printing trainingFeatures and checking the properties of the features).\nNow that we have our training points, copy and paste the code below to extract the band information for each class at each point location. First, we define the prediction bands to extract different spectral and thermal information from different bands for each class. Then, we use the sampleRegions method to sample the information from the Landsat image at each point location. This method requires information about the FeatureCollection (our reference points), the property to extract (“class”), and the pixel scale (in meters).\n// Define prediction bands.  \nvar predictionBands = [   'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7',   'ST_B10'  \n];  \n  \n// Sample training points.  \nvar classifierTraining = landsat.select(predictionBands)  \n   .sampleRegions({  \n       collection: trainingFeatures,  \n       properties: ['class'],  \n       scale: 30   });\nYou can check whether the classifierTraining object extracted the properties of interest by printing it and expanding the first feature. You should see the band and class information (Fig. F2.1.9).\n\n\n\nFig. F2.1.9 Example of extracted band information for one point of class 0 (forest)\n\n\nNow we can choose a classifier. The choice of classifier is not always obvious, and there are many options from which to pick—you can quickly expand the ee.Classifier object under Docs to get an idea of how many options we have for image classification. Therefore, we will be testing different classifiers and comparing their results. We will start with a Classification and Regression Tree (CART) classifier, a well-known classification algorithm (Fig. F2.1.10) that has been around for decades.\n\n\n\nFig. F2.1.10 Example of a decision tree for satellite image classification. Values and classes are hypothetical.\n\n\nCopy and paste the code below to instantiate a CART classifier (ee.Classifier.smileCart) and train it.\n//////////////// CART Classifier ///////////////////  \n  \n// Train a CART Classifier.  \nvar classifier = ee.Classifier.smileCart().train({  \n   features: classifierTraining,  \n   classProperty: 'class',  \n   inputProperties: predictionBands  \n});\nEssentially, the classifier contains the mathematical rules that link labels to spectral information. If you print the variable classifier and expand its properties, you can confirm the basic characteristics of the object (bands, properties, and classifier being used). If you print classifier.explain, you can find a property called “tree” that contains the decision rules.\nAfter training the classifier, copy and paste the code below to classify the Landsat image and add it to the Map.\n// Classify the Landsat image.  \nvar classified = landsat.select(predictionBands).classify(classifier);  \n  \n// Define classification image visualization parameters.  \nvar classificationVis = {  \n   min: 0,  \n   max: 3,  \n   palette: ['589400', 'ff0000', '1a11ff', 'd0741e']  \n};  \n  \n// Add the classified image to the map.  \nMap.addLayer(classified, classificationVis, 'CART classified');\nNote that, in the visualization parameters, we define a palette parameter which in this case represents colors for each pixel value (0–3, our class codes). We use the same hexadecimal colors used when creating our training points for each class. This way, we can associate a color with a class when visualizing the classified image in the Map.\nInspect the result: Activate the Landsat composite layer and the satellite basemap to overlay with the classified images (Fig. F2.1.11). Change the layers’ transparency to inspect some areas. What do you notice? The result might not look very satisfactory in some areas (e.g., confusion between developed and herbaceous classes). Why do you think this is happening? There are a few options to handle misclassification errors:\n\nCollect more training data We can try incorporating more points to have a more representative sample of the classes.\nTune the model Classifiers typically have “hyperparameters,” which are set to default values. In the case of classification trees, there are ways to tune the number of leaves in the tree, for example. Tuning models is addressed in Chap. F2.2.\nTry other classifiers If a classifier’s results are unsatisfying, we can try some of the other classifiers in Earth Engine to see if the result is better or different.\nExpand the collection location It is good practice to collect points across the entire image and not just focus on one location. Also, look for pixels of the same class that show variability (e.g., for the developed class, building rooftops look different than house rooftops; for the herbaceous class, crop fields show distinctive seasonality/phenology).\nAdd more predictors We can try adding spectral indices to the input variables; this way, we are feeding the classifier new, unique information about each class. For example, there is a good chance that a vegetation index specialized for detecting vegetation health (e.g., NDVI) would improve the developed versus herbaceous classification.\n\n\n\n\nFig. F2.1.11 CART classification\n\n\nFor now, we will try another supervised learning classifier that is widely used: Random Forests (RF). The RF algorithm (Breiman 2001, Pal 2005) builds on the concept of decision trees, but adds strategies to make them more powerful. It is called a “forest” because it operates by constructing a multitude of decision trees. As mentioned previously, a decision tree creates the rules which are used to make decisions. A Random Forest will randomly choose features and make observations, build a forest of decision trees, and then use the full set of trees to estimate the class. It is a great choice when you do not have a lot of insight about the training data.\n\n\n\nFig. F2.1.12 General concept of Random Forests\n\n\nCopy and paste the code below to train the RF classifier (ee.Classifier.smileRandomForest) and apply the classifier to the image. The RF algorithm requires, as its argument, the number of trees to build. We will use 50 trees.\n/////////////// Random Forest Classifier /////////////////////  \n  \n// Train RF classifier.  \nvar RFclassifier = ee.Classifier.smileRandomForest(50).train({  \n   features: classifierTraining,  \n   classProperty: 'class',  \n   inputProperties: predictionBands  \n});  \n  \n// Classify Landsat image.  \nvar RFclassified = landsat.select(predictionBands).classify(  \n   RFclassifier);  \n  \n// Add classified image to the map.  \nMap.addLayer(RFclassified, classificationVis, 'RF classified');\nNote that in the ee.Classifier.smileRandomForest documentation (Docs tab), there is a seed (random number) parameter. Setting a seed allows you to exactly replicate your model each time you run it. Any number is acceptable as a seed.\nInspect the result (Fig. F2.1.13). How does this classified image differ from the CART one? Is the classifications better or worse? Zoom in and out and change the transparency of layers as needed. In Chap. F2.2, you will see more systematic ways to assess what is better or worse, based on accuracy metrics.\n\n\n\nFig. F2.1.13 Random Forest classified image\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F21b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n12.2.2 Unsupervised Classification\nIn an unsupervised classification, we have the opposite process of supervised classification. Spectral classes are grouped first and then categorized into clusters. Therefore, in Earth Engine, these classifiers are ee.Clusterer objects. They are “self-taught” algorithms that do not use a set of labeled training data (i.e., they are “unsupervised”). You can think of it as performing a task that you have not experienced before, starting by gathering as much information as possible. For example, imagine learning a new language without knowing the basic grammar, learning only by watching a TV series in that language, listening to examples, and finding patterns.\nSimilar to the supervised classification, unsupervised classification in Earth Engine has this workflow:\n\nAssemble features with numeric properties in which to find clusters (training data).\nSelect and instantiate a clusterer.\nTrain the clusterer with the training data.\nApply the clusterer to the scene (classification).\nLabel the clusters.\n\nIn order to generate training data, we will use the sample method, which randomly takes samples from a region (unlike sampleRegions, which takes samples from predefined locations). We will use the image’s footprint as the region by calling the geometry method. Additionally, we will define the number of pixels (numPixels) to sample—in this case, 1000 pixels—and define a tileScale of 8 to avoid computation errors due to the size of the region. Copy and paste the code below to sample 1000 pixels from the Landsat image. You should add to the same script as before to compare supervised versus unsupervised classification results at the end.\n//////////////// Unsupervised classification ////////////////  \n  \n// Make the training dataset.  \nvar training = landsat.sample({  \n   region: landsat.geometry(),  \n   scale: 30,  \n   numPixels: 1000,  \n   tileScale: 8  \n});\nNow we can instantiate a clusterer and train it. As with the supervised algorithms, there are many unsupervised algorithms to choose from. We will use the k-means clustering algorithm, which is a commonly used approach in remote sensing. This algorithm identifies groups of pixels near each other in the spectral space (image x bands) by using an iterative regrouping strategy. We define a number of clusters, k, and then the method randomly distributes that number of seed points into the spectral space. A large sample of pixels is then grouped into its closest seed, and the mean spectral value of this group is calculated. That mean value is akin to a center of mass of the points, and is known as the centroid. Each iteration recalculates the class means and reclassifies pixels with respect to the new means. This process is repeated until the centroids remain relatively stable and only a few pixels change from class to class on subsequent iterations.\n\n\n\nFig. F2.1.14 K-means visual concept\n\n\nCopy and paste the code below to request four clusters, the same number as for the supervised classification, in order to directly compare them.\n// Instantiate the clusterer and train it.  \nvar clusterer = ee.Clusterer.wekaKMeans(4).train(training);\nNow copy and paste the code below to apply the clusterer to the image and add the resulting classification to the Map (Fig. F2.1.15). Note that we are using a method called randomVisualizer to assign colors for the visualization. We are not associating the unsupervised classes with the color palette we defined earlier in the supervised classification. Instead, we are assigning random colors to the classes, since we do not yet know which of the unsupervised classes best corresponds to each of the named classes (e.g., forest , herbaceous). Note that the colors in Fig. F1.2.15 might not be the same as you see on your Map, since they are assigned randomly.\n// Cluster the input using the trained clusterer.  \nvar Kclassified = landsat.cluster(clusterer);  \n  \n// Display the clusters with random colors.  \nMap.addLayer(Kclassified.randomVisualizer(), {},   'K-means classified - random colors');\n\n\n\nFig. F2.1.15 K-means classification\n\n\nInspect the results. How does this classification compare to the previous ones? If preferred, use the Inspector to check which classes were assigned to each pixel value (“cluster” band) and change the last line of your code to apply the same palette used for the supervised classification results (see Code Checkpoint below for an example).\nAnother key point of classification is the accuracy assessment of the results. This will be covered in Chap. F2.2.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F21c. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\nConclusion\nClassification algorithms are key for many different applications because they allow you to predict categorical variables. You should now understand the difference between supervised and unsupervised classification and have the basic knowledge on how to handle misclassifications. By being able to map the landscape for land use and land cover, we will also be able to monitor how it changes (Part F4).\n\n\nReferences\nBreiman L (2001) Random forests. Mach Learn 45:5–32. https://doi.org/10.1023/A:1010933404324\nGareth J, Witten D, Hastie T, Tibshirani R (2013) An Introduction to Statistical Learning. Springer\nGéron A (2019) Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, Inc.\nGoodfellow I, Bengio Y, Courville A (2016) Deep Learning. MIT Press\nHastie T, Tibshirani R, Friedman JH (2009) The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer\nLi M, Zang S, Zhang B, et al (2014) A review of remote sensing image classification techniques: The role of spatio-contextual information. Eur J Remote Sens 47:389–411. https://doi.org/10.5721/EuJRS20144723\nMüller AC, Guido S (2016) Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Media, Inc.\nPal M (2005) Random forest classifier for remote sensing classification. Int J Remote Sens 26:217–222. https://doi.org/10.1080/01431160412331269698\nWitten IH, Frank E, Hall MA, et al (2005) Practical machine learning tools and techniques. In: Data Mining. pp 4"
  },
  {
    "objectID": "F2.html#accuracy-assessment-quantifying-classification-quality",
    "href": "F2.html#accuracy-assessment-quantifying-classification-quality",
    "title": "12  Interpreting Images",
    "section": "12.3 Accuracy Assessment: Quantifying Classification Quality",
    "text": "12.3 Accuracy Assessment: Quantifying Classification Quality\n\n\n\n\n\n\nChapter Information\n\n\n\n\nAuthor\nAndréa Puzzi Nicolau, Karen Dyson, David Saah, Nicholas Clinton\n\n\nOverview\nThis chapter will enable you to assess the accuracy of an image classification. You will learn about different metrics and ways to quantify classification quality in Earth Engine. Upon completion, you should be able to evaluate whether your classification needs improvement and know how to proceed when it does.\n\n\nLearning Outcomes\n\nLearning how to perform accuracy assessment in Earth Engine.\nUnderstanding how to generate and read a confusion matrix.\nUnderstanding overall accuracy and the kappa coefficient.\nUnderstanding the difference between user’s and producer’s accuracy, and the difference between omission and commission errors.\n\n\n\nAssumes you know how to:\n\n​​Create a graph using ui.Chart (Chap. F1.3).\nPerform a supervised Random Forest image classification (Chap. F2.1).\n\n\n\n\n\nIntroduction\nAny map or remotely sensed product is a generalization or model that will have inherent errors. Products derived from remotely sensed data used for scientific purposes and policymaking require a quantitative measure of accuracy to strengthen the confidence in the information generated (Foody 2002, Strahler et al. 2006, Olofsson et al. 2014). Accuracy assessment is a crucial part of any classification project, as it measures the degree to which the classification agrees with another data source that is considered to be accurate, ground-truth data (i.e., “reality”).\nThe history of accuracy assessment reveals increasing detail and rigor in the analysis, moving from a basic visual appraisal of the derived map (Congalton 1994, Foody 2002) to the definition of best practices for sampling and response designs and the calculation of accuracy metrics (Foody 2002, Stehman 2013, Olofsson et al. 2014, Stehman and Foody 2019). The confusion matrix (also called the “error matrix”) (Stehman 1997) summarizes key accuracy metrics used to assess products derived from remotely sensed data.\nIn Chap. F2.1, we asked whether the classification results were satisfactory. In remote sensing, the quantification of the answer to that question is called accuracy assessment. In the classification context, accuracy measurements are often derived from a confusion matrix.\nIn a thorough accuracy assessment, we think carefully about the sampling design, the response design, and the analysis (Olofsson et al. 2014). Fundamental protocols are taken into account to produce scientifically rigorous and transparent estimates of accuracy and area, which requires robust planning and time. In a standard setting, we would calculate the number of samples needed for measuring accuracy (sampling design). Here, we will focus mainly on the last step, analysis, by examining the confusion matrix and learning how to calculate the accuracy metrics. This will be done by partitioning the existing data into training and testing sets.\n\n\n12.3.1 Quantifying Classification Accuracy Through a Confusion Matrix\nIf you have not already done so, be sure to add the book’s code repository to the Code Editor by entering https://code.earthengine.google.com/?accept_repo=projects/gee-edu/book into your browser. The book’s scripts will then be available in the script manager panel. If you have trouble finding the repo, you can visit this link for help.\nTo illustrate some of the basic ideas about classification accuracy, we will revisit the data and location of part of Chap. F2.1, where we tested different classifiers and classified a Landsat image of the area around Milan, Italy. We will name this dataset ‘data’. This variable is a FeatureCollection with features containing the “class” values and spectral information of four land cover / land use classes: forest, developed, water, and herbaceous (see Fig. F2.1.8 and Fig. F2.1.9 for a refresher). We will also define a variable, predictionBands, which is a list of bands that will be used for prediction (classification)—the spectral information in the data variable.\nClass Values:\n\nForest: 0\nDeveloped: 1\nWater: 2\nHerbaceous: 3\n\nThe first step is to partition the set of known values into training and testing sets in order to have something for the classifier to predict over that it has not been shown before (the testing set), mimicking unseen data that the model might see in the future. We add a column of random numbers to our FeatureCollection using the randomColumn method. Then, we filter the features into about 80% for training and 20% for testing using ee.Filter. Copy and paste the code below to partition the data and filter features based on the random number.\n// Import the reference dataset.  \nvar data = ee.FeatureCollection(   'projects/gee-book/assets/F2-2/milan_data');  \n  \n// Define the prediction bands.  \nvar predictionBands = [   'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7',   'ST_B10',   'ndvi', 'ndwi'  \n];  \n  \n// Split the dataset into training and testing sets.  \nvar trainingTesting = data.randomColumn();  \nvar trainingSet = trainingTesting  \n   .filter(ee.Filter.lessThan('random', 0.8));  \nvar testingSet = trainingTesting  \n   .filter(ee.Filter.greaterThanOrEquals('random', 0.8));\nNote that randomColumn creates pseudorandom numbers in a deterministic way. This makes it possible to generate a reproducible pseudorandom sequence by defining the seed parameter (Earth Engine uses a seed of 0 by default). In other words, given a starting value (i.e., the seed), randomColumn will always provide the same sequence of pseudorandom numbers.\nCopy and paste the code below to train a Random Forest classifier with 50 decision trees using the trainingSet.\n// Train the Random Forest Classifier with the trainingSet.  \nvar RFclassifier = ee.Classifier.smileRandomForest(50).train({  \n   features: trainingSet,  \n   classProperty: 'class',  \n   inputProperties: predictionBands  \n});\nNow, let’s discuss what a confusion matrix is. A confusion matrix describes the quality of a classification by comparing the predicted values to the actual values. A simple example is a confusion matrix for a binary classification into the classes “positive” and “negative,” as shown in Table F2.2.1.\nTable F2.2.1 Confusion matrix for a binary classification where the classes are “positive” and “negative”\n\n\n\n\n\n\n\n\n\n\n\nActual values\n\n\n\n\n\n\n\nPositive\nNegative\n\n\nPredicted values\nPositive\nTP (true positive)\nFP (false positive)\n\n\n\nNegative\nFN (false negative)\nTN (true negative)\n\n\n\nIn Table F2.2.1, the columns represent the actual values (the truth), while the rows represent the predictions (the classification). “True positive” (TP) and “true negative” (TN) mean that the classification of a pixel matches the truth (e.g., a water pixel correctly classified as water). “False positive” (FP) and “false negative” (FN) mean that the classification of a pixel does not match the truth (e.g., a non-water pixel incorrectly classified as water).\n\nTP: classified as positive and the actual class is positive\nFP: classified as positive and the actual class is negative\nFN: classified as negative and the actual class is positive\nTN: classified as negative and the actual class is negative\n\nWe can extract some statistical information from a confusion matrix.. Let’s look at an example to make this clearer. Table F2.2.2 is a confusion matrix for a sample of 1,000 pixels for a classifier that identifies whether a pixel is forest (positive) or non-forest (negative), a binary classification.\nTable F2.2.2 Confusion matrix for a binary classification where the classes are “positive” (forest) and “negative” (non-forest)\n\n\n\n\n\nActual values\n\n\n\n\n\n\n\nPositive\nNegative\n\n\nPredicted values\nPositive\n307\n18\n\n\n\nNegative\n14\n661\n\n\n\nIn this case, the classifier correctly identified 307 forest pixels, wrongly classified 18 non-forest pixels as forest, correctly identified 661 non-forest pixels, and wrongly classified 14 forest pixels as non-forest. Therefore, the classifier was correct 968 times and wrong 32 times. Let’s calculate the main accuracy metrics for this example.\nThe overall accuracy tells us what proportion of the reference data was classified correctly, and is calculated as the total number of correctly identified pixels divided by the total number of pixels in the sample.\n\nIn this case, the overall accuracy is 96.8%, calculated using (.\nTwo other important accuracy metrics are the producer’s accuracy and the user’s accuracy, also referred to as the “recall” and the “precision,” respectively. Importantly, these metrics quantify aspects of per-class accuracy.\nThe producer’s accuracy is the accuracy of the map from the point of view of the map maker (the “producer”), and is calculated as the number of correctly identified pixels of a given class divided by the total number of pixels actually in that class. The producer’s accuracy for a given class tells us the proportion of the pixels in that class that were classified correctly.\n\n\nIn this case, the producer’s accuracy for the forest class is 95.6%, calculated using ). The producer’s accuracy for the non-forest class is 97.3%, calculated from ).\nThe user’s accuracy (also called the “consumer’s accuracy”) is the accuracy of the map from the point of view of a map user, and is calculated as the number of correctly identified pixels of a given class divided by the total number of pixels claimed to be in that class. The user’s accuracy for a given class tells us the proportion of the pixels identified on the map as being in that class that are actually in that class on the ground.\n\n\nIn this case, the user’s accuracy for the forest class is 94.5%, calculated using ). The user’s accuracy for the non-forest class is 97.9%, calculated from ).\nFig. F2.2.1 helps visualize the rows and columns used to calculate each accuracy.\n\n\n\nFig. F2.2.1 Confusion matrix for a binary classification where the classes are “positive” (forest) and “negative” (non-forest), with accuracy metrics\n\n\nIt is very common to talk about two types of error when addressing remote-sensing classification accuracy: omission errors and commission errors. Omission errors refer to the reference pixels that were left out of (omitted from) the correct class in the classified map. In a two-class system, an error of omission in one class will be counted as an error of commission in another class. Omission errors are complementary to the producer’s accuracy.\n\nCommission errors refer to the class pixels that were erroneously classified in the map and are complementary to the user’s accuracy.\n\nFinally, another commonly used accuracy metric is the kappa coefficient, which evaluates how well the classification performed as compared to random. The value of the kappa coefficient can range from −1 to 1: a negative value indicates that the classification is worse than a random assignment of categories would have been; a value of 0 indicates that the classification is no better or worse than random; and a positive value indicates that the classification is better than random.\n\nThe chance agreement is calculated as the sum of the product of row and column totals for each class, and the observed accuracy is the overall accuracy. Therefore, for our example, the kappa coefficient is 0.927.\n\nNow, let’s go back to the script. In Earth Engine, there are API calls for these operations. Note that our confusion matrix will be a 4 x 4 table, since we have four different classes.\nCopy and paste the code below to classify the testingSet and get a confusion matrix using the method errorMatrix. Note that the classifier automatically adds a property called “classification,” which is compared to the “class” property of the reference dataset.\n// Now, to test the classification (verify model's accuracy),  \n// we classify the testingSet and get a confusion matrix.  \nvar confusionMatrix = testingSet.classify(RFclassifier)  \n   .errorMatrix({  \n       actual: 'class',  \n       predicted: 'classification'   });\nCopy and paste the code below to print the confusion matrix and accuracy metrics. Expand the confusion matrix object to inspect it. The entries represent the number of pixels. Items on the diagonal represent correct classification. Items off the diagonal are misclassifications, where the class in row i is classified as column j (values from 0 to 3 correspond to our class codes: forest, developed, water, and herbaceous, respectively). Also expand the producer’s accuracy, user’s accuracy (consumer’s accuracy), and kappa coefficient objects to inspect them.\n// Print the results.  \nprint('Confusion matrix:', confusionMatrix);  \nprint('Overall Accuracy:', confusionMatrix.accuracy());  \nprint('Producers Accuracy:', confusionMatrix.producersAccuracy());  \nprint('Consumers Accuracy:', confusionMatrix.consumersAccuracy());  \nprint('Kappa:', confusionMatrix.kappa());\nHow is the classification accuracy? Which classes have higher accuracy compared to the others? Can you think of any reasons why? (Hint: Check where the errors in these classes are in the confusion matrix—i.e., being committed and omitted.)\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F22a. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\n12.3.2 Hyperparameter tuning\nWe can also assess how the number of trees in the Random Forest classifier affects the classification accuracy. Copy and paste the code below to create a function that charts the overall accuracy versus the number of trees used. The code tests from 5 to 100 trees at increments of 5, producing Fig. F2.2.2. (Do not worry too much about fully understanding each item at this stage of your learning. If you want to find out how these operations work, you can see more in Chaps. F4.0 and F4.1.)\n// Hyperparameter tuning.  \nvar numTrees = ee.List.sequence(5, 100, 5);  \n  \nvar accuracies = numTrees.map(function(t) {   var classifier = ee.Classifier.smileRandomForest(t)  \n       .train({  \n           features: trainingSet,  \n           classProperty: 'class',  \n           inputProperties: predictionBands  \n       });   return testingSet  \n       .classify(classifier)  \n       .errorMatrix('class', 'classification')  \n       .accuracy();  \n});  \n  \nprint(ui.Chart.array.values({  \n   array: ee.Array(accuracies),  \n   axis: 0,  \n   xLabels: numTrees  \n}).setOptions({  \n   hAxis: {  \n       title: 'Number of trees'   },  \n   vAxis: {  \n       title: 'Accuracy'   },  \n   title: 'Accuracy per number of trees'  \n}));\n\n\n\nFig. F2.2.2 Chart showing accuracy per number of Random Forest trees\n\n\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F22b. The book’s repository contains a script that shows what your code should look like at this point.\n\n\nSection 3. Spatial autocorrelation\nWe might also want to ensure that the samples from the training set are uncorrelated with the samples from the testing set. This might result from the spatial autocorrelation of the phenomenon being predicted. One way to exclude samples that might be correlated in this manner is to remove samples that are within some distance to any other sample. In Earth Engine, this can be accomplished with a spatial join. The following Code Checkpoint replicates Sect. 1 but with a spatial join that excludes training points that are less than 1000 meters distant from testing points.\n\n\n\n\n\n\nNote\n\n\n\nCode Checkpoint F22c. The book’s repository contains a script that shows what your code should look like at this point.\n\n\n\n\nConclusion\nYou should now understand how to calculate how well your classifier is performing on the data used to build the model. This is a useful way to understand how a classifier is performing, because it can help indicate which classes are performing better than others. A poorly modeled class can sometimes be improved by, for example, collecting more training points for that class.\nNevertheless, a model may work well on training data but work poorly in locations randomly chosen in the study area. To understand a model’s behavior on testing data, analysts employ protocols required to produce scientifically rigorous and transparent estimates of the accuracy and area of each class in the study region. We will not explore those practices in this chapter, but if you are interested, there are tutorials and papers available online that can guide you through the process. Links to some of those tutorials can be found in the “For Further Reading” section of this book.\nReferences\nCongalton R (1994) Accuracy assessment of remotely sensed data: Future needs and directions. In: Proceedings of Pecora 12 land information from space-based systems. pp 385–388\nFoody GM (2002) Status of land cover classification accuracy assessment. Remote Sens Environ 80:185–201. https://doi.org/10.1016/S0034-4257(01)00295-4\nOlofsson P, Foody GM, Herold M, et al (2014) Good practices for estimating area and assessing accuracy of land change. Remote Sens Environ 148:42–57. https://doi.org/10.1016/j.rse.2014.02.015\nStehman SV (2013) Estimating area from an accuracy assessment error matrix. Remote Sens Environ 132:202–211. https://doi.org/10.1016/j.rse.2013.01.016\nStehman SV (1997) Selecting and interpreting measures of thematic classification accuracy. Remote Sens Environ 62:77–89. https://doi.org/10.1016/S0034-4257(97)00083-7\nStehman SV, Foody GM (2019) Key issues in rigorous accuracy assessment of land cover products. Remote Sens Environ 231:111199. https://doi.org/10.1016/j.rse.2019.05.018\nStrahler AH, Boschetti L, Foody GM, et al (2006) Global land cover validation: Recommendations for evaluation and accuracy assessment of global land cover maps. Eur Communities, Luxemb 51:1–60"
  },
  {
    "objectID": "W08_ISM.html",
    "href": "W08_ISM.html",
    "title": "8  Informal Settlement Classification",
    "section": "",
    "text": "9 Classification Workflow"
  },
  {
    "objectID": "W08_ISM.html#problem-statement",
    "href": "W08_ISM.html#problem-statement",
    "title": "8  Informal Settlement Classification",
    "section": "8.1 Problem Statement",
    "text": "8.1 Problem Statement\nInformation on informal settlements is often missing from official statistics, and many times, these populations face greater vulnerability to urban food insecurity, disease, and other health risks. The goal of this project is to map informal settlements in Dar es Salaam, Tanzania, using satellite imagery and machine learning. The project will use a combination of supervised and unsupervised classification techniques to identify informal settlements in the city. The project will also use a variety of data sources, including open-access satellite imagery and building footprints.\nThe final application, which I originally developed as part of a collaboration between the Rhodes AI Lab and the World Food Programme, is displayed below:"
  },
  {
    "objectID": "W08_ISM.html#pre-processing",
    "href": "W08_ISM.html#pre-processing",
    "title": "8  Informal Settlement Classification",
    "section": "9.1 Pre processing",
    "text": "9.1 Pre processing\nThe first step in the classification process is to pre-process the satellite imagery. This involves the following steps:\n// // --------------------- Step 1: Importing and Pre-Processing  --------------------------------\n\nvar wards=ee.FeatureCollection(\"users/ollielballinger/Tanzania_Wards\")\n\n// // Sentinel-2 multispectral imagery collection and processing\n\nfunction maskS2clouds(image) {\n  var qa = image.select('QA60')\n\n  var cloudBitMask = 1 &lt;&lt; 10;\n  var cirrusBitMask = 1 &lt;&lt; 11;\n\n  var mask = qa.bitwiseAnd(cloudBitMask).eq(0).and(\n             qa.bitwiseAnd(cirrusBitMask).eq(0))\n\n  return image.updateMask(mask)\n      .select(\"B.*\")\n      .copyProperties(image, [\"system:time_start\"])\n}\n\nvar collection = ee.ImageCollection('COPERNICUS/S2')\n    .filterDate('2020-01-01', '2020-12-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(maskS2clouds)\n    .median()\n\n\nMap.addLayer(collection.toUint16(), {bands:['B4','B3','B2'], max:4000},'Sentinel-2')\n\n\nvar glcm=collection.toUint16().select('B8').glcmTexture({size: 4})\n\n\n// Using OpenStreetMap (OSM) building footprints\n\nvar OSM=ee.FeatureCollection(\"users/ollielballinger/Tanzania_OSM_clean\")\n\n\nvar empty = ee.Image().byte();\n//calculate reciprocal of each building footprint's area \nvar OSM=OSM.map(function(feature) {\n              return feature.set({area: ee.Number(1).divide(feature.geometry().area()).multiply(100)}).set({const: 1})})\n\n//convert to raster using reciprocal area as the band  \nvar empty = ee.Image().byte();\nvar OSM_color = empty.paint({\n  featureCollection: OSM,\n  color: 'area',\n}).unmask(0)  \n  \n  \n// define kernel\nvar gaussian = ee.Kernel.gaussian({\n  radius: 60, units: 'meters', normalize: true, sigma:15\n});\n\n// calculate density of small buildings\nvar density= OSM_color.reduceNeighborhood({\n  reducer: ee.Reducer.sum(),\n  kernel: gaussian\n})\n\n//i've saved the \"density\" layer as a raster, imported below. Using it speeds up the RF classification \nvar Tanzania_Density = ee.Image(\"users/ollielballinger/Tanzania_Density\")\n\nMap.addLayer(Tanzania_Density,{palette:[\"0034f5\",\"1e7d83\",\"4da910\",\"b3c120\",\"fcc228\",\"ff8410\",\"fd3000\"], min:0, max:1}, 'OSM Density', false);\n\n\n// We can now use the Sentinel-2 imagery to filter out non-urban areas\n// using the Normalized Difference Vegetation Index (NDVI) and Normalized Difference Built-up Index (NDBI)\n\n//Normalized Difference Vegetation Index     \nvar ndvi=(collection.select('B8')\n          .subtract(collection.select('B4')))\n          .divide(collection.select('B8')\n          .add(collection.select('B4')))\n          .select(['B8'],['NDVI'])\n\n\n// Normalized Difference Built-Up Index          \nvar ndbi=(collection.select('B11')\n          .subtract(collection.select('B8')))\n          .divide(collection.select('B11')\n          .add(collection.select('B8')))\n          .select(['B11'],['NDBI'])\n\n\n// add bands from all analysis layers \nvar image = collection.addBands(Tanzania_Density)//OSM density\n                      .addBands(ndvi)\n                      .addBands(ndbi)\n                      .addBands(glcm)\n                      .clip(wards)\n                      .updateMask(ndvi.lt(0.3).and(ndbi.gt(0)))//filter out non-urban landcover\n                      \n\n\n//input bands used for classification \nvar bands = ['B2', 'B3', 'B4', 'B8', 'B8A', 'B11', 'B12',\n            'NDVI','B8_contrast']"
  },
  {
    "objectID": "W08_ISM.html#step-2-classification",
    "href": "W08_ISM.html#step-2-classification",
    "title": "8  Informal Settlement Classification",
    "section": "9.2 Step 2: Classification",
    "text": "9.2 Step 2: Classification\nvar metal_roof_points=ee.FeatureCollection.randomPoints(metal_roof_poly, 500).map(function(i){\n  return i.set({'class': 0})})\n  \nvar apartment_points=ee.FeatureCollection.randomPoints(apartments_poly, 500).map(function(i){\n  return i.set({'class': 1})})\n  \nvar suburban_points=ee.FeatureCollection.randomPoints(suburban_poly, 500).map(function(i){\n  return i.set({'class': 2})})\n\nvar road_points=ee.FeatureCollection.randomPoints(road, 500).map(function(i){\n  return i.set({'class': 3})})\n  \n  \nvar sample=ee.FeatureCollection([metal_roof_points,\n                                  suburban_points,\n                                  apartment_points,\n                                  road_points])\n                                  .flatten()\n                                  .randomColumn();\n\n\n// assign 70% of training points to validation \nvar split=0.7\nvar training_sample = sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = sample.filter(ee.Filter.gte('random', split));\n\n\n\n\n// take samples from image for training and validation  \nvar training = image.select(bands).sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\nvar validation = image.select(bands).sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Random Forest Classification\n\nvar rf1 = ee.Classifier.smileRandomForest(100)\n    .train(training, 'class');\n   \nvar rf2 = image.classify(rf1);\n\nMap.addLayer(rf2,{palette:palette, min:0 , max:3},'Random Forest', false)"
  },
  {
    "objectID": "W08_ISM.html#step-3-validation",
    "href": "W08_ISM.html#step-3-validation",
    "title": "8  Informal Settlement Classification",
    "section": "9.3 Step 3: Validation",
    "text": "9.3 Step 3: Validation\n\n// Calculate the confusion matrix for the training data\nvar trainAccuracy = rf1.confusionMatrix();\nprint('Resubstitution error matrix: ', trainAccuracy);\nprint('Training overall accuracy: ', trainAccuracy.accuracy());\n\n// Calculate the confusion matrix for the validation data\nvar validated = validation.classify(rf1);\nvar testAccuracy = validated.errorMatrix('class', 'classification');\nvar consumers=testAccuracy.consumersAccuracy()\n\nprint('Validation error matrix: ', testAccuracy);\nprint('Validation overall accuracy: ', testAccuracy.accuracy())\nprint('Validation consumer accuracy: ', consumers);\n\n\n// Consumer's accuracy can be interpreted in a sentence as follows:\nprint(ee.Number(consumers.get([0,0])).multiply(100),'% of the areas identified as informal settlements \\nin the classification are actually informal settlements \\n(according to the verification data)')\n\nvar resub = rf2.sample({\n  region: validation_sample,\n  geometries:true,\n  scale:10\n}).map(function(feature){return feature.buffer(10)})\n\nvar resub_color = empty.paint({\n  featureCollection: resub,\n  color: 'classification',\n  width:4\n})\nvar palette =['#ff6200','#a126ff','#4a26ff','#00c3ff']\n\nMap.addLayer(resub_color,{min:0, max:3, palette:palette},'Validation Error', false)"
  },
  {
    "objectID": "W08_ISM.html#step-4-build-a-user-interface",
    "href": "W08_ISM.html#step-4-build-a-user-interface",
    "title": "8  Informal Settlement Classification",
    "section": "9.4 Step 4: Build a User Interface",
    "text": "9.4 Step 4: Build a User Interface\n//function to add the core layers \n\nfunction core_layers(){\nMap.addLayer(Classified.clip(wards),{min:1, max:9, palette: Vis, format:'png'},'RF Sentinel',false);\nMap.addLayer(Classified2.clip(wards),{min:1, max:9, palette: Vis, format:'png'},'RF Sentinel+OSM');\nMap.addLayer(subwards_outline,{color:'black'}, 'Sub-Wards',false);\nMap.addLayer(image, {bands: ['B4', 'B3', 'B2'], min: 0, max: 0.3}, 'Sentinel-2', false)\nMap.addLayer(Tanzania_Density,{palette:palette, min:0, max:1}, 'Reciprocal Area Density', false);\nMap.addLayer(wards_outline,{color:'red'}, 'Wards');\n}\n\ncore_layers()\n\n\n/// User interface\n\n// create the main panel \n\nvar console = ui.Panel({\n  layout: ui.Panel.Layout.flow('vertical'),\n  style: {\n    position: 'top-right',\n    padding: '8px 15px',\n    width: '350px',\n    height: '450px'\n    \n  }\n});\n\n \n// Create legend title\nvar title = ui.Label({\n  value: 'Informal Settlement Mapper',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0'\n    }\n});\n\n// add text labels\n\nvar intro1= ui.Label('This tool uses machine learning to map informal settlements in Dar es Salaam. Data layers can be toggled using the \"Layers\" tab. To compare the model\\'s prediction to high-resolution imagery, click \\\"Zoom to Informal Settlement\\\": ', {whiteSpace: 'wrap'})\n\nvar stats_label= ui.Label('To explore Ward-level data on informal settlements, click \\\"Ward Statistics\\\": ', {whiteSpace: 'wrap'})\n\nvar info_label= ui.Label('For more information about the model or any of its input layers, click \\\"How it Works\\\": ', {whiteSpace: 'wrap'})\n\nvar intro2= ui.Label('These maps were generated by a Random Forest algorithm built to identify informal settlements. The identification strategy exploits two characteristics of informal settlements: variation in roofing material, and the spatial distribution of structures. Further details about the data layers can be found below:', {whiteSpace: 'wrap'})\n\nvar contact= ui.Label('Please direct queries to ollie.ballinger@sant.ox.ac.uk', {whiteSpace: 'wrap'})\n\n\n// add \"how it works\" viz checkboxes\n\nvar rfs_label=ui.Checkbox({\n  label: 'RF Sentinel',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 6px 0',\n    padding: '0'\n    },\n  onChange: function(checked) {\n  Map.layers().get(0).setShown(checked)\n  Map.layers().get(1).setShown(false)\n  Map.layers().get(2).setShown(false)\n  Map.layers().get(3).setShown(false)\n  Map.layers().get(4).setShown(false)\n  }\n})\n\nvar rfs_desc=ui.Label(\"The \\\"RF Sentinel\\\" layer predicts the locations of informal settlements using freely available Sentinel-2 satellite imagery as its sole input. Accuracy: 81%\", {whiteSpace: 'wrap'})\n\nvar rfsosm_label=ui.Checkbox({\n  label: 'RF Sentinel+OSM',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0'\n    },\n  onChange: function(checked) {\n  Map.layers().get(1).setShown(checked)\n  Map.layers().get(0).setShown(false)\n  Map.layers().get(2).setShown(false)\n  Map.layers().get(3).setShown(false)\n  Map.layers().get(4).setShown(false)\n  }\n})\n\nvar rfsosm_desc=ui.Label(\"The \\\"RF Sentinel+OSM\\\" layer utilizes an additional input layer on housing density derived from Open Street Maps. Accuracy: 92%\", {whiteSpace: 'wrap'})\n\nvar s_label=ui.Checkbox({\n  label: 'Sentinel-2',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0'\n    },\n  onChange: function(checked) {\n  Map.layers().get(3).setShown(checked)\n  Map.layers().get(0).setShown(false)\n  Map.layers().get(1).setShown(false)\n  Map.layers().get(2).setShown(false)\n  Map.layers().get(4).setShown(false)\n  }\n})\n\nvar s_desc=ui.Label(\"Satellite images from Sentinel-2 are the primary input for supervised classification. Images have a resolution of 10 meters per pixel, and are split into 12 spectral bands. Despite its low resolution, multispectral imagery allows us to distinguish materials based on how they reflect light. Here, we exploit the fact that the roofing of informal settlements are primarily built using corrugated iron and plastic rather than concrete.\", {whiteSpace: 'wrap'})\n\nvar rad_label=ui.Checkbox({\n  label: 'Reciprocal Area Density',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0'\n    },\n  onChange: function(checked) {\n  Map.layers().get(4).setShown(checked)\n  Map.layers().get(0).setShown(false)\n  Map.layers().get(1).setShown(false)\n  Map.layers().get(2).setShown(false)\n  Map.layers().get(3).setShown(false)\n  }\n})\n\nvar rad_desc=ui.Label(\"Data extracted from Open Street Maps provide information on a second important characteristic of informal settlements. Unlike planned developments, which are mainly composed of larger structures that are spaced apart, informal settlements are composed of smaller structures that are closer together. This layer uses freely available data from Open Street Maps to detect clusters of small structures\", {whiteSpace: 'wrap'})\n\n\n// home panel config \nvar home= function(){\n  Map.setCenter(39.2854, -6.8167, 14)\n  Map.setOptions(\"Terrain\")\n  console.clear()\n  console.add(title);\n  console.add(intro1);\n  console.add(zoom);\n  console.add(stats_label)\n  console.add(ward_stats);\n  console.add(info_label)\n  console.add(details)\n  console.add(contact)\n}\n\n\n//home button config\nvar home_button = ui.Button({\n  style:{stretch: 'horizontal'},\n  label: 'Home',\n  onClick: function(){\n    home()\n    Map.layers().get(1).setShown(true)\n    Map.remove(Map.layers().get(6))\n    \n  }\n})\n\n\n// \"Zoom to informal settlement\" button config \n\nvar zoom = ui.Button({\n  style:{stretch: 'horizontal'},\n  label: 'Zoom to Informal Settlement',\n  onClick: \n  function() {\n    Map.setOptions(\"Satellite\")\n    console.clear()\n    console.add(title);\n    \n    Map.addLayer(drone, {bands: ['b1', 'b2', 'b3'], min: 0, max: 255}, 'Drone')\n    \n    Map.setCenter(39.27145, -6.771062, 17)\n    \n    console.add(ui.Label('This is a sample informal settlement identified in the Namanga area of Dar es Salaam. Use the slider below to compare drone imagery to the model prediction:', {whiteSpace: 'wrap'}))\n   \n    console.add(ui.Label({\n      value: 'Drone Layer Opacity:',\n      style: {\n      fontWeight: 'bold',\n      fontSize: '14px',\n      padding: '0'}}))\n   \n    var slider = ui.Slider({style:{stretch: 'horizontal'}});\n    slider.setValue(0.9);\n    slider.onChange(function(value) {\n      Map.layers().get(6).setOpacity(value);\n    });\n    \n    console.add(slider)\n    console.add(ui.Label('The informal settlement on the left and the suburban neighborhood on the right are both located in the Msasani Ward of Dar es Salaam. Because most statistics are aggregated at the ward level, data on population density, income, health outcomes, and other developmental indicators are likely to be highly biased by variation within city wards. By identifying the geographic extent of informal settlements, policy interventions and crisis responses can be targeted with a high degree of precision.', {whiteSpace: 'wrap'}))\n    console.add(home_button)\n\n  }\n});\n\n\n// \"How it Works\" button config\nvar details = ui.Button({\n  style:{stretch: 'horizontal'},\n  label: 'How it Works',\n  onClick: \n  function() {\n    console.clear()\n    console.add(title);\n      console.add(intro2);\n      console.add(rfs_label);\n      console.add(rfs_desc);\n      console.add(rfsosm_label);\n      console.add(rfsosm_desc)\n      console.add(s_label)\n      console.add(s_desc)\n      console.add(rad_label)\n      console.add(rad_desc)\n    console.add(home_button)\n\n  }\n});\n\n\n//\"Ward Statistics\" button config\nvar ward_stats = ui.Button({\n  style:{stretch: 'horizontal'},\n  label: 'Ward Statistics',\n  onClick: \n  function ward_stats_panel() {\n    \n    Map.setOptions(\"Satellite\")\n    Map.addLayer(buildings,{color:'red', max:1}, 'Informal Dwellings',false)\n    \n    console.clear()\n    console.add(title)\n    console.add(ui.Label('Click on the map to get a rough estimate of the number of informal dwellings in a given Ward.', {whiteSpace: 'wrap'}))\n    \n    // select a ward to return statistics on by clicking on the map \n      \n      Map.onClick(function(coords) {\n          \n          Map.layers().get(1).setShown(false)\n          ward_stats_panel()\n          \n          //select ward based on click \n          var point = ee.Geometry.Point(coords.lon, coords.lat);\n          \n          var saveAllJoin = ee.Join.saveAll({\n                matchesKey: 'Ward_Name',\n                outer:true\n              });\n\n          var ward_name = ee.Feature(ee.List(saveAllJoin.apply(point, wards, spatialFilter).first().get(\"Ward_Name\")).getInfo()[0]).get('Ward_Name')\n          ward_name.evaluate(function(val){ward_name.setValue(val)});\n          ward_stats(ward_name) \n      })\n        \n      // get ward level IS/normal building counts \n      \n      function ward_stats(ward_name){\n        \n        var ward=wards.filter(ee.Filter.eq('Ward_Name', ward_name))\n        var centroid=ward.geometry().centroid()\n        \n        // count number of OSM features within the bounds of the IS vector in the selected ward \n        var sum=buildings.filterBounds(ward).size()\n        \n        //count all OSM features in ward \n        var allBuildings=OSM.filterBounds(ward).size()\n        \n        //get % area of ward landcover that is IS\n        var mean = ee.Number(ee.Image(is).reduceRegions({collection: ward, reducer: ee.Reducer.mean()}).first().get('mean')).multiply(100)\n        \n\n        var sumLabel = ui.Label({\n            value: 'Calculating...'\n          })\n        var allLabel = ui.Label({\n            value: 'Calculating...'\n          })\n        var meanLabel = ui.Label({\n            value: 'Calculating...'\n          })\n        \n        \n        //grab server-side info      \n        \n        sum.evaluate(function(val){sumLabel.setValue(val)});\n        \n        allBuildings.evaluate(function(val){allLabel.setValue(val)});\n        \n        mean.evaluate(function(val){meanLabel.setValue(val)});\n\n        console.add(ui.Label({\n          value: ward_name.getInfo(),\n          style: {\n          fontWeight: 'bold',\n          fontSize: '18px',\n          padding: '0'}}))\n          \n        //add labels and values   \n        \n        console.add(ui.Label('Total number of buildings:', {whiteSpace: 'wrap'}))\n        console.add(allLabel, {whiteSpace: 'wrap'})\n        console.add(ui.Label('Estimated number of informal dwellings:', {whiteSpace: 'wrap'}))\n        console.add(sumLabel, {whiteSpace: 'wrap'})\n        console.add(ui.Label('Percent of ward under informal settlement:', {whiteSpace: 'wrap'}))\n        console.add(meanLabel, {whiteSpace: 'wrap'})\n        \n        //return IS footprint layer, clear old layer\n        Map.remove(Map.layers().get(6))\n        Map.addLayer(buildings.filterBounds(ward),{color:'red', max:1}, 'Informal Dwellings')\n        Map.remove(Map.layers().get(6))\n        \n        //zoom to ward center\n        Map.setCenter(ee.Number(centroid.coordinates().get(0)).getInfo(), \n                    ee.Number(centroid.coordinates().get(1)).getInfo(), 15);\n          }\n      console.add(home_button)     \n        }\n      });\n\n\n// set position of panel\nvar legend = ui.Panel({\n  style: {\n    position: 'bottom-left',\n    padding: '8px 15px'\n  }\n});\n \n// Create legend title\nvar legendTitle = ui.Label({\n  value: 'Legend',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0'\n    }\n});\n\n\n\n// Creates and styles 1 row of the legend.\nvar makeRow = function(color, name) {\n \n      // Create the label filled with the description text.\n      var description = ui.Label({\n        value: name,\n        style: {margin: '0 0 4px 6px'}\n      });\n \n      // return the panel\n      return ui.Panel({\n        widgets: [colorBox, description],\n        layout: ui.Panel.Layout.Flow('horizontal')\n      });\n};\n \n\n// set position of panel\nvar legend = ui.Panel({\n  style: {\n    position: 'bottom-left',\n    padding: '8px 15px'\n  }\n});\n \n// Create legend title\nvar legendTitle = ui.Label({\n  value: 'Legend',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0'\n    }\n});\n \n// Add the title to the panel\nlegend.add(legendTitle);\n \n// Creates and styles 1 row of the legend.\nvar makeRow = function(color, name, border) {\n \n      // Create the label that is actually the colored box.\n      var colorBox = ui.Label({\n        style: {\n          backgroundColor: '#' + color,\n          // Use padding to give the box height and width.\n          padding: '7px',\n          margin: '0 0 4px 0',\n          border: '3px solid #'+border\n        }\n      });\n \n      // Create the label filled with the description text.\n      var description = ui.Label({\n        value: name,\n        style: {margin: '0 0 4px 6px'}\n      });\n \n      // return the panel\n      return ui.Panel({\n        widgets: [colorBox, description],\n        layout: ui.Panel.Layout.Flow('horizontal')\n      });\n};\n \n// palette with the colors\n\nvar palette_vis= [\n  '49cc47', \n  '0008ff',\n  'fff700']\n\n// name of the legend\nvar names = ['vegetation','urban','informal settlement'];\n \nlegend.add(makeRow('49cc47', 'vegetation', '000000'));\nlegend.add(makeRow('2ff5ff', 'urban','0008ff'));\nlegend.add(makeRow('fff700', 'informal settlement','ff0000'));\n\n// add legend, main panel with home configuration\n\nMap.add(console)\nMap.add(legend);\n\nhome()"
  },
  {
    "objectID": "W07_refineries.html",
    "href": "W07_refineries.html",
    "title": "7  Refinery Identification",
    "section": "",
    "text": "8 Machine Learning Workflow\nNow that we’ve got a model that can identify oil from multispectral satellite imagery fairly well, we can set about making our results accessible.\nOne of the things we’re particularly interested in is the distribution of small refineries. The way we’re currently visualizing the prediction (the raster output from the model where predicted oil is shown in red and everything else is transparent) makes it hard to see these small refineries when we zoom out:\nWe can convert our raster into a series of points using the reduceToVectors function. In essence, this takes homogenous regions of an image (e.g., an area predicted to be oil surrounded by an area not predicted to be oil) and converts it into a point:\nNow the distribution of small refineries is much more easily visible as blue dots:\nIf we zoom out even further, we can see clusters of points that correspond to areas of high oil production. Using geolocated photographs, we can roughly ground-truth the model output:\nWe can also create a user interface that allows users to draw an Area of Interest (AOI) and get the total number of contaminated points as well as the total number of contaminated square meters within the AOI. Having exported the oil contamination points and areas from the model output, we can use the filterBounds function to filter the points to the AOI. We can then use the size function to get the number of points in the AOI, and the geometry function to get the area of the AOI.\nThe code for the application with the user interface can be found here.\nNow, we can create a function called getData that will get the AOI drawn by the user, filter the oil contamination points to the AOI, and get the number of points in the AOI. We can also filter the oil contamination areas to the AOI and get the area of the AOI. We can then create labels to display the number of points and the area of the AOI, and add them to the console.\nNext, we can create a button that will draw an AOI and call the getData function when clicked.\nFinally, we can create a label to display information about the map, and add the label and buttons to the console panel.\nThis study dates from 2020-2021. The situation in Syria has changed since then. First, try conducting inference using the model on newer satellite imagery. You can do this by changing the start and end variables in the first code block."
  },
  {
    "objectID": "W07_refineries.html#pre-processing",
    "href": "W07_refineries.html#pre-processing",
    "title": "7  Refinery Identification",
    "section": "8.1 Pre-Processing",
    "text": "8.1 Pre-Processing\nAs always, the first step in our project will be to load and pre-process satellite imagery. For this project, we’ll be using Sentinel-2 imagery. Let’s load imagery from 2020-2021, filter out cloudy images, and define visualization parameters:\nvar start='2020-04-01'\nvar end='2021-07-01'\n\nvar bands = ['B2', 'B3', 'B4','B5','B6','B7','B8', 'B8A','B11','B12']\n\nvar sentinel = ee.ImageCollection('COPERNICUS/S2_SR')\n                  .filter(ee.Filter.date(start, end))\n                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n                  .mean()\n                  .select(bands)\n\nvar s_rgb = {\n  min: 0.0,\n  max: 3000,\n  bands:['B4', 'B3', 'B2'],\n  opacity:1\n};\nWhen loading the Sentinel-2 imagery, I’ve also onlyh selected the bands that we will ultimately use in our analysis. There are a number of other bands included in the data that we don’t need. I’ve omitted a few bands (B1, B9, B10) because they’re collected at a much lower spatial resolution (60 meters) compared to the other bands.\nA couple types of landcover are so readily identifiable that we can remove them with thresholds. Water and vegetation both have spectral indices; we looked at NDVI above, but there’s a similar one for water called NDWI. These can be calculated from Sentinel-2 imagery as follows:\nvar ndvi=sentinel.normalizedDifference(['B8','B4'])\n                  .select(['nd'],['ndvi'])\n\nvar ndwi=sentinel.normalizedDifference(['B3','B8'])\n                  .select(['nd'],['ndwi'])\nWe use the normalizedDifference function and specify which bands we want to use for each index. NDVI uses the red and near infrared bands (B4 and B8), while NDWI uses bands 3 and 8. Finally, we want to rename the resulting band from ‘nd’ to the name of the spectral index.\nNow we can use these indices to filter out water and vegetation. We do this using the updateMask function, and specify that we want to remove areas that have an NDVI value higher than 0.2 and and NDWI value higher than 0.3. You can play around with these thesholds until you achieve the desired results.\n\nvar image=sentinel.updateMask(ndwi.lt(0.3))\n                  .updateMask(ndvi.lt(0.2))\n                  .addBands(ndvi)\n                  .select(bands)\nWe also want to only select bands that are relevant to our analysis; Sentinel\nFinally, let’s clip the image to our Area of Interest (AOI) and add it to the map using the visualization parameters we defined earlier.\nMap.addLayer(image.clip(AOI), s_rgb, 'Sentinel');\n\n\n\nwater and vegetation have been removed from this Sentinel-2 image. What remains is largely fallow agricultural land, urban areas, and oil spills.\n\n\nNow that we’ve loaded and preporcessed our satellite imagery, we can proceed with the rest of our task. Ultimately, we want to create a map of the study area which shows us different “landcovers” (materials). This can broadly be achieved in three steps:\n\nGenerate labeled landcover data\nTrain a model using labeled data\nValidate the model"
  },
  {
    "objectID": "W07_refineries.html#generating-labeled-data",
    "href": "W07_refineries.html#generating-labeled-data",
    "title": "7  Refinery Identification",
    "section": "8.2 1. Generating Labeled Data",
    "text": "8.2 1. Generating Labeled Data\nA vital step in any machine learning workflow is the generation of labeled data, which we will use to train a model to differentiated between different types of land cover and later to test the model’s accuracy. By looking around the study area, we can get a sense of the different land cover classes that we might encounter:\n\nAgricultural Land\nUrban Areas\nOil Contamination\n\nNaturally we could subdivide each of these into sub-categories, and there are probably other classes we haven’t included that may be present in the study area. The choice of classes is partly informed by the nature of the task at hand. In theory, the most efficient number of classes for this task would be two: oil, and everything else. The problem is that the “everything else” category would be pretty noisy since it would include a wide range of materials, making it harder to distinguish this from oil. In practice, a visual inspection of major landcover classes in the study area is a quick-and-dirty way of getting at roughly the right number of classes. This is also an iterative process: you can start with a set of labeled data, look at the model results, and adjust your sampling accordingly. More on this later.\nThe main landcover class we’re interested in is, of course, oil. Some oil contamination is readily visible from the high resolution satellite basemap; rivers of oil flow from the leaking Ger Zero refinery. We can draw polygons around the oil contamination like so:\n\nThe same process is applied to agricultural land and urban areas. In general, you want to make sure that you’re sampling from all across the study area. I’ve generated between 4-10 polygons per landcover class in different places. We’re now left with a featureCollection composed of polygons for each class. I’ve named them oil, agriculture, and urban.\nHowever, I don’t just want to use all of the pixels contained in these polygons for training. There are several reasons for this. First, it would likely lead to overfitting. Second, there are probably over a million pixels between all of the polygons, which would slow things down unnecessarily. Third, I haven’t drawn the polygons to be equal sizes across classes, so I could end up with way more points from one class compared to another. It’s OK to have some imbalance between classes, but you don’t want it to be extreme.\nAs such, the next step involves taking random samples of points from within these polygons. I do so using the randomPoints function:\nvar oil_points=ee.FeatureCollection.randomPoints(oil, 3000).map(function(i){\n  return i.set({'class': 0})})\n  \nvar urban_points=ee.FeatureCollection.randomPoints(urban, 1000).map(function(i){\n  return i.set({'class': 1})})\n  \nvar agriculture_points=ee.FeatureCollection.randomPoints(agriculture, 2000).map(function(i){\n  return i.set({'class': 2})})\nIn the first line, I create a new featureCollection called oil_points which contains 3000 points sampled from the polygons in the oil featureCollection. I then map through each of these points, and set a property called “class” equal to 0. I do the same for the urban and agricultural areas, setting the “class” property of these featureCollections to 1 and 2, respectively. Ultimately, our model will output a raster in which each pixel will contain one of these three values. A value of 0 in the output will represent the model predicting that that pixel is oil, based on the training data; a value of 1 would indicate predicted urban land cover, and 2 predicted agricultural landcover.\nNow we want to create one feature collection called “sample”, which will contain all three sets of points.\nvar sample=ee.FeatureCollection([oil_points,\n                                  urban_points,\n                                  agriculture_points\n                                  ])\n                                  .flatten()\n                                  .randomColumn();\nWe’ve also assigned a property called “random” using the randomColumn function. This lets us split our featureCollection into two: one used for training the model, and one used for validation. We’ll use a 70-30 split.\nvar split=0.7\nvar training_sample = sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = sample.filter(ee.Filter.gte('random', split));"
  },
  {
    "objectID": "W07_refineries.html#training-a-model",
    "href": "W07_refineries.html#training-a-model",
    "title": "7  Refinery Identification",
    "section": "8.3 2. Training a Model",
    "text": "8.3 2. Training a Model\nHaving generated labeled training and testing data, we now want to teach an algorithm to associate the pixels in those areas (in particular, their spectral profiles) with a specific landcover class.\nThe list of points we generated in the previous step contain a label (0: oil, 1: urban, 2: agriculture). However, they do not yet contain any information about the spectral profile of the Sentinel-2 image. The sampleRegions function lets us assign a the band values from an image as properties to our feature collection. We do this for both training sample and the validation sample.\nvar training = image.sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\nvar validation = image.sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\nEach point in the featureCollections above will contain a property denoting each Sentinel-2 band’s value at that location, as well as the property denoting the class label.\nNow we’re ready to train the model. We’ll be using a Random Forest classifier, which basically works by trying to separate your data into the specified classes by setting lots of thresholds in your input properties (in our case, Sentinel-2 band values). It’s a versatile and widely-used model.\nWe first call a random forest classifier with 500 trees. More trees usually yields higher accuracy, though there are diminishing returns. Too many trees will result in your computation timing out. We then train the model using the train function, which we supply with the training data as well as the name of the property that contains our class labels (“class”).\nvar model = ee.Classifier.smileRandomForest(500)\n                          .train(training, 'class');\nThe trained model now associates Sentinel-2 band values with one of three landcover classes. We can now feed the model pixels it has never seen before, and it will use what it now knows about the spectral profiles of the differnt classes to predict the class of the new pixel.\nvar prediction = image.classify(model)\nprediction is now a raster which contains one of three values (0: oil, 1: urban, 2: agriculture). We’re only interested in oil, so let’s isolate the regions in this raster that have a value of 0, and add them in red to the map:\nvar oil_prediction=prediction.updateMask(prediction.eq(0))\n\nMap.addLayer(oil_prediction, {palette:'red'}, 'Predicted Oil Conamination')"
  },
  {
    "objectID": "W07_refineries.html#validation",
    "href": "W07_refineries.html#validation",
    "title": "7  Refinery Identification",
    "section": "8.4 3. Validation",
    "text": "8.4 3. Validation\nThe image above should look somewhat familiar. It’s Ger Zero, where we trained part of our model. We can see in red the areas which the model predicts to be oil pollution. These largley align with the areas that we can see as being contaminated based on the high resolution basemap. It’s not perfect, but it’s pretty good.\nLet’s scroll to another area, far from where the model was trained.  This image shows two clusters of makeshift refineries which were identified by the model. This is good, though we can only get so far by visually inspecting the output from our model. To get a better sense of our model’s performance, we can use the validation data that we generated previously. Remember, these are labeled points which our model was not trained on, and has never seen before.\nWe’ll take the validation featureCollection containing our labeled points, and have our model classify it.\nvar validated = validation.classify(model);\nNow the validated variable is a featureCollection which contains both manual labels and predicted labels from our model. We can compare the manual labels to the predicted output to get a sense of how well our model is performing. This is called a Confusion Matrix (or an Error Matrix):\nvar testAccuracy = validated.errorMatrix('class', 'classification');\n\nprint('Confusion Matrix ', testAccuracy);\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabels\n\n\n\n\n\n\n\nOil\nUrban\nAgriculture\n\n\n\nOil\n876\n1\n5\n\n\nPrediction\nUrban\n0\n168\n8\n\n\n\nAgriculture\n1\n4\n514\n\n\n\nNow, we can see that of the 877 points that were labeled “oil”, only one was falsely predicted to be agicultural land. The model also falsely predicted as oil one point that was labeled urban, and five points that were labeled agriculture. Not bad. We can get a sense of the model’s overall accuracy using the accuracy function on the confusion matrix:\nprint('Validation overall accuracy: ', testAccuracy.accuracy())\nThis tells us that the overall accuracy of our model is around 98%. However, we shouldn’t take this estimate at face value. There are a number of complicated reasons (spatial autocorrelation in the training data, for example) why this figure is probably inflatred. If we were submitting this analysis to a peer-reviewed journal, we’d take great care in addressing this, but for our purposes we can use the accuracy statistics to guide our analysis and get a rough sense of how well the model is performing.\nThis model isn’t perfect; it often misclassifies the shorelines of lakes as oil, or certain parts of urban areas. As previously mentioned, training a model is often an iterative process. At this stage, if your accuracy is not as high as you’d like it to be, you can use the output to figure out how to tweak the model. For example, you may observe that your model is confusing urban areas with oil spills. You can draw a polygon over the erroneous area, label it urban landcover and retrain the model thereby hopefully improving accuracy. We could further refine our model in this way."
  },
  {
    "objectID": "W01_intro.html",
    "href": "W01_intro.html",
    "title": "1  Installation",
    "section": "",
    "text": "2 Creating a Spatial Database\nSupported by a wide variety of libraries and applications, PostGIS provides many options for loading data.\nWe will first load our working data from a database backup file, then review some standard ways of loading different GIS data formats using common tools."
  },
  {
    "objectID": "W01_intro.html#postgresql-for-microsoft-windows",
    "href": "W01_intro.html#postgresql-for-microsoft-windows",
    "title": "1  Installation",
    "section": "1.1 PostgreSQL for Microsoft Windows",
    "text": "1.1 PostgreSQL for Microsoft Windows\nFor a Windows install:\n\nGo to the Windows PostgreSQL download page.\nSelect the latest version of PostgreSQL and save the installer to disk.\nRun the installer and accept the defaults.\nFind and run the \"StackBuilder\" program that was installed with the database.\nSelect the \"Spatial Extensions\" section and choose latest \"PostGIS ..Bundle\" option.\n\nAccept the defaults and install."
  },
  {
    "objectID": "W01_intro.html#postgresql-for-apple-macos",
    "href": "W01_intro.html#postgresql-for-apple-macos",
    "title": "1  Installation",
    "section": "1.2 PostgreSQL for Apple MacOS",
    "text": "1.2 PostgreSQL for Apple MacOS\nFor a MacOS install:\n\nGo to the Postgres.app site, and download the latest release.\nOpen the disk image, and drag the Postgres icon into the Applications folder.\n\nIn the Applications folder, double-click the Postgres icon to start the server.\nClick the Initialize button to create a new blank database instance.\n{.inline, .border .inline, .border}\nIn the Applications folder, go to the Utilities folder and open Terminal.\nAdd the command-line utilities to your PATH for convenience.\n\n\nsudo mkdir -p /etc/paths.d\necho /Applications/Postgres.app/Contents/Versions/latest/bin | sudo tee /etc/paths.d/postgresapp"
  },
  {
    "objectID": "W01_intro.html#pgadmin-for-windows-and-macos",
    "href": "W01_intro.html#pgadmin-for-windows-and-macos",
    "title": "1  Installation",
    "section": "1.3 PgAdmin for Windows and MacOS",
    "text": "1.3 PgAdmin for Windows and MacOS\nPgAdmin is available for multiple platforms, at https://www.pgadmin.org/download/.\n\nDownload and install the latest version for your platform.\nStart PgAdmin!"
  },
  {
    "objectID": "W01_intro.html#pgadmin",
    "href": "W01_intro.html#pgadmin",
    "title": "1  Installation",
    "section": "2.1 PgAdmin",
    "text": "2.1 PgAdmin\nPostgreSQL has a number of administrative front-ends. The primary one is psql, a command-line tool for entering SQL queries. Another popular PostgreSQL front-end is the free and open source graphical tool pgAdmin. All queries done in pgAdmin can also be done on the command line with psql. pgAdmin also includes a geometry viewer you can use to spatial view PostGIS queries.\n\nFind pgAdmin and start it up.\n\nIf this is the first time you have run pgAdmin, you probably don't have any servers configured. Right click the Servers item in the Browser panel.\nWe'll name our server PostGIS. In the Connection tab, enter the Host name/address. If you're working with a local PostgreSQL install, you'll be able to use localhost. If you're using a cloud service, you should be able to retrieve the host name from your account.\nLeave Port set at 5432, and both Maintenance database and Username as postgres. The Password should be what you specified with a local install or with your cloud service."
  },
  {
    "objectID": "W01_intro.html#creating-a-database",
    "href": "W01_intro.html#creating-a-database",
    "title": "1  Installation",
    "section": "2.2 Creating a Database",
    "text": "2.2 Creating a Database\n\nOpen the Databases tree item and have a look at the available databases. The postgres database is the user database for the default postgres user and is not too interesting to us.\nRight-click on the Databases item and select New Database.\n\nFill in the Create Database form as shown below and click OK.\n\n\n\nName\nnyc\n\n\nOwner\npostgres\n\n\n\n\nSelect the new nyc database and open it up to display the tree of objects. You'll see the public schema.\n\nClick on the SQL query button indicated below (or go to Tools &gt; Query Tool).\n\nEnter the following query into the query text field to load the PostGIS spatial extension:\nCREATE EXTENSION postgis;\nClick the Play button in the toolbar (or press F5) to \"Execute the query.\"\nNow confirm that PostGIS is installed by running a PostGIS function:\nSELECT postgis_full_version();\n\nYou have successfully created a PostGIS spatial database!!"
  },
  {
    "objectID": "W01_intro.html#function-list",
    "href": "W01_intro.html#function-list",
    "title": "1  Installation",
    "section": "2.3 Function List",
    "text": "2.3 Function List\nPostGIS_Full_Version: Reports full PostGIS version and build configuration info."
  },
  {
    "objectID": "W01_intro.html#loading-the-backup-file",
    "href": "W01_intro.html#loading-the-backup-file",
    "title": "1  Installation",
    "section": "3.1 Loading the Backup File",
    "text": "3.1 Loading the Backup File\n\nIn the PgAdmin browser, right-click on the nyc database icon, and then select the Restore... option.\n{.inline, .border .inline, .border}\nBrowse to the location of your workshop data data directory (available in the workshop data bundle), and select the nyc_data.backup file.\n{.inline, .border .inline, .border}\nClick on the Restore options tab, scroll down to the Do not save section and toggle Owner to Yes.\n{.inline, .border .inline, .border}\nClick the Restore button. The database restore should run to completion without errors.\n{.inline, .border .inline, .border}\nAfter the load is complete, right click the nyc database, and select the Refresh option to update the client information about what tables exist in the database.\n{.inline, .border .inline, .border}\n\n\n\nNote\n\nIf you want to practice loading data from the native spatial formats, instead of using the PostgreSQL db backup files just covered, the next couple of sections will guide you thru loading using various command-line tools and QGIS DbManager. Note you can skip these sections, if you have already loaded the data with pgAdmin."
  },
  {
    "objectID": "W01_intro.html#shapefiles-whats-that",
    "href": "W01_intro.html#shapefiles-whats-that",
    "title": "1  Installation",
    "section": "3.2 Shapefiles? What's that?",
    "text": "3.2 Shapefiles? What's that?\nYou may be asking yourself -- \"What's this shapefile thing?\" A \"shapefile\" commonly refers to a collection of files with .shp, .shx, .dbf, and other extensions on a common prefix name (e.g., nyc_census_blocks). The actual shapefile relates specifically to files with the .shp extension. However, the .shp file alone is incomplete for distribution without the required supporting files.\nMandatory files:\n\n.shp—shape format; the feature geometry itself\n.shx—shape index format; a positional index of the feature geometry\n.dbf—attribute format; columnar attributes for each shape, in dBase III\n\nOptional files include:\n\n.prj—projection format; the coordinate system and projection information, a plain text file describing the projection using well-known text format\n\nThe shp2pgsql utility makes shape data usable in PostGIS by converting it from binary data into a series of SQL commands that are then run in the database to load the data."
  },
  {
    "objectID": "W01_intro.html#loading-with-shp2pgsql",
    "href": "W01_intro.html#loading-with-shp2pgsql",
    "title": "1  Installation",
    "section": "3.3 Loading with shp2pgsql",
    "text": "3.3 Loading with shp2pgsql\nThe shp2pgsql converts Shape files into SQL. It is a conversion utility that is part of the PostGIS code base and ships with PostGIS packages. If you installed PostgreSQL locally on your computer, you may find that shp2pgsql has been installed along with it, and it is available in the executable directory of your installation.\nUnlike ogr2ogr, shp2pgsql does not connect directly to the destination database, it just emits the SQL equivalent to the input shape file. It is up to the user to pass the SQL to the database, either with a \"pipe\" or by saving the SQL to file and then loading it.\nHere is an example invocation, loading the same data as before:\nexport PGPASSWORD=mydatabasepassword\n\nshp2pgsql \\\n  -D \\\n  -I \\\n  -s 26918 \\\n  nyc_census_blocks_2000.shp \\\n  nyc_census_blocks_2000 \\\n  | psql dbname=nyc user=postgres host=localhost\nHere is a line-by-line explanation of the command.\nshp2pgsql \\\nThe executable program! It reads the source data file, and emits SQL which can be directed to a file or piped to psql to load directly into the database.\n-D \\\nThe D flag tells the program to generate \"dump format\" which is much faster to load than the default \"insert format\".\n-I \\\nThe I flag tells the program to create a spatial index on the table after loading is complete.\n-s 26918 \\\nThe s flag tells the program what the \"spatial reference identifier (SRID)\" of the data is. The source data for this workshop is all in \"UTM 18\", for which the SRID is 26918 (see below).\nnyc_census_blocks_2000.shp \\\nThe source shape file to read.\nnyc_census_blocks_2000 \\\nThe table name to use when creating the destination table.\n| psql dbname=nyc user=postgres host=localhost\nThe utility program is generating a stream of SQL. The \"|\" operator takes that stream and uses it as input to the psql database terminal program. The arguments to psql are just the connection string for the destination database."
  },
  {
    "objectID": "W01_intro.html#srid-26918-whats-with-that",
    "href": "W01_intro.html#srid-26918-whats-with-that",
    "title": "1  Installation",
    "section": "3.4 SRID 26918? What's with that?",
    "text": "3.4 SRID 26918? What's with that?\nMost of the import process is self-explanatory, but even experienced GIS professionals can trip over an SRID.\nAn \"SRID\" stands for \"Spatial Reference IDentifier.\" It defines all the parameters of our data's geographic coordinate system and projection. An SRID is convenient because it packs all the information about a map projection (which can be quite complex) into a single number.\nYou can see the definition of our workshop map projection by looking it up either in an online database,\n\nhttps://epsg.io/26918\n\nor directly inside PostGIS with a query to the spatial_ref_sys table.\nSELECT srtext FROM spatial_ref_sys WHERE srid = 26918;\n\n\nNote\n\nThe PostGIS spatial_ref_sys table is an OGC-standard table that defines all the spatial reference systems known to the database. The data shipped with PostGIS, lists over 3000 known spatial reference systems and details needed to transform/re-project between them.\n\nIn both cases, you see a textual representation of the 26918 spatial reference system (pretty-printed here for clarity):\nPROJCS[\"NAD83 / UTM zone 18N\",\n  GEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n      SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],\n      AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4269\"]],\n  UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],\n  PROJECTION[\"Transverse_Mercator\"],\n  PARAMETER[\"latitude_of_origin\",0],\n  PARAMETER[\"central_meridian\",-75],\n  PARAMETER[\"scale_factor\",0.9996],\n  PARAMETER[\"false_easting\",500000],\n  PARAMETER[\"false_northing\",0],\n  AUTHORITY[\"EPSG\",\"26918\"],\n  AXIS[\"Easting\",EAST],\n  AXIS[\"Northing\",NORTH]]\nIf you open up the nyc_neighborhoods.prj file from the data directory, you'll see the same projection definition.\nData you receive from local agencies—such as New York City—will usually be in a local projection noted by \"state plane\" or \"UTM\". Our projection is \"Universal Transverse Mercator (UTM) Zone 18 North\" or EPSG:26918."
  },
  {
    "objectID": "W01_intro.html#things-to-try-view-data-using-qgis",
    "href": "W01_intro.html#things-to-try-view-data-using-qgis",
    "title": "1  Installation",
    "section": "3.5 Things to Try: View data using QGIS",
    "text": "3.5 Things to Try: View data using QGIS\nQGIS, is a desktop GIS viewer/editor for quickly looking at data. You can view a number of data formats including flat shapefiles and a PostGIS database. Its graphical interface allows for easy exploration of your data, as well as simple testing and fast styling.\nTry using this software to connect your PostGIS database. The application can be downloaded from https://qgis.org\nYou'll first want to create a connection to a PostGIS database using menu Layer-&gt;Add Layer-&gt;PostGIS Layers-&gt;New and then filling in the prompts. Once you have a connection, you can add Layers by clicking connect and selecting a table to display."
  },
  {
    "objectID": "W01_intro.html#loading-data-using-qgis-dbmanager",
    "href": "W01_intro.html#loading-data-using-qgis-dbmanager",
    "title": "1  Installation",
    "section": "3.6 Loading data using QGIS DbManager",
    "text": "3.6 Loading data using QGIS DbManager\nQGIS comes with a tool called DbManager that allows you to connect to various different kinds of databases, including a PostGIS enabled one. After you have a PostGIS Database connection configured, go to Database-&gt;DbManager and expand to your database as shown below:\n{.inline, .border .inline, .border}\nFrom there you can use the Import Layer/File menu option to load numerous different spatial formats. In addition to being able to load data from many spatial formats and export data to many formats, you can also add ad-hoc queries to the canvas or define views in your database, using the highlighted wrench icon.\nThis section is based on the PostGIS Intro Workshop, sections 3, 4, 5,and 7"
  },
  {
    "objectID": "W02_SQL.html",
    "href": "W02_SQL.html",
    "title": "2  Introduction to SQL",
    "section": "",
    "text": "3 Getting Started\nThis week we’ll be learning how to use SQL to query data from a database. To get started, work your way through the following two notebooks:\nClicking the link will take you to Google Colab, where you can run the notebook in your browser. You can also download the notebook and run it locally if you prefer.\nOnce you’ve completed these you can test your knowledge by answering the questions in the lab below. You can also access it on Google Colab here.\nDatasets:\nThe following datasets are used in this lab. You don’t need to download them manually, they can be accessed directly from the notebook."
  },
  {
    "objectID": "W02_SQL.html#spatial-databases",
    "href": "W02_SQL.html#spatial-databases",
    "title": "2  Introduction to SQL",
    "section": "2.1 Spatial databases",
    "text": "2.1 Spatial databases\nA database management system (DBMS) allows users to store, insert, delete, and update information in a database. Spatial databases go a step further because they record data with geographic coordinates.\nFrom Esri Geodatabase to PostGIS, spatial databases have quickly become the primary method of managing spatial data.\nTo learn more about spatial databases, check out the resources below:\n\nWikipedia: Spatial database\n7 Spatial Databases for Your Enterprise\nGISGeography: Spatial Databases – Build Your Spatial Data Empire\nEsri: What is a geodatabase?\nIntroduction to PostGIS\nPostGEESE? Introducing The DuckDB Spatial Extension"
  },
  {
    "objectID": "W02_SQL.html#duckdb",
    "href": "W02_SQL.html#duckdb",
    "title": "2  Introduction to SQL",
    "section": "2.2 DuckDB",
    "text": "2.2 DuckDB\nDuckDB is an in-process SQL OLAP database management system. It is designed to be used as an embedded database in applications, but it can also be used as a standalone SQL database.\n\nIn-process SQL means that DuckDB’s features run in your application, not an external process to which your application connects. In other words: there is no client sending instructions nor a server to read and process them. SQLite works the same way, while PostgreSQL, MySQL…, do not.\nOLAP stands for OnLine Analytical Processing, and Microsoft defines it as a technology that organizes large business databases and supports complex analysis. It can be used to perform complex analytical queries without negatively affecting transactional systems.\n\nDuckDB is a great option if you’re looking for a serverless data analytics database management system."
  },
  {
    "objectID": "W02_SQL.html#question-1-creating-tables",
    "href": "W02_SQL.html#question-1-creating-tables",
    "title": "2  Introduction to SQL",
    "section": "4.1 Question 1: Creating Tables",
    "text": "4.1 Question 1: Creating Tables\nCreate a database, then write a SQL query to create a table named nyc_subway_stations and load the data from the file nyc_subway_stations.tsv into it. Similarly, create a table named nyc_neighborhoods and load the data from the file nyc_neighborhoods.tsv into it."
  },
  {
    "objectID": "W02_SQL.html#question-2-column-filtering",
    "href": "W02_SQL.html#question-2-column-filtering",
    "title": "2  Introduction to SQL",
    "section": "4.2 Question 2: Column Filtering",
    "text": "4.2 Question 2: Column Filtering\nWrite a SQL query to display the ID, NAME, and BOROUGH of each subway station in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W02_SQL.html#question-3-row-filtering",
    "href": "W02_SQL.html#question-3-row-filtering",
    "title": "2  Introduction to SQL",
    "section": "4.3 Question 3: Row Filtering",
    "text": "4.3 Question 3: Row Filtering\nWrite a SQL query to find all subway stations in the nyc_subway_stations dataset that are located in the borough of Manhattan."
  },
  {
    "objectID": "W02_SQL.html#question-4-sorting-results",
    "href": "W02_SQL.html#question-4-sorting-results",
    "title": "2  Introduction to SQL",
    "section": "4.4 Question 4: Sorting Results",
    "text": "4.4 Question 4: Sorting Results\nWrite a SQL query to list the subway stations in the nyc_subway_stations dataset in alphabetical order by their names."
  },
  {
    "objectID": "W02_SQL.html#question-5-unique-values",
    "href": "W02_SQL.html#question-5-unique-values",
    "title": "2  Introduction to SQL",
    "section": "4.5 Question 5: Unique Values",
    "text": "4.5 Question 5: Unique Values\nWrite a SQL query to find the distinct boroughs represented in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W02_SQL.html#question-6-counting-rows",
    "href": "W02_SQL.html#question-6-counting-rows",
    "title": "2  Introduction to SQL",
    "section": "4.6 Question 6: Counting Rows",
    "text": "4.6 Question 6: Counting Rows\nWrite a SQL query to count the number of subway stations in each borough in the nyc_subway_stations dataset."
  },
  {
    "objectID": "W02_SQL.html#question-7-aggregating-data",
    "href": "W02_SQL.html#question-7-aggregating-data",
    "title": "2  Introduction to SQL",
    "section": "4.7 Question 7: Aggregating Data",
    "text": "4.7 Question 7: Aggregating Data\nWrite a SQL query to list the number of subway stations in each borough, sorted in descending order by the count."
  },
  {
    "objectID": "W02_SQL.html#question-8-joining-tables",
    "href": "W02_SQL.html#question-8-joining-tables",
    "title": "2  Introduction to SQL",
    "section": "4.8 Question 8: Joining Tables",
    "text": "4.8 Question 8: Joining Tables\nWrite a SQL query to join the nyc_subway_stations and nyc_neighborhoods datasets on the borough name, displaying the subway station name and the neighborhood name."
  },
  {
    "objectID": "W02_SQL.html#question-9-string-manipulation",
    "href": "W02_SQL.html#question-9-string-manipulation",
    "title": "2  Introduction to SQL",
    "section": "4.9 Question 9: String Manipulation",
    "text": "4.9 Question 9: String Manipulation\nWrite a SQL query to display the names of subway stations in the nyc_subway_stations dataset that contain the word “St” in their names."
  },
  {
    "objectID": "W02_SQL.html#question-10-filtering-with-multiple-conditions",
    "href": "W02_SQL.html#question-10-filtering-with-multiple-conditions",
    "title": "2  Introduction to SQL",
    "section": "4.10 Question 10: Filtering with Multiple Conditions",
    "text": "4.10 Question 10: Filtering with Multiple Conditions\nWrite a SQL query to find all subway stations in the nyc_subway_stations dataset that are in the borough of Brooklyn and have routes that include the letter “R”."
  },
  {
    "objectID": "W03_postgis1.html",
    "href": "W03_postgis1.html",
    "title": "3  Spatial SQL I",
    "section": "",
    "text": "4 Geometry Exercises\nHere's a reminder of all the functions we have seen so far. They should be useful for the exercises!\nAlso remember the tables we have available:\nHere's a reminder of the functions we saw in the last section. They should be useful for the exercises!\nAlso remember the tables we have available:"
  },
  {
    "objectID": "W03_postgis1.html#introduction",
    "href": "W03_postgis1.html#introduction",
    "title": "3  Spatial SQL I",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this lesson, you will learn how to use SQL to query spatial data. You will learn how to use the duckdb Python library to connect to a DuckDB database and run SQL queries. You will also learn how to use the leafmap Python library to visualize spatial data."
  },
  {
    "objectID": "W03_postgis1.html#learning-objectives",
    "href": "W03_postgis1.html#learning-objectives",
    "title": "3  Spatial SQL I",
    "section": "3.2 Learning Objectives",
    "text": "3.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nConnect to a DuckDB database using Python\nRun SQL queries to query spatial data\nVisualize spatial data using Leafmap\nRun spatial SQL queries using PgAdmin"
  },
  {
    "objectID": "W03_postgis1.html#materials",
    "href": "W03_postgis1.html#materials",
    "title": "3  Spatial SQL I",
    "section": "3.3 Materials",
    "text": "3.3 Materials\nTo get started, work your way through the following two notebooks:\n\nGeometries\nSpatial Relationships\n\nClicking the link will take you to Google Colab, where you can run the notebook in your browser. These workbooks use the duckdb Python library to connect to a DuckDB database and run SQL queries. They also use the leafmap Python library to visualize spatial data. Most of the spatial functions and syntax are the same or very similar to their PostGIS equivalents.\nOnce you’ve completed these you can test your knowledge by answering the questions in the lab below. To complete this lab, open PgAdmin and connect to the nyc database. Then, open a new query window and write your SQL queries there."
  },
  {
    "objectID": "W03_postgis1.html#exercises",
    "href": "W03_postgis1.html#exercises",
    "title": "3  Spatial SQL I",
    "section": "4.1 Exercises",
    "text": "4.1 Exercises\n\nWhat is the area of the 'West Village' neighborhood? (Hint: The area is given in square meters. To get an area in hectares, divide by 10000. To get an area in acres, divide by 4047.)\nWhat is the geometry type of ‘Pelham St’? The length?\nWhat is the GeoJSON representation of the 'Broad St' subway station?\nWhat is the total length of streets (in kilometers) in New York City? (Hint: The units of measurement of the spatial data are meters, there are 1000 meters in a kilometer.)\nWhat is the area of Manhattan in acres? (Hint: both nyc_census_blocks and nyc_neighborhoods have a boroname in them.)\nWhat is the most westerly subway station?\nHow long is 'Columbus Cir' (aka Columbus Circle)?\nWhat is the length of streets in New York City, summarized by type?\n\nAnswers (only check after you’ve given it your best shot!)"
  },
  {
    "objectID": "W03_postgis1.html#exercises-1",
    "href": "W03_postgis1.html#exercises-1",
    "title": "3  Spatial SQL I",
    "section": "5.1 Exercises",
    "text": "5.1 Exercises\n\nWhat is the geometry value for the street named 'Atlantic Commons'?\nWhat neighborhood and borough is Atlantic Commons in?\nWhat streets does Atlantic Commons join with?\nApproximately how many people live on (within 50 meters of) Atlantic Commons?\n\nAnswers (only check after you’ve given it your best shot!)"
  },
  {
    "objectID": "W04_postgis2.html",
    "href": "W04_postgis2.html",
    "title": "4  Spatial SQL II",
    "section": "",
    "text": "5 Spatial Joins Exercises\nHere's a reminder of some of the functions we have seen. Hint: they should be useful for the exercises!\nAlso remember the tables we have available:\nNow for a less structured exercise. We’re going to look at ship-to-ship transfers. The idea is that two ships meet up in the middle of the ocean, and one ship transfers cargo to the other. This is a common way to avoid sanctions, and is often used to transfer oil from sanctioned countries to other countries. We’re going to look at a few different ways to detect these transfers using AIS data."
  },
  {
    "objectID": "W04_postgis2.html#step-1",
    "href": "W04_postgis2.html#step-1",
    "title": "4  Spatial SQL II",
    "section": "7.1 Step 1",
    "text": "7.1 Step 1\nCreate a spatial database using the following AIS data:\nhttps://storage.googleapis.com/qm2/casa0025_ships.csv\nEach row in this dataset is an AIS ‘ping’ indicating the position of a ship at a particular date/time, alongside vessel-level characteristics.\nIt contains the following columns:\n\nvesselid: A unique numerical identifier for each ship, like a license plate\nvessel_name: The ship’s name\nvsl_descr: The ship’s type\ndwt: The ship’s Deadweight Tonnage (how many tons it can carry)\nv_length: The ship’s length in meters\ndraught: How many meters deep the ship is draughting (how low it sits in the water). Effectively indicates how much cargo the ship is carrying\nsog: Speed over Ground (in knots)\ndate: A timestamp for the AIS signal\nlat: The latitude of the AIS signal (EPSG:4326)\nlon: The longitude of the AIS signal (EPSG:4326)\n\nCreate a table called ‘ais’ where each row is a different AIS ping, with no superfluous information. Construct a geometry column.\nCreate a second table called ‘vinfo’ which contains vessel-level information with no superfluous information."
  },
  {
    "objectID": "W04_postgis2.html#step-2",
    "href": "W04_postgis2.html#step-2",
    "title": "4  Spatial SQL II",
    "section": "7.2 Step 2",
    "text": "7.2 Step 2\nUse a spatial join to identify ship-to-ship transfers in this dataset. Two ships are considered to be conducting a ship to ship transfer IF:\n\nThey are within 500 meters of each other\nFor more than two hours\nAnd their speed is lower than 1 knot\n\nSome things to consider: make sure you’re not joining ships with themselves. Try working with subsets of the data first while you try different things out."
  },
  {
    "objectID": "W05_quiz.html",
    "href": "W05_quiz.html",
    "title": "5  Quiz",
    "section": "",
    "text": "Placeholder for the quiz. This will be administered on Moodle, under the “Assessments” tab."
  },
  {
    "objectID": "W06_RS.html",
    "href": "W06_RS.html",
    "title": "6  Remote Sensing",
    "section": "",
    "text": "7 Data Acquisition\nOne of the main advantages of GEE is that it hosts several Petabytes of satellite imagery and other spatial data sets, all in one place. Among these are a many that could prove useful to those investigating illegal mining and logging, estimating conflict-induced damage, monitoring pollution from extractive industries, conducting maritime surveillance without relying on ship transponders, and many other topics.\nThis section highlights ten categories of geospatial data available natively in the GEE catalogue ranging from optical satellite imagery, to atmospheric data, to building footprints. Each sub-section provides an overview of the given data type, suggests potential applications, and lists the corresponding datasets in the GEE catalogue. The datasets listed under each heading are not an exhaustive list– there are over 500 in the whole catalogue, and the ones listed in this section are simply the ones with the most immediate relevance to open source investigations. If a particular geospatial dataset you want to work with isn’t hosted in the GEE catalog, you can upload your own data. We’ll cover that in the next section."
  },
  {
    "objectID": "W06_RS.html#active-and-passive-sensors",
    "href": "W06_RS.html#active-and-passive-sensors",
    "title": "6  Remote Sensing",
    "section": "6.1 Active and Passive Sensors",
    "text": "6.1 Active and Passive Sensors\nRemote sensing is the science of obtaining information about an object or phenomenon without making physical contact with the object. Remote sensing can be done with various types of electromagnetic radiation such as visible, infrared, or microwave. The electromagnetic radiation is either emitted or reflected from the object being sensed. The reflected radiation is then collected by a sensor and processed to obtain information about the object.\n\nWhile most satellite imagery is optical, meaning it captures sunlight reflected by the earth’s surface, Synthetic Aperture Radar (SAR) satellites such as Sentinel-1 work by emitting pulses of radio waves and measuring how much of the signal is reflected back. This is similar to the way a bat uses sonar to “see” in the dark: by emitting calls and listening to echoes."
  },
  {
    "objectID": "W06_RS.html#resolution",
    "href": "W06_RS.html#resolution",
    "title": "6  Remote Sensing",
    "section": "6.2 Resolution",
    "text": "6.2 Resolution\nResolution is one of the most important attributes of satellite imagery. There are three types of resolution: spatial, spectral, and temporal. Let’s look at each of these.\n\n6.2.1 Spatial Resolution\nSpatial resolution governs how “sharp” an image looks. The Google Maps satellite basemap, for example, is really sharp Most of the optical imagery that is freely available has relatively low spatial resolution (it looks more grainy than, for example, the Google satellite basemap),\n  \n\n\n6.2.2 Spectral Resolution\nWhat open access imagery lacks in spatial resolution it often makes up for with spectral resolution. Really sharp imagery from MAXAR, for example, mostly collects light in the visible light spectrum, which is what our eyes can see. But there are other parts of the electromagnetic spectrum that we can’t see, but which can be very useful for distinguishing between different materials. Many satellites that have a lower spatial resolution than MAXAR, such as Landsat and Sentinel-2, collect data in a wider range of the electromagnetic spectrum.\nDifferent materials reflect light differently. An apple absorbs shorter wavelengths (e.g. blue and green), and reflects longer wavelengths (red). Our eyes use that information– the color– to distinguish between different objects. Below is a plot of the spectral profiles of different materials:\n\n\n\nThe visible portion of the spectrum is highlighted on the left, ranging from 400nm (violet) to 700nm (red). Our eyes (and satellite imagery in the visible light spectrum) can only see this portion of the light spectrum; we can’t see UV or infrared wavelengths, for example, though the extent to which different materials reflect or absorb these wavelengths is just as useful for distinguishing between them. The European Space Agency’s Sentinel-2 satellite collects spectral information well beyond the visible light spectrum, enabling this sort of analysis. It chops the electromagnetic spectrum up into “bands”, and measures how strongly wavelengths in each of those bands is reflected:\n\nTo illustrate why this is important, consider Astroturf (fake plastic grass). Astroturf and real grass will both look green to us, espeically from a satellite image. But living plants strongly reflect radiation from the sun in a part of the light spectrum that we can’t see (near-infrared). There’s a spectral index called the Normalized Difference Vegetation Index (NDVI) which exploits this fact to isolate vegetation in multispectral satellite imagery. So if we look at Gilette Stadium near Boston, we can tell that the three training fields south of the stadium are real grass (they generate high NDVI values, showing up red), while the pitch in the stadium itself is astroturf (generating low NDVI values, showing up blue).\n\n\n\nVHR image of Gilette Stadium with Sentinel-2 derived NDVI overlay\n\n\nIn other words, even though these fields are all green and indistinguishable to the human eye, their spectral profiles beyond the visible light spectrum differ, and we can use this information to distinguish between them.\nAstroturf is a trivial example. But suppose we were interested in identifying makeshift oil refineries in Northern Syria that constitute a key source of rents for whichever group controls them. As demonstrated in the ‘Refinery Identification’ case study, we can train an algorithm to identify the spectral signatures of oil, and use that to automatically detect them in satellite imagery.\n\n\n6.2.3 Temporal Resolution\nFinally, the frequency with which we can access new imagery is an important consideration. This is called the temporal resolution.\nThe Google Maps basemap is very high resolution, available globally, and is freely available. But it has no temporal dimension: it’s a snapshot from one particular point in time. If the thing we’re interested in involves changes over time, this basemap will be of limited use.\nThe “revisit rate” is the amount of time it takes for the satellite to pass over the same location twice. For example, the Sentinel-2 constellation’s two satellites can achieve a revisit rate of 5 days, as shown in this cool video from the European Space Agency:\n\nSome satellite constellations are able to achieve much higher revisit rates. Sentinel-2 has a revisit rate of 5 days, but SkySat capable of imaging the same point on earth around 12 times per day! How is that possible? Well, as the video above demonstrated, the Sentinel-2 constellation is composed of two satellites that share the same orbit, 180 degrees apart. In contrast, the SkySat constellation comprises 21 satellites, each with its own orbital path:\n\nThis allows SkySat to achieve a revisit rate of 2-3 hours. The catch, however, is that you need to pay for it (and it ain’t cheap). Below is a comparison of revisit rates for various other optical satellites:\n\n\n\nA chart of revisit times for different satellites from Sutlieff et. al.(2021)"
  },
  {
    "objectID": "W06_RS.html#summary",
    "href": "W06_RS.html#summary",
    "title": "6  Remote Sensing",
    "section": "6.3 Summary",
    "text": "6.3 Summary\nYou should hopefully have a better understanding of what satellite imagery is, and how it can be used to answer questions about the world. In the next section, we’ll look at the various types of satellite imagery stored in the Google Earth Engine catalogue."
  },
  {
    "objectID": "W06_RS.html#optical-imagery",
    "href": "W06_RS.html#optical-imagery",
    "title": "6  Remote Sensing",
    "section": "7.1 Optical Imagery",
    "text": "7.1 Optical Imagery\n\n\n\nAutomatic detection of vehicles using artificial intelligence in high resolution optical imagery. See the object detection tutorial.\n\n\nOptical satellite imagery is the bread and butter of many open source investiagtions. It would be tough to list off all of the possible use cases, so here’s a handy flowchart:\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#FFFFFF' ,'primaryBorderColor':'#000000' , 'lineColor':'#009933'}}}%%\n\nflowchart\n  A(Does it happen outside?) \n  A--&gt; B(Yes)\n  A--&gt; C(No)\n  D(Is it very small?)\n  B--&gt;D\n  E(Yes)\n  F(No)\n    D--&gt;F\n    D--&gt;E\nG(Use optical satellite imagery)\nH(Don't use optical satellite imagery)\nE--&gt;H\nF--&gt;G\nC--&gt;H\n\n\n\n\n\nThis is, of course, a bit of an exaggeration. But if you’re interested in a visible phenomenon that happens outdoors and that isn’t very tiny, chances are an earth-observing satellite has taken a picture of it. What that picture can tell you naturally depends on what you’re interested in learning. For a deeper dive into analyzing optical satellite imagery, see the subsection on multispectral remote sensing..\nThere are several different types of optical satellite imagery available in the GEE catalogue. The main collections are the Landsat and Sentinel series of satellites, which are operated by NASA and the European Space Agency, respectively. Landsat satellites have been in orbit since 1972, and Sentinel satellites have been in orbit since 2015. Norway’s International Climate and Forest Initiative (NICFI) has also contributed to the GEE catalogue by providing a collection of optical imagery from Planet’s PlanetScope satellites. These are higher resolution (4.7 meters per pixel) than Landsat (30m/px) and Sentinel-2 (10m/px), but are only available for the tropics. Even higher resolution imagery (60cm/px) is available from the GEE catalogue from the National Agriculture Imagery Program, but it is only available for the United States. For more details, see the “Datasets” section below.\n\nApplications\n\nGeolocating pictures\n\nSome of Bellingcat’s earliest work involved figuring out where a picture was taken by cross-referencing it with optical satellite imagery.\n\nGeneral surveillance\n\nMonitoring Chinese missile silo construction.\nAmassing evidence of genocide in Bucha, Ukraine\n\nDamage detection\n\nUkraine\nMali\nAround the World\n\nVerifying the locations of artillery/missile/drone strikes\n\nThe 2019 attack on Saudi Arabia’s Abqaiq oil processing facility.\n\nMonitoring illegal mining/logging\n\nGlobal Witness investigation into illegal mining by militias in Myanmar.\nTracking illegal logging across the world.\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nLandsat 1-5\n1972–1999\n30m\nGlobal\n\n\nLandsat 7\n1999–2021\n30m\nGlobal\n\n\nLandsat 8\n2013–Present\n30m\nGlobal\n\n\nLandsat 9\n2021–Present\n30m\nGlobal\n\n\nSentinel-2\n2015–Present\n10m\nGlobal\n\n\nNICFI\n2015-Present\n4.7m\nTropics\n\n\nNAIP\n2002-2021\n0.6m\nUSA"
  },
  {
    "objectID": "W06_RS.html#radar-imagery",
    "href": "W06_RS.html#radar-imagery",
    "title": "6  Remote Sensing",
    "section": "7.2 Radar Imagery",
    "text": "7.2 Radar Imagery\n\n\n\nShips and interference from a radar system are visible in Zhuanghe Wan, near North Korea.\n\n\nSynthetic Aperture Radar imagery (SAR) is a type of remote sensing that uses radio waves to detect objects on the ground. SAR imagery is useful for detecting objects that are small, or that are obscured by clouds or other weather phenomena. SAR imagery is also useful for detecting objects that are moving, such as ships or cars.\n\nApplications\n\nChange/Damage detection\nTracking military radar systems\nMaritime surveillance\nMonitoring illegal mining/logging\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nSentinel 1\n2014-Present\n10m\nGlobal"
  },
  {
    "objectID": "W06_RS.html#nighttime-lights",
    "href": "W06_RS.html#nighttime-lights",
    "title": "6  Remote Sensing",
    "section": "7.3 Nighttime Lights",
    "text": "7.3 Nighttime Lights\n\n\n\nA timelapse of nighttime lights over Northern Iraq showing the capture and liberation of Mosul by ISIS.\n\n\nSatellite images of the Earth at night a useful proxy for human activity. The brightness of a given area at night is a function of the number of people living there and the nature of their activities. The effects of conflict, natural disasters, and economic development can all be inferred from changes in nighttime lights.\nThe timelapse above reveals a number of interesting things: The capture of Mosul by ISIS in 2014 and the destruction of its infrastructure during the fighting (shown as the city darkening), as well as the liberation of the city by the Iraqi military in 2017 are all visible in nighttime lights. The code to create this gif, as well as a more in-depth tutorial on the uses of nighttime lights, can be found in the “War at Night” case study.\n\nApplications\n\nDamage detection\nIdentifying gas flaring/oil production\nIdentifying urban areas/military bases illuminated at night\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nDMSP-OLS\n1992-2014\n927m\nGlobal\n\n\nVIIRS\n2014-Present\n463m\nGlobal"
  },
  {
    "objectID": "W06_RS.html#climate-and-atmospheric-data",
    "href": "W06_RS.html#climate-and-atmospheric-data",
    "title": "6  Remote Sensing",
    "section": "7.4 Climate and Atmospheric Data",
    "text": "7.4 Climate and Atmospheric Data\n\n\n\nSulphur Dioxide plume resulting from ISIS attack on the Al-Mishraq Sulphur Plant in Iraq\n\n\nClimate and atmospheric data can be used to track the effects of conflict on the environment. The European Space Agency’s Sentinel-5p satellites measure the concentration of a number of atmospheric gases, including nitrogen dioxide, methane, and ozone. Measurements are available on a daily basis at a fairly high resolution (1km), allowing for the detection of localized sources of pollution such as oil refineries or power plants. For example, see this Bellingcat article in which Wim Zwijnenburg and I trace pollution to specific facilities operated by multinational oil companies in Iraq.\nThe Copernicus Atmosphere Monitoring Service (CAMS) provides similar data at a lower spatial resolution (45km), but measurements are avaialble on an hourly basis. The timelapse above utilizes CAMS data to show a sulphur dioxide plume resulting from an ISIS attack on the Al-Mishraq Sulphur Plant in Iraq. The plant was used to produce sulphuric acid, for use in fertilizers and pesticides. The attack destroyed the plant, causing a fire which burned for a month and released 21 kilotons of sulphur dioxide into the atmosphere per day; the largest human-made release of sulphur dioxide in history.\n\nApplications\n\nMonitoring of airborne pollution\nTracing pollution back to specific facilities and companies\nVisualizing the effects of one-off environmental catastrophes\n\nNordstream 1 leak\nISIS setting Mishraq sulphur plant on fire\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nCAMS NRT\n2016-Present\n44528m\nGlobal\n\n\nSentinel-5p\n2018-Present\n1113m\nGlobal"
  },
  {
    "objectID": "W06_RS.html#mineral-deposits",
    "href": "W06_RS.html#mineral-deposits",
    "title": "6  Remote Sensing",
    "section": "7.5 Mineral Deposits",
    "text": "7.5 Mineral Deposits\n\n\n\nZinc deposits across Central Africa\n\n\nMining activities often play an important role in conflict. According to an influential study, “the historical rise in mineral prices might explain up to one-fourth of the average level of violence across African countries” between 1997 and 2010. Data on the location of mineral deposits can be used to identify areas where mining activities are likely to be taking place, and several such datasets are available in Google Earth Engine.\n\nApplications\n\nMonitoring mining activity\nIdentifying areas where mining activities are likely to be taking place\nMapping the distribution of resources in rebel held areas in conflicts fueled by resource extraction\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\niSDA\n2001-2017\n30m\nAfrica"
  },
  {
    "objectID": "W06_RS.html#fires",
    "href": "W06_RS.html#fires",
    "title": "6  Remote Sensing",
    "section": "7.6 Fires",
    "text": "7.6 Fires\n\n\n\nDetected fires over Ukraine since 27/02/2022 showing the frontline of the war\n\n\nEarth-observing satellites can detect “thermal anomalies” (fires) from space. NASA’s Fire Information for Resource Management System (FIRMS) provides daily data on active fires in near real time, going back to the year 2000. Carlos Gonzales wrote a comprehensive Bellingcat article on the use of FIRMS to monitor war zones from Ukraine to Ethiopia. The map above shows that FIRMS detected fires over Eastern Ukraine trace the frontline of the war.\nFIRMS data are derived from the MODIS satellite, but only show the central location and intensity of a detected fire. Another MODIS product (linked in the table below) generates a monthly map of burned areas, which can be used to assess the spatial extent of fires.\n\nApplications\n\nIdentification of possible artillery strikes/fighting in places like Ukraine\nEnvironmental warfare and “scorched earth” policies\nLarge scale arson\n\ne.g. Refugee camps burned down in Myanmar\n\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nFIRMS\n2000-Present\n1000m\nGlobal\n\n\nMODIS Burned Area\n2000-Present\n500m\nGlobal"
  },
  {
    "objectID": "W06_RS.html#population-density-estimates",
    "href": "W06_RS.html#population-density-estimates",
    "title": "6  Remote Sensing",
    "section": "7.7 Population Density Estimates",
    "text": "7.7 Population Density Estimates\n\n\n\nPopulation density estimates around Pyongyang, North Korea\n\n\nSometimes, we may want to get an estimate the population in a specific area to ballpark how many people might be affected by a natural disaster, a counteroffensive, or a missile strike. You can’t really google “what is the population in this rectangle i’ve drawn in Northeastern Syria?” and get a good answer. Luckily, there are several spatial population datasets hosted in GEE that let you do just that. Some, such as WorldPop, provide estimated breakdowns by age and sex as well. However, it is extremely important to bear in mind that these are estimates, and will not take into account things like conflict-induced displacement. For example, Oak Ridge National Laboratory’s LandScan program has released high-resolution population data for Ukraine, but this pertains to the pre-war population distribution. The war has radically changed this distribution, so these estimates no longer reflect where people are. Still, this dataset could be used to roughly estimate displacement or the number of people who will need new housing.\n\nApplications:\n\nRough estimates of civilians at risk from conflict or disaster, provided at a high spatial resolution\n\n\n\nDatasets\n\n\n\nSensor\nTimeframe\nResolution\nCoverage\n\n\n\n\nWorldpop\n2000-2021\n92m\nGlobal\n\n\nGPW\n2000-2021\n927m\nGlobal\n\n\nLandScan\n2013–Present\n100m\nUkraine"
  },
  {
    "objectID": "W06_RS.html#building-footprints",
    "href": "W06_RS.html#building-footprints",
    "title": "6  Remote Sensing",
    "section": "7.8 Building Footprints",
    "text": "7.8 Building Footprints\n\n\n\nBuilding footprints in Mariupol, Ukraine colored by whether the building is damaged\n\n\nA building footprint dataset contains the two dimensional outlines of buildings in a given area. Currently, GEE hosts one building footprint dataset which covers all of Africa. In 2022, Microsoft released a free global building footprint dataset, though to use it in Earth Engine you’ll have to download it from their GitHub page and upload it manually to GEE. The same goes for OpenStreetMap (OSM), a public database of building footprints, roads, and other features that also contains useful annotations for many buildings indicating their use. Benjamin Strick has a great youtube video on conducting investigations using OSM data.\n\nApplications:\n\nJoining damage estimate data with the number of buildings in an area\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nOpen Buildings\n2022\nAfrica"
  },
  {
    "objectID": "W06_RS.html#administrative-boundaries",
    "href": "W06_RS.html#administrative-boundaries",
    "title": "6  Remote Sensing",
    "section": "7.9 Administrative Boundaries",
    "text": "7.9 Administrative Boundaries\n\n\n\nSecond-level administrative boundaries in Yemen\n\n\nSpatial analysis often have to aggregate information over a defined area; we may want to assess the total burned area by province in Ukraine, or count the number of Saudi airstrikes by district in Yemen. For that, we need data on these administrative boundaries. GEE hosts several such datasets at the country, province, and district (or equivalent) level.\n\nApplications\n\nQuick spatial calculations for different provinces/districts in a country\n\ne.g. counts of conflict events by district over time\n\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nFAO GAUL\n2015\nGlobal"
  },
  {
    "objectID": "W06_RS.html#global-power-plant-database",
    "href": "W06_RS.html#global-power-plant-database",
    "title": "6  Remote Sensing",
    "section": "7.10 Global Power Plant Database",
    "text": "7.10 Global Power Plant Database\n\n\n\nPower plants in Ukraine colored by type\n\n\nThe Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights. Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. As of June 2018, the database includes around 28,500 power plants from 164 countries. The database is curated by the World Resources Institude (WRI).\n\nApplications:\n\nAnalyzing the impact of conflict on critical infrastructure.\n\ne.g. fighting in Ukraine taking place around nuclear power facilities.\n\nCould be combined with the atmospheric measurements of different pollutants and the population estimates data to assess the impact of various forms of energy generation on air quality and public health.\n\n\n\nDatasets\n\n\n\nDataset\nTimeframe\nCoverage\n\n\n\n\nGPPD\n2018\nGlobal"
  },
  {
    "objectID": "W09_blast.html#change-detection",
    "href": "W09_blast.html#change-detection",
    "title": "9  Blast Damage Assessment",
    "section": "9.1 Change Detection",
    "text": "9.1 Change Detection\nThere are many ways to detect change between two images. The simplest way would be to take an image taken before an event, and subtract it from an image taken after. This is a good way to get a general sense of where change has occurred, but if you only use two images (one before an event and another after), it would be difficult to differentiate between areas that have changed as a result of the event in question, and areas that have changed for other reasons. Things in Beirut (and cities in general) are constantly changing: construction, cars/planes/ships moving, vegetation growing, etc. So we wouldn’t know if the change we’re seeing is a result of the explosion or whether that area is generally prone to change. We can overcome this by comparing a bunch of pre-event images to a bunch of post-event images. This way we can see if the change we’re seeing is consistent across all of the images. If it is, then we can be fairly confident that the change is a result of the event in question. The mean is simply the sum of all the values (\\(x_i\\)) in a set divided by the number of values (\\(n\\)):\n\\[\\large \\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\]\nBut if we just take the average pixel value before and subtract the average pixel value after, we’re not accounting for the variability of that pixel’s values. For example, if we have a pixel that has had an average value of 1 for the month before the event, and a value of 2 in the month after the event, the difference is 1. If that pixel’s value is extremely consistent (it never varies by more than 0.1), such a change would be very significant. But if that pixel’s value is very variable (it varies by 2 or even 3 on a regular basis), then the change is not significant. So we need to account for the variability of the pixel’s values using the standard deviation. It is calculated as the square root of the variance, which is the average of the squared differences from the mean:\n\\[\\large s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2}\\]\nWith just the mean and the standard deviations of two sets of numbers, we can use a statistical test to determine whether the change in means is significant. The simplest way to do this is to use a pixelwise t-test, which is basically just a signal-to-noise ratio: it calculates the difference between two sample means (signal), and divides it by the standard deviations of both samples (noise). In this case, the two samples are the pre- and post-event images. The t-test is applied to each pixel in the image, allowing us to determine whether the change is statistically significant. Given two groups, \\(x_1\\) before the event, and \\(x_2\\) after the event, the \\(t\\) statistic is calculated as:\n\\[ \\Large t = {\\frac{\\overline{x_1}-\\overline{x_2}} {\\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}}} \\]\nWhere:\n\n\\(\\overline{x}\\): Sample Mean\n\\(s^2\\): Sample Standard Deviation\n\\(n\\): Number of observations\n\nThis procedure gives us a number called a t-value, which is a measure of how many standard deviations the difference between the two means is. We’re not going to get into the details here, but a rule of thumb is that if the t-value is greater than 2, then the difference between the two means is significant. If the t-value is less than 2, then the difference is not significant. We’re going to calculate the t-value for each pixel in the image to determine whether that pixel has changed significantly following the event in question. You don’t need to know the details of the t-test to understand the results (but hopefully you’ve got an intuition for what it’s doing). If you’re interested in learning more about statistical tests of this sort, I teach a course on Data Science at the University College London, and have made all of the lectures and courseware open-source. The T-test lecture is here."
  },
  {
    "objectID": "W09_blast.html#implementing-a-t-test-in-earth-engine",
    "href": "W09_blast.html#implementing-a-t-test-in-earth-engine",
    "title": "9  Blast Damage Assessment",
    "section": "9.2 Implementing a t-test in Earth Engine",
    "text": "9.2 Implementing a t-test in Earth Engine\nNow lets go about implementing this in Earth Engine. We’ll start by centering the map on the port of Beirut, and setting the map to satellite view, and defining an area of interest (AOI) as a 3km buffer around the port:\nMap.setCenter(35.51898, 33.90153, 15);\n\nMap.setOptions(\"satellite\");\n\nvar aoi = ee.Geometry.Point(35.51898, 33.90153).buffer(3000);\nNext, let’s define a function in earth engine that will perform the T-Test. The block of code below defines a function to implement a t-test for every pixel in a set of images. The function will be called ‘ttest’, and takes four arguments:\n\ns1: the image collection\nshock: the date of the event\npre_interval: the number of months before the event\npost_interval: the number of months after the event\n\nThe function will return a t-value image, which we can use to determine whether a pixel has changed significantly following the event in question.\nfunction ttest(s1, shock, pre_interval, post_interval) {\n  \n  // Convert the shock date to a date object\n  var shock = ee.Date(shock);\n  // Filter the image collection to the pre-event period\n  var pre = s1.filterDate(\n    shock.advance(ee.Number(pre_interval).multiply(-1), \"month\"),\n    shock\n  );\n  // Filter the image collection to the post-event period\n  var post = s1.filterDate(shock, shock.advance(post_interval, \"month\"));\n  \n  // Calculate the mean, standard deviation, and number of images for the pre-event period\n  var pre_mean = pre.mean();\n  var pre_sd = pre.reduce(ee.Reducer.stdDev());\n  var pre_n = ee.Number(pre.filterBounds(aoi).size());\n  \n  // Calculate the mean, standard deviation, and number of images for the pre-event period\n  var post_mean = post.mean();\n  var post_sd = post.reduce(ee.Reducer.stdDev());\n  var post_n = ee.Number(post.filterBounds(aoi).size());\n  \n  // Calculate the pooled standard deviation\n  var pooled_sd = pre_sd\n    .multiply(pre_sd)\n    .multiply(pre_n.subtract(1))\n    .add(post_sd.multiply(post_sd).multiply(post_n.subtract(1)))\n    .divide(pre_n.add(post_n).subtract(2))\n    .sqrt();\n\n    // Calculate the denominator of the t-test\n  var denom = pooled_sd.multiply(\n    ee.Number(1).divide(pre_n).add(ee.Number(1).divide(post_n)).sqrt()\n  );\n\n    // Calculate the Degrees of Freedom, which is the number of observations minus 2\n  var df = pre_n.add(post_n).subtract(2);\n\n  print(\"Number of Images: \", df);\n\n    // Calculate the t-test using the:\n        // mean of the pre-event period, \n        // the mean of the post-event period, \n        // and the pooled standard deviation\n  var change = post_mean\n    .abs()\n    .subtract(pre_mean.abs())\n    .divide(denom)\n    .abs()\n    .subtract(2);\n\n    // return the t-values for each pixel\n  return change;\n}\nAn important detail in the code above is that we’ve actually tweaked the t-test slightly, in two ways.\nFirst, the algorithm above returns tha absolute value of t (i.e. the absolute value of the difference between the two means). This is because we’re interested in whether the pixel has changed at all, not whether it’s changed in a particular direction. Second, we’ve subtracted 2 from the t-value.\nThe t-value is a measure of how many standard deviations the difference between the two means is. Generally speaking, if the t-value is greater than 2, then the difference between the two means is considered statistically significant. 2 is a fairly abitrary cutoff, but it’s the most commonly used one since it corresponds to the 95% confidence interval (i.e., theres less than a 5% chance of observing a difference that big due to random chance). Now we’ve got a function that can take an image collection and return a t-value image, where a value greater than 0 corresponds to a statistically significant change between the pre-event and post-event periods."
  },
  {
    "objectID": "W09_blast.html#filtering-the-sentinel-1-imagery",
    "href": "W09_blast.html#filtering-the-sentinel-1-imagery",
    "title": "9  Blast Damage Assessment",
    "section": "9.3 Filtering the Sentinel-1 Imagery",
    "text": "9.3 Filtering the Sentinel-1 Imagery\nWe can’t just blindly apply this algorithm to the entire image collection, because the image collection contains images from both ascending and descending orbits. We need to filter the image collection to the ascending and descending orbits, and then calculate the t-value for each orbit separately: this is because the satellite is viewing the scene from a completely different angle when it’s in ascending and descending orbits, which will generate a lot of noise in our data. In fact, even when the satellite is either ascending or descending, we can have multiple images of the same place taken from slightly different orbital tracks because these overlap (see this visualization of orbits). We need to filter the image collection to the relative orbit number that is most common within the image collection. For that, we define a new function called ‘filter_s1’, which takes a single argument: the path (either ‘ASCENDING’ or ‘DESCENDING’).\nfunction filter_s1(path) {\n  \n  // Filter the image collection to the ascending or descending orbit\n  var s1 = ee\n    .ImageCollection(\"COPERNICUS/S1_GRD\")\n    .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VH\"))\n    .filter(ee.Filter.eq(\"instrumentMode\", \"IW\"))\n    .filter(ee.Filter.eq(\"orbitProperties_pass\", path))\n    .filterBounds(aoi)\n    .select(\"VH\");\n\n    // Find the most common relative orbit number\n  var orbit = s1\n    .aggregate_array(\"relativeOrbitNumber_start\")\n    .reduce(ee.Reducer.mode());\n\n    // Filter the image collection to the most common relative orbit number\n  var s1 = s1.filter(ee.Filter.eq(\"relativeOrbitNumber_start\", orbit));\n\n    // Calculate the t-test for the filtered image collection using the function we defined earlier\n  var change = ttest(s1, \"2020-08-04\", 12, 2);\n\n    // Return the t-values for each pixel\n  return change;\n}\nYou’ll notice that we’ve called the ttest function we defined earlier with four arguments:\n\ns1: the Sentinel-1 image collection filtered to the ascending or descending orbit\n2020-08-04: the date of the explosion\n12: the number of months before the explosion to use for the pre-event period; I’m choosing to include the full year prior to the explosion to get a good baseline\n2: the number of months after the explosion to use for the post-event period; I’m including 2 months after the explosion. Much less than that, and we risk missing the effects of the explosion. Much more than that, and we risk including the effects other changes that happened after the explosion, including the reconstruction effort.\n\nNow we want to apply this function to the image collection twice (once for each orbit) and then combine the two images into a single image. After that, we can clip it to the area of interest and display it on the map:\n\n// Call the filter_s1 function twice, once for each orbit, and then combine the two images into a single image\nvar composite = ee\n  .ImageCollection([filter_s1(\"ASCENDING\"), filter_s1(\"DESCENDING\")])\n  .mean()\n  .clip(aoi);\n\n// Define a color palette\nvar palette = [\"440154\", \"3b528b\", \"21918c\", \"5ec962\", \"fde725\"];\n\n// Add the composite to the map\nMap.addLayer(\n  composite,\n  { min: 0, max: 4, opacity: 0.8, palette: palette },\n  \"change\"\n);\nThe visualization parameters correspond the statitical significance of the change in pixel values. Using the Viridis color palette which ranges from purple to yellow, dark purple pixels indicate no significant change, and yellow pixels indicate a significant change with with 95% confidence. The brighter the yellow of a pixel, the more significant the change.\n\n\n\nPixelwise T-Test, 2020\n\n\nThis seems to be working quite well; but remember, ports are generally prone to change. The t-test is accounting for this by calculating each pixel’s variance over the entire time period, but it’s still possible that the change we’re seeing is due to the port rather than the explosion. To test this, we can run the same algorithm on the same area, using the same date cutoff (August 4th), but in a different year; i’ve chosen 2018. This is what’s known as a placebo test: if it’s still showing loads of statistically significant change around the cutoff, our algorithm is probably picking up on port activity rather than the explosion.\n\n\n\nPixelwise T-Test, 2018\n\n\nCompared to the 2020 image, there’s a lot less yellow (significant change). That being said there are a few yellow areas. This could be due to a number of reasons: ships coming and going, cranes moving, and containers being loaded and unloaded would all register in the change detection algorithm. There are also a number of yellow specks throughout the city, which is also to be expected since cities are also generally in a state of flux. Construction, demolition, and even the growth of vegetation can all be detected by the algorithm.\nHowever, the scale and quantity of the change is nowhere near what it was for the 2020 image. This is a good sign that the algorithm detecting change resulting from the explosion."
  },
  {
    "objectID": "W09_blast.html#validation",
    "href": "W09_blast.html#validation",
    "title": "9  Blast Damage Assessment",
    "section": "9.4 Validation",
    "text": "9.4 Validation\nGreat. We’ve developed our very own change detection algorithm in earth engine, applied it to the Beirut explosion, and it seems to be working using a basic placebo test. But how do we know that it’s correctly predicting the extent of the damage, and not wildly over/underestimating?\nGiven that this was a few years ago, we have the benefit of hindsight. In particular, the United Nations and the Municipality of Beirut have published a report on the damage caused by the explosion. This report includes estimates of the number of buildings damaged or destroyed by the explosion, as well as the number of people displaced. The report states that approximately 10,000 buildings were damaged within a 3km radius of the port. If our algorithm suggests that only 1,000 buildings were damaged, it’s undershooting. If it suggests that 100,000 buildings were damaged, it’s overshooting.\nUsing building footprint data and the t-test image we just generated, we can generate an estimate of the number of damaged buildings according to our model. First, we want to generate a thresholded image, where pixels with a value greater than 0 are set to 1, and all other pixels are set to 0. We can then use this mask to reduce the building footprints to a single value for each building, where the value is the mean of the t-test image within the footprint. If the mean value is greater than 0, the building is damaged. If it’s less than 0, the building is not damaged.\n// Create a mask of the t-test image, where pixels with a value greater than 0 are set to 1, and all other pixels are set to 0\nvar threshold = composite.updateMask(composite.gt(0));\n\n// Load the building footprints\nvar buildings = ee\n  .FeatureCollection(\"projects/sat-io/open-datasets/MSBuildings/Lebanon\")\n  .filterBounds(aoi);\n\n// Calculate the mean value of the t-test image within each building footprint\nvar damaged_buildings = threshold.reduceRegions({\n  collection: buildings,\n  reducer: ee.Reducer.mean(),\n  scale: 1,\n});\n\n// Print the number of buildings with a mean value greater than 0\n// i.e., those displaying statistically  significant change\nprint(damaged_buildings.filter(ee.Filter.gt(\"mean\", 0)).size());\nThe result is 9,256, which is pretty damn close to 10,000. We can also visualize the building footprints on the map, colored according the mean value of the t-test image within the footprint, where:\n\nBlue = no damage\nGreen = low damage\nYellow/Orange = medium damage\nRed = high levels of damage\n\n\n// Create an empty image\nvar empty = ee.Image().byte();\n\n// Paint the building footprints onto the empty image\nvar outline = empty.paint({\n  featureCollection: damaged_buildings,\n  color: \"mean\",\n  width: 5,\n});\n\n// Define a color palette\nvar building_palette = [\n  \"0034f5\",\n  \"1e7d83\",\n  \"4da910\",\n  \"b3c120\",\n  \"fcc228\",\n  \"ff8410\",\n  \"fd3000\",\n];\n\n// Add the image to the map\nMap.addLayer(\n  outline,\n  { palette: building_palette, min: 0, max: 2 },\n  \"Damaged Buildings\"\n);\nThe result naturally resembles the underlying t-test image, with high levels of damage concetrated around the port, and progressively decreasing damage with distance:\n\n\n\nBuilding Footprints colored according to estimated blast damage\n\n\nTo get a better sense of how these predicitons correspond to actual damage, we can zoom in and turn on the Google satellite basemap, which has imagery taken just after the explosion; you can still see capsized boats in the port. Zooming in to the epicentre, we can see several warehouses that were effectively vaporized. Our change detection algorithm picks up on a high degree of change, as indicated by the red outlines of the building footprints:\n\n\n\nPredicted damage and optical satellite imagery in the Port of Beirut, August 2020\n\n\nThis is pretty low-hanging fruit. Let’s look at a different area, around 1.3km east from the epicentre with a mix of warehouses and residential buildings:\n\n\n\nArea east of the port: 35.533194, 33.9024\n\n\nHere, there’s greater variation in the predictions. I’ve highlighted three areas.\nIn Area A, we see a warehouse with a highily deformed roof; panels of corrugated iron are missing, and much of the roof is warped. The building footprint for this warehouse is red, suggesting that our algorithm correctly predicts a significant amount of blast damage.\nIn Area B, we see a medium-rise building. If you look closely at the southern edge of the building, you’ll see the siding has been completely torn off and is laying on the sidewalk. The bulding footprint is orange, suggesting a medium amount of change. We may be underestimating a bit here.\nIn Area C, there are a bunch of high rise buildings clustered together. The building footprints are all blue, suggesting little to no damage. This is a bit of a surprise given how damaged areas A and B are. If you squint at the satellite image, it is indeed hard to tell if these buildings are damaged because we’re looking at them from the top down, when much of the damage (e.g., the windows being blown out) would only be visible from the side. Indeed, our own estimate of the number of damaged buildings based on the algorithm we developed is about 8% shy of the U.N.’s estimate. This may be why."
  },
  {
    "objectID": "W09_blast.html#conclusion",
    "href": "W09_blast.html#conclusion",
    "title": "9  Blast Damage Assessment",
    "section": "9.5 Conclusion",
    "text": "9.5 Conclusion\nIn this practical, we created a custom change detection algorithm that conducts a pixelwise t-test to detect change resulting from the 2020 explosion in the port of Beirut. By defining our own functions to do most of this analysis, we can apply the same workflow quite easily to a different context by simply moving the AOI and inputting the date of the shock. A placebo test showed that it’s not just detecting general change in the area, but specifically change resulting from the explosion: when we keep everythgin the same but change the year of the shock, we see very little significant change being detected. Finally, by joining the predicted damage map to building footprints, we come up with an estimate of 9,256 damaged buildings, which is pretty close to the U.N.’s estimate of 10,000. That concludes the portion of this case study that deals with Earth Engine, but if you’re interested in learning more about why we’re coming up a bit short on the damage estimate (and some different ways of looking at the problem), read on."
  },
  {
    "objectID": "W09_blast.html#extension-satellite-imagery-and-its-limits",
    "href": "W09_blast.html#extension-satellite-imagery-and-its-limits",
    "title": "9  Blast Damage Assessment",
    "section": "9.6 Extension: Satellite Imagery and its Limits",
    "text": "9.6 Extension: Satellite Imagery and its Limits\nThough satellite imagery analysis is undoubtedly one of the best tools we have at our disposal to analyze this sort of phenomenon, it appears to systematically underestimate the extent of damage in Beirut. I outline an alternative approach using Open Street Map data to create a 3D model of Beirut and the explosion to analyze directional blast damage. Again, we’re now leaving Earth Engine and moving to Blender, so if you’re not interested in that feel free to skip ahead to the next case study.\nBelow is one of the most viewed videos of the explosion:\n\n\nStunning video shows explosions just minutes ago at Beirut port pic.twitter.com/ZjltF0VcTr\n\n— Borzou Daragahi 🖊🗒 ((borzou?)) August 4, 2020\n\n\nGeolocating this video was pretty simple thanks to the Greek Orthodox church (highlighted in green below) and the road leading to it (highlighted in blue). The red box indicates the likely location (33.889061, 35.515909) from which the person was filming:\n\nThe video shows heavy damage being sustained by areas well outside the zones classified as damaged in the maps above (both my own and NASA’s). Indeed, substantial damage was reported several kilometers away.\nWhy are satellite images underestimating damage in Beirut? Satellite images are taken from above, and are two-dimensional. Much of the damage caused by the blast, however, was directional; the pressure wave hit the sides of buildings, as shown in this diagram from a FEMA manual:\n\nAreas close to the explosion suffered so much damage that it could be seen from above, but even if an apartment building had all of its windows blown out, this would not necessarily be visible in a top-down view. Even for radar, which does technically collect data in three dimensions, the angle problem remains; a high resolution radar might be able to tell you how tall an apartment complex is, but it won’t give you a clear image of all sides. Case in point: the NASA damage map was created using Sentinel-1 SAR data. In a nutshell, damage assessment in this case is a three-dimensional problem, and remote sensing is a two-dimensional solution.\n\n9.6.1 Creating a 3D model of Beirut\nTo create a more accurate rendering of directional blast damage, three dimensional data are required. Data from Open Street Maps (OSM) contains information on both the “footprints” (i.e., the location and shape) as well as the height of buildings, which is enough to create a three dimensional model of Beirut. 3D rendering was done in Blender using the Blender-OSM add-on to import a satellite basemap, terrain raster, and OSM data.\nGeolocated videos of the blast can be used to verify and adjust the model. Below is a side-by-side comparison of the twitter video and a 3D rendition of OSM data:\n\nSome slight adjustments to the raw OSM data were made to achieve the image on the right. The building footprints are generally very accurate and comprehensive in coverage, but the building height data does occasionally have to be adjusted manually. A simple and reliable way of doing this is to look at the shadows cast by the building on the satellite base map and scale accordingly. I also added a rough texture to the buildings to help differentiate them, and added the domed roof of the Greek Orthodox church for reference.\nFor good measure, a second video is geolocated following the same procedure:\n\n\nAnother view of the explosions in Beirut pic.twitter.com/efT5VlpMkj\n\n— Borzou Daragahi 🖊🗒 ((borzou?)) August 4, 2020\n\n\nThe second pier (highlighted in green) and the angle (in blue) serve as references:\n\nThe video was taken from the rooftop of a japanese restaurant called Clap Beirut (in red above). This is confirmed by a picture of the rooftop bar on google images, which matches the bar that can be seen at 0:02 in the twitter video. Below is a comparison of the video view and the 3D OSM model:\n\nThough somewhat grainy, the basemap on the OSM rendering shows the same parking lot in the foreground, the second pier, and the same two buildings highlighted in yellow. Having created a 3D model of Beirut using OSM data, we can now simulate how the explosion would interact with the cityscape.\n\n\n9.6.2 Using a Viewshed Analysis to Assess Blast Exposure\nAs the pressure wave moved through the Beirut, some buildings bore the full force of the explosion, while others were partially shielded by taller structures. A viewshed analysis can be conducted to identify surfaces that were directly exposed to the explosion by creating a lighting object at ground zero; areas that are lit up experienced unobstructed exposure to the blast:\n\nPressure waves, like sound, are capable of diffraction (beding around small obstructions). To roughly simluate this, the lighting object is gradually raised, allowing the light to pass “around” obstructions. Warehouses on the Eastern side of the docks, as well as the first row of apartment buildings facing the docks are immediately affected. As the lighting object rises above the warehouse, more areas suffer direct exposure.\nUsing two lighting objects– a red one at 10 meters and a blue one at 20 meters above the warehouse at ground zero– the intensity of the blast in different areas is highlighted; red areas suffered direct exposure, blue areas suffered partially obstructed exposure, and black areas were indirectly exposed.\n\nIn the immediate vicinity of the explosion the large “L” shaped building (Lebanon’s strategic grain reserve) is bright red, and was barely left standing. It absorbed a large amount of the blast, shielding areas behind it and thereby casting a long blue shadow to the West. If one refers back to the satellite damage maps above, there appears to be significantly less damage in the area just West of (“behind”) the grain silo, roughly corresponding to the blue shadow above. While these areas were still heavily damaged, they seem to have suffered less damage than areas of equal distance to the East.\n\n\n9.6.3 Accounting for Diffraction\nThe viewshed analysis tells us which sides of a building are exposed to the blast, but it’s a pretty rough approximation of the way the pressure wave would respond to obstacles in its path. As previously mentioned, pressure waves behave much like sound waves or waves in water: they bounce off of objects, move around obstructions, and gradually fade.\nTo get a more precise idea of the way in which the blast interacted with the urban environment, we can model the blast as an actual wave using the “dynamic wave” feature in Blender. This effectively involves creating a two-dimensional plane, telling it to behave like water, and simulating an object being dropped into the water. By putting an obstruction in this plane, we can see how the wave responds to it. As an example, the grain silo has been isolated below:\n\nAs the blast hits the side of the silo, it is reflected. Two large waves can be seen traveling to the right: the initial blast wave, and the reflection from the silo which rivals the initial wave in magnitude. To the left, the wave travels around the silo but is significantly weakened.\nBroadening the focus and adding the rest of the OSM data back in, we can observe how the pressure wave interacted with buildings on the waterfront:\n\nThe warehouses on the docks were omitted to emphasize the interaction between the pressure wave and the waterfront buildings; their light metal structure and low height means they would have caused little reflection anyway. The general pattern of the dynamic wave is consistent with the viewshed, but adds a layer of detail. The blast is reflected off of the silo towards the East, leading to a double hit. Though the wave still moves around the silo to the West, the pressure is diminished. Once the wave hits the highrises, the pattern becomes noisy as the wave both presses forward into the mainland and is reflected back towards the pier.\n\n\n9.6.4 Modeling the Pressure Wave\nNow that we’ve accounted for the directionality of the blast and the influence of buildings, we can model the pressure wave itself. An expanding sphere centered at ground zero is used to model the progression of the pressure wave through the city. To get a visual sense of the blast’s force, the color of the sphere will be a function of the pressure exerted by pressure wave.\nThe pressure exerted by the explosion in kilopascals (kPa) at various distances can be calculated using the DoD’s Blast Effects Computer, which allows users to input variables such as the TNT equivalent of the ordnance, storage method, and elevation. Though there are several estimates, the blast was likely equivalent to around 300 tons of TNT. The direct “incident pressure” of the pressure wave is shown in blue. However, pressure waves from explosions that occur on the ground are reflected upwards, amplifying the total pressure exerted by the blast. This “reflected pressure” is shown in orange:\n\n\n\nFor reference, 137 kPa results in 99% fatalities, 68 kPa is enough to cause structural damage to most buildings, and 20 kPa results in serious injuries. 1-6 kPa is enough to break an average window. At 1km, the reflected pressure of the blast (18 kPa) was still enough to seriously injure. Precisely calculating the force exerted by an explosion is exceptionally complicated, however, so these numbers should be treated as rough estimates. Further analysis of the damage caused by blasts blast can be derived from the UN’s Explosion Consequences Analysis calculator which provides distance values for different types of damage and injuries.\nLinking the values in this graph to the color of the pressure wave sphere provides a visual representation of the blast’s force as it expands. An RGB color scale corresponds to the blast’s overpressure at three threshold values.\n\nBy keeping the lighting object from the viewshed analysis and placing it within the expanding sphere of the pressure wave, we combine two key pieces of information: the pressure exerted by the blast (the color of the sphere), and the level of directional exposure (brightness).\nNow, referring back to the two geolocated twitter videos from earlier, we can recreate the blast in our 3D model and get some new insights. Below is a side-by-side comparison of the first video and the 3D model:\n\nJudging by the twitter video alone, it would be very hard to tell the fate of the person filming or the damage caused to the building that they were in. However, the 3D model shows that despite having an unobstructed view of the explosion, the incident pressure of the pressure wave had decreased significtantly by the time it reached the viewing point. The blue-green color corresponds to roughly 15 kPa– enough to injure and break windows, but not enough to cause structural damage to the building.\nThe second twitter video was taken slightly closer to ground zero, but the view was partially obstructed by the grain silo:\n\nThough the pressure wave probably exerted more pressure compared to the first angle, the partial obstruction of the grain silo likely tempered the force of the blast.\n\n\n9.6.5 Assessing Damage to the Skyline Tower\nAs a concrete example of how this approach can be used to assess damage (or predict it, if one had the foresight), let us consider the Skyline Tower, pictured below following the explosion:\n\nThis partial side view shows two faces of the building, labelled “A” and “B” above. Side A was nearly perpendicular to the blast, and just 600 m from ground zero. Based on the previous modeling, the pressure wave exerted roughly 40 kPa on this side of the building. The corner where sides A and B meet, highlighted in green, shows total destruction of windows, removal of most siding panels, and structural damage. The back corner, highlighted in red, shows many windows still intact, indicating that the maximum overpressure on this side of the building likely didn’t exeed 10 kPa. In other words, standing on the front balcony would likely have led to serious injury but standing on the back balcony would have been relatively safe.\nThe animation below shows the Skyline Tower as it is hit by the pressure wave, with sides A and B labeled:\n\nThe bright green color of the pressure wave indicates a strong likelihood of structural damage. Side A can be seen taking a direct hit, while side B is angled slighly away. Despite not being directly exposed to the blast, it likely still took reflective damage from some of the neighbouring buildings. Both the incident overpressure indicated by the color of the sphere, as well as the relative brightness of sides A and B both correspond closely to the observed damage taken by the Skyline Tower.\n\n\n9.6.6 Further Research\nThough satellite imagery analysis is an indispensable tool in disaster response, it has limitations. Urban blast damage in particular is difficult to assess accurately because it is highly directional and much of it cannot be seen from a bird’s eye view. Using free and open source tools, an interactive 3D model of an urban explosion can be generated, allowing for a highly detailed investigation of directional blast damage. This can be achieved in three steps:\nFirst, creating a 3D model of the urban area using Blender and Open Street Maps data. Second, conducting a viewshed analysis using lighting objects to gauge levels of unobstructed exposure to the pressure wave. Third, modeling the explosion using geolocated videos of the event and ordnance calculators. For added detail, a dynamic wave analysis can be used to more precisely model how the pressure wave interacts with buildings.\nOnce properly modeled, the explosion can be viewed from any angle in the city. The viewshed analysis can be calibrated more finely by ground-truthing various damage levels (e.g. broken windows) at different locations. In the absence of an official address registry in Beirut, OSM is already being used by the Lebanese Red Cross (donate here) to conduct neighborhood surveys assessing blast damage. As such, this type of damage analysis can quickly be integrated into relief efforts, adapted to model disasters in different cities, and can even be used to simulate the destructive potential of hypothetical explosions to promote readiness.\nSpeacial thanks to my nuclear physicist brother, Sean, for making sure I didn’t commit too many crimes against Physics."
  }
]