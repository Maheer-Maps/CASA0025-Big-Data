{"title":"Vectors and Tables","markdown":{"headingText":"Vectors and Tables","containsRefs":false,"markdown":"\n\n\n\nIn addition to raster data processing, Earth Engine supports a rich set of vector processing tools. This Part introduces you to the vector framework in Earth Engine, shows you how to create and to import your vector data, and how to combine vector and raster data for analyses.\n\n\n\n## Exploring Vectors\n\n\n\n\n \n:::{.callout-tip}\n## Chapter Information\n\n#### Author {.unlisted .unnumbered}\n\n\nAJ Purdy, Ellen Brock, David Saah\n\n\n#### Overview {.unlisted .unnumbered}\n\nIn this chapter, you will learn about features and feature collections and how to use them in conjunction with images and image collections in Earth Engine. Maps are useful for understanding spatial patterns, but scientists often need to extract statistics to answer a question. For example, you may make a false-color composite showing which areas of San Francisco are more “green”—i.e., have more healthy vegetation—than others, but you will likely not be able to directly determine which block in a neighborhood is the most green. This tutorial will demonstrate how to do just that by utilizing vectors.\n\nAs described in Chap. F4.0, an important way to summarize and simplify data in Earth Engine is through the use of reducers. Reducers operating across space were used in Chap. F3.0, for example, to enable image regression between bands. More generally, chapters in Part F3 and Part F4 used reducers mostly to summarize the values across bands or images on a pixel-by-pixel basis. What if you wanted to summarize information within the confines of given spatial elements- for example, within a set of polygons? In this chapter, we will illustrate and explore Earth Engine’s method for doing that, which is through a reduceRegions call.\n\n#### Learning Outcomes {.unlisted .unnumbered}\n\n\n\n*   Uploading and working with a shapefile as an asset to use in Earth Engine.\n*   Creating a new feature using the geometry tools.\n*   Importing and filtering a feature collection in Earth Engine.\n*   Using a feature to clip and reduce image values within a geometry.\n*   Use reduceRegions to summarize an image in irregular neighborhoods.\n*   Exporting calculated data to tables with Tasks.\n\n### Assumes you know how to: {.unlisted .unnumbered}\n\n*   Import images and image collections, filter, and visualize (Part F1).\n*   Calculate and interpret vegetation indices (Chap. F2.0).\n*   Use drawing tools to create points, lines, and polygons (Chap. F2.1).\n:::\n\n### Introduction {.unlisted .unnumbered}\n\nIn the world of geographic information systems (GIS), data is typically thought of in one of two basic data structures: raster and vector. In previous chapters, we have principally been focused on raster data—data using the remote sensing vocabulary of pixels, spatial resolution, images, and image collections. Working within the vector framework is also a crucial skill to master. If you don’t know much about GIS, you can find any number of online explainers of the distinctions between these data types, their strengths and limitations, and analyses using both data types. Being able to move fluidly between a raster conception and a vector conception of the world is powerful, and is facilitated with specialized functions and approaches in Earth Engine.    \n\nFor our purposes, you can think of vector data as information represented as points (e.g., locations of sample sites), lines (e.g., railroad tracks), or polygons (e.g., the boundary of a national park or a neighborhood). Line data and polygon data are built up from points: for example, the latitude and longitude of the sample sites, the points along the curve of the railroad tracks, and the corners of the park that form its boundary. These points each have a highly specific location on Earth’s surface, and the vector data formed from them can be used for calculations with respect to other layers. As will be seen in this chapter, for example, a polygon can be used to identify which pixels in an image are contained within its borders. Point-based data have already been used in earlier chapters for filtering image collections by location (see Part F1), and can also be used to extract values from an image at a point or a set of points (see Chap. F5.2). Lines possess the dimension of length and have similar capabilities for filtering image collections and accessing their values along a transect. In addition to using polygons to summarize values within a boundary, they can be used for other, similar purposes—for example, to clip an image.\n\nAs you have seen, raster features in Earth Engine are stored as an Image or as part of an ImageCollection. Using a similar conceptual model, vector data in Earth Engine is stored as a Feature or as part of a FeatureCollection. Features and feature collections provide useful data to filter images and image collections by their location, clip images to a boundary, or statistically summarize the pixel values within a region.\n\nIn the following example, you will use features and feature collections to identify which city block near the University of San Francisco (USF) campus is the most green.\n\n\n### Using Geometry Tools to Create Features in Earth Engine\n\nTo demonstrate how geometry tools in Earth Engine work, let’s start by creating a point, and two polygons to represent different elements on the USF campus.\n\nClick on the geometry tools in the top left of the Map pane and create a point feature. Place a new point where USF is located (see Fig. F5.0.1).\n\n![Fig. F5.0.1 Location of the USF campus in San Francisco, California. Your first point should be in this vicinity. The red arrow points to the geometry tools.](F5/image54.png)\n\n\nUse Google Maps to search for “Harney Science Center” or “Lo Schiavo Center for Science.” Hover your mouse over the Geometry Imports to find the +new layer menu item and add a new layer to delineate the boundary of a building on campus.\n\nNext, create another new layer to represent the entire campus as a polygon.\n\nAfter you create these layers, rename the geometry imports at the top of your script. Name the layers usf_point, usf_building, and usf_campus. These names are used within the script shown in Fig. F5.0.2.\n\n![Fig. F5.0.2 Rename the default variable names for each layer in the Imports section of the code at the top of your script](F5/image10.png)\n\n\n:::{.callout-note}\nCode Checkpoint F50a. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Loading Existing Features and Feature Collections in Earth Engine\n\nIf you wish to have the exact same geometry imports in this chapter for the rest of this exercise, begin this section using the code at the Code Checkpoint above.\n\nNext, you will load a city block dataset to determine the amount of vegetation on blocks near USF. The code below imports an existing feature dataset in Earth Engine. The Topologically Integrated Geographic Encoding and Referencing (TIGER) boundaries are census-designated boundaries that are a useful resource when comparing socioeconomic and diversity metrics with environmental datasets in the United States.\n\n```js\n// Import the Census Tiger Boundaries from GEE.  \nvar tiger = ee.FeatureCollection('TIGER/2010/Blocks');  \n  \n// Add the new feature collection to the map, but do not display.  \nMap.addLayer(tiger, {   'color': 'black'}, 'Tiger', false);\n\n```\nYou should now have the geometry for USF’s campus and a layer added to your map that is not visualized for census blocks across the United States. Next, we will use neighborhood data to spatially filter the TIGER feature collection for blocks near USF’s campus.\n\n### Importing Features into Earth Engine\n\nThere are many image collections loaded in Earth Engine, and they can cover a very large area that you might want to study. Borders can be quite intricate (for example, a detailed coastline), and fortunately there is no need for you to digitize the intricate boundary of a large geographic area. Instead, we will show how to find a spatial dataset online, download the data, and load this into Earth Engine as an asset for use.\n\n#### Find a Spatial Dataset of San Francisco Neighborhoods\n\nUse your internet searching skills to locate the “Analysis Neighborhoods” dataset covering San Francisco. This data might be located in a number of places, including DataSF, the City of San Francisco’s public-facing data repository.\n\n![Fig. F5.0.3 DataSF website neighborhood shapefile to download](F5/image27.png)\n\n\nAfter you find the Analysis Neighborhoods layer, click Export and select Shapefile (Fig. F5.0.3). Keep track of where you save the zipped file, as we will load this into Earth Engine. Shapefiles contain vector-based data—points, lines, polygons—and include a number of files, such as the location information, attribute information, and others.\n\nExtract the folder to your computer. When you open the folder, you will see that there are actually many files. The extensions (.shp, .dbf, .shx, .prj) all provide a different piece of information to display vector-based data. The .shp file provides data on the geometry. The .dbf file provides data about the attributes. The .shx file is an index file. Lastly, the .prj file describes the map projection of the coordinate information for the shapefile. You will need to load all four files to create a new feature asset in Earth Engine.\n\n#### Upload SF Neighborhoods File as an Asset\n\nNavigate to the Assets tab (near Scripts). Select New > Table Upload > Shape files (Fig. F5.0.4).\n\n![Fig. F5.0.4 Import an asset as a zipped folder](F5/image52.png)\n\n\n#### Select Files and Name Asset\n\nClick the Select button and then use the file navigator to select the component files of the shapefile structure (i.e., .shp, .dbf, .shx, and .prj) (Fig. F5.0.5). Assign an Asset Name so you can recognize this asset.\n\n![Fig. F5.0.5 Select the four files extracted from the zipped folder. Make sure each file has the same name and that there are no spaces in the file names of the component files of the shapefile structure.](F5/image43.png)\n\n\nUploading the asset may take a few minutes. The status of the upload is presented under the Tasks tab. After your asset has been successfully loaded, click on the asset in the Assets folder and find the collection ID. Copy this text and use it to import the file into your Earth Engine analysis.\n\nAssign the asset to the table (collection) ID using the script below. Note that you will need to replace 'path/to/your/asset/assetname' with the actual path copied in the previous step.\n\n```js\n// Assign the feature collection to the variable sfNeighborhoods.  \nvar sfNeighborhoods = ee.FeatureCollection(   'path/to/your/asset/assetname');  \n  \n// Print the size of the feature collection.  \n// (Answers the question how many features?)  \nprint(sfNeighborhoods.size());  \nMap.addLayer(sfNeighborhoods, {   'color': 'blue'}, 'sfNeighborhoods');\n\n```\nNote that if you have any trouble with loading the FeatureCollection using the technique above, you can follow directions in the Checkpoint script below to use an existing asset loaded for this exercise.\n\n:::{.callout-note}\nCode Checkpoint F50b. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Filtering Feature Collections by Attributes\n\n#### Filter by Geometry of Another Feature\n\nFirst, let’s find the neighborhood associated with USF. Use the first point you created to find the neighborhood that intersects this point; filterBounds is the tool that does that, returning a filtered feature.\n\n```js\n// Filter sfNeighborhoods by USF.  \nvar usfNeighborhood = sfNeighborhoods.filterBounds(usf_point);\n\n```\nNow, filter the blocks layer by USF’s neighborhood and visualize it on the map.\n\n```js\n// Filter the Census blocks by the boundary of the neighborhood layer.  \nvar usfTiger = tiger.filterBounds(usfNeighborhood);  \nMap.addLayer(usfTiger, {}, 'usf_Tiger');\n\n```\n#### Filter by Feature (Attribute) Properties\n\nIn addition to filtering a FeatureCollection by the location of another feature, you can also filter it by its properties. First, let’s print the usfTiger variable to the Console and inspect the object.\n\n```js\nprint(usfTiger);\n```\n\nYou can click on the feature collection name in the Console to uncover more information about the dataset. Click on the columns to learn about what attribute information is contained in this dataset. You will notice this feature collection contains information on both housing ('housing10') and population ('pop10').\n\nNow you will filter for blocks with just the right amount of housing units. You don’t want it too dense, nor do you want too few neighbors.\n\nFilter the blocks to have fewer than 250 housing units.\n\n```js\n// Filter for census blocks by housing units.  \nvar housing10_l250 = usfTiger  \n   .filter(ee.Filter.lt('housing10', 250));\n\n```\nNow filter the already-filtered blocks to have more than 50 housing units.\n\n```js\nvar housing10_g50_l250 = housing10_l250.filter(ee.Filter.gt(   'housing10', 50));\n```\nNow, let’s visualize what this looks like.\n\n```js\nMap.addLayer(housing10_g50_l250, {   'color': 'Magenta'}, 'housing');\n```\n\nWe have combined spatial and attribute information to narrow the set to only those blocks that meet our criteria of having between 50 and 250 housing units.\n\n#### Print Feature (Attribute) Properties to Console\n\nWe can print out attribute information about these features. The block of code below prints out the area of the resultant geometry in square meters.\n\n```js\nvar housing_area = housing10_g50_l250.geometry().area();  \nprint('housing_area:', housing_area);\n```\n\nThe next block of code reduces attribute information and prints out the mean of the housing10 column.\n\n```js\nvar housing10_mean = usfTiger.reduceColumns({  \n   reducer: ee.Reducer.mean(),  \n   selectors: ['housing10']  \n});  \n\nprint('housing10_mean', housing10_mean);\n```\n\nBoth of the above sections of code provide meaningful information about each feature, but they do not tell us which block is the most green. The next section will address that question.\n\n:::{.callout-note}\nCode Checkpoint F50c. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Reducing Images Using Feature Geometry\n\nNow that we have identified the blocks around USF’s campus that have the right housing density, let’s find which blocks are the greenest.\n\nThe Normalized Difference Vegetation Index (NDVI), presented in detail in Chap. F2.0, is often used to compare the greenness of pixels in different locations. Values on land range from 0 to 1, with values closer to 1 representing healthier and greener vegetation than values near 0.\n\n#### Create an NDVI Image\n\nThe code below imports the Landsat 8 ImageCollection as landsat8. Then, the code filters for images in 2021. Lastly, the code sorts the images from 2021 to find the least cloudy day.\n\n```js\n// Import the Landsat 8 TOA image collection.  \nvar landsat8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_TOA');  \n  \n// Get the least cloudy image in 2015.  \nvar image = ee.Image(  \n   landsat8  \n   .filterBounds(usf_point)  \n   .filterDate('2015-01-01', '2015-12-31')  \n   .sort('CLOUD_COVER')  \n   .first());\n\n```\nThe next section of code assigns the near-infrared band (B5) to variable nir and assigns the red band (B4) to red. Then the bands are combined together to compute NDVI as (nir − red)/(nir + red).\n\n```js\nvar nir = image.select('B5');  \nvar red = image.select('B4');  \nvar ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI');\n```\n#### Clip the NDVI Image to the Blocks Near USF\n\nNext, you will clip the NDVI layer to only show NDVI over USF’s neighborhood.\n\nThe first section of code provides visualization settings.\n```js\nvar ndviParams = {  \n   min: -1,  \n   max: 1,  \n   palette: ['blue', 'white', 'green']  \n};\n```\nThe second block of code clips the image to our filtered housing layer.\n\n```js\nvar ndviUSFblocks = ndvi.clip(housing10_g50_l250);  \nMap.addLayer(ndviUSFblocks, ndviParams, 'NDVI image');  \nMap.centerObject(usf_point, 14);\n```\nThe NDVI map for all of San Francisco is interesting, and shows variability across the region. Now, let’s compute mean NDVI values for each block of the city.\n\n#### Compute NDVI Statistics by Block\n\nThe code below uses the clipped image ndviUSFblocks and computes the mean NDVI value within each boundary. The scale provides a spatial resolution for the mean values to be computed on.\n\n```js\n// Reduce image by feature to compute a statistic e.g. mean, max, min etc.  \nvar ndviPerBlock = ndviUSFblocks.reduceRegions({  \n   collection: housing10_g50_l250,  \n   reducer: ee.Reducer.mean(),  \n   scale: 30,  \n});\n\n```\nNow we’ll use Earth Engine to find out which block is greenest.  \n\n#### Export Table of NDVI Data by Block from Earth Engine to Google Drive\n\nJust as we loaded a feature into Earth Engine, we can export information from Earth Engine. Here, we will export the NDVI data, summarized by block, from Earth Engine to a Google Drive space so that we can interpret it in a program like Google Sheets or Excel.\n\n```js\n// Get a table of data out of Google Earth Engine.  \nExport.table.toDrive({  \n   collection: ndviPerBlock,  \n   description: 'NDVI_by_block_near_USF'  \n});\n\n```\nWhen you run this code, you will notice that you have the Tasks tab highlighted on the top right of the Earth Engine Code Editor (Fig. F5.0.6). You will be prompted to name the directory when exporting the data.\n\n![Fig. F5.0.6 Under the Tasks tab, select Run to initiate download](F5/image4.png)\n\n\nAfter you run the task, the file will be saved to your Google Drive. You have now brought a feature into Earth Engine and also exported data from Earth Engine.\n\n:::{.callout-note}\nCode Checkpoint F50d. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Identifying the Block in the Neighborhood Surrounding USF with the Highest NDVI\n\nYou are already familiar with filtering datasets by their attributes. Now you will sort a table and select the first element of the table.\n\n```js\nndviPerBlock = ndviPerBlock.select(['blockid10', 'mean']);  \nprint('ndviPerBlock', ndviPerBlock);  \n\nvar ndviPerBlockSorted = ndviPerBlock.sort('mean', false);  \nvar ndviPerBlockSortedFirst = ee.Feature(ndviPerBlock.sort('mean',false) //Sort by NDVI mean in descending order.   \n                                       .first()); //Select the block with the highest NDVI.  \nprint('ndviPerBlockSortedFirst', ndviPerBlockSortedFirst);\n```\n\nIf you expand the feature of ndviPerBlockSortedFirst in the Console, you will be able to identify the blockid10 value of the greenest block and the mean NDVI value for that area.\n\nAnother way to look at the data is by exporting the data to a table. Open the table using Google Sheets or Excel. You should see a column titled “mean.” Sort the mean column in descending order from highest NDVI to lowest NDVI, then use the blockid10 attribute to filter our feature collection one last time and display the greenest block near USF.\n\n```js\n// Now filter by block and show on map!  \nvar GreenHousing = usfTiger.filter(ee.Filter.eq('blockid10',  \n'###')); //< Put your id here prepend a 0!  \n\nMap.addLayer(GreenHousing, {   'color': 'yellow'}, 'Green Housing!');\n\n```\n:::{.callout-note}\nCode Checkpoint F50e. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Conclusion {.unnumbered}\n\nIn this chapter, you learned how to import features into Earth Engine. In Sect. 1, you created new features using the geometry tools and loaded a feature from Earth Engine’s Data Catalog. In Sect. 2, you loaded a shapefile to an Earth Engine asset. In Sect. 3, you filtered feature collections based on their properties and locations. Finally, in Sects. 4 and 5, you used a feature collection to reduce an image, then exported the data from Earth Engine. Now you have all the tools you need to load, filter, and apply features to extract meaningful information from images using vector features in Earth Engine.\n\n\n\n## Raster/Vector Conversions\n\n \n:::{.callout-tip}\n## Chapter Information\n\n#### Author {.unlisted .unnumbered}\n\n\n\nKeiko Nomura, Samuel Bowers\n\n\n\n#### Overview {.unlisted .unnumbered}\n \n\nThe purpose of this chapter is to review methods of converting between raster and vector data formats, and to understand the circumstances in which this is useful. By way of example, this chapter focuses on topographic elevation and forest cover change in Colombia, but note that these are generic methods that can be applied in a wide variety of situations.\n\n#### Learning Outcomes {.unlisted .unnumbered}\n\n\n*   Understanding raster and vector data in Earth Engine and their differing properties.\n*   Knowing how and why to convert from raster to vector.\n*   Knowing how and why to convert from vector to raster.\n*   Write a function and map it over a FeatureCollection.\n\n#### Assumes you know how to:{.unlisted .unnumbered}\n\n\n*   Import images and image collections, filter, and visualize (Part F1).\n*   Understand distinctions among Image, ImageCollection, Feature and FeatureCollection Earth Engine objects (Part F1, Part F2, Part F5).\n*   Perform basic image analysis: select bands, compute indices, create masks (Part F2).\n*   Perform image morphological operations (Chap. F3.2).\n*   Understand the filter, map, reduce paradigm (Chap. F4.0).\n*   Write a function and map it over an ImageCollection (Chap. F4.0).\n*   Use reduceRegions to summarize an image in irregular shapes (Chap. F5.0).\n\n:::\n### Introduction {.unlisted .unnumbered}\n\n\nRaster data consists of regularly spaced pixels arranged into rows and columns, familiar as the format of satellite images. Vector data contains geometry features (i.e., points, lines, and polygons) describing locations and areas. Each data format has its advantages, and both will be encountered as part of GIS operations.\n\nRaster and vector data are commonly combined (e.g., extracting image information for a given location or clipping an image to an area of interest); however, there are also situations in which conversion between the two formats is useful. In making such conversions, it is important to consider the key advantages of each format. Rasters can store data efficiently where each pixel has a numerical value, while vector data can more effectively represent geometric features where homogenous areas have shared properties. Each format lends itself to distinctive analytical operations, and combining them can be powerful.\n\nIn this exercise, we’ll use topographic elevation and forest change images in Colombia as well as a protected area feature collection to practice the conversion between raster and vector formats, and to identify situations in which this is worthwhile.\n\n\n### Raster to Vector Conversion \n\n#### Raster to Polygons\n\nIn this section we will convert an elevation image (raster) to a feature collection (vector). We will start by loading the Global Multi-Resolution Terrain Elevation Data 2010 and the Global Administrative Unit Layers 2015 dataset to focus on Colombia. The elevation image is a raster at 7.5 arc-second spatial resolution containing a continuous measure of elevation in meters in each pixel.\n\n```js\n// Load raster (elevation) and vector (colombia) datasets.  \nvar elevation = ee.Image('USGS/GMTED2010').rename('elevation');  \nvar colombia = ee.FeatureCollection(       'FAO/GAUL_SIMPLIFIED_500m/2015/level0')  \n   .filter(ee.Filter.equals('ADM0_NAME', 'Colombia'));  \n  \n// Display elevation image.  \nMap.centerObject(colombia, 7);  \nMap.addLayer(elevation, {  \n   min: 0,  \n   max: 4000}, 'Elevation');\n\n```\nWhen converting an image to a feature collection, we will aggregate the categorical elevation values into a set of categories to create polygon shapes of connected pixels with similar elevations. For this exercise, we will create four zones of elevation by grouping the altitudes to 0-100 m = 0, 100–200 m = 1, 200–500 m = 2, and >500 m = 3.\n\n```js\n// Initialize image with zeros and define elevation zones.  \nvar zones = ee.Image(0)  \n   .where(elevation.gt(100), 1)  \n   .where(elevation.gt(200), 2)  \n   .where(elevation.gt(500), 3);  \n  \n// Mask pixels below sea level (<= 0 m) to retain only land areas.  \n// Name the band with values 0-3 as 'zone'.  \nzones = zones.updateMask(elevation.gt(0)).rename('zone');  \n  \nMap.addLayer(zones, {  \n   min: 0,  \n   max: 3,  \n   palette: ['white', 'yellow', 'lime', 'green'],  \n   opacity: 0.7}, 'Elevation zones');\n\n```\nWe will convert this zonal elevation image in Colombia to polygon shapes, which is a vector format (termed a FeatureCollection in Earth Engine), using the ee.Image.reduceToVectors method. This will create polygons delineating connected pixels with the same value. In doing so, we will use the same projection and spatial resolution as the image. Please note that loading the vectorized image in the native resolution (231.92 m) takes time to execute. For faster visualization, we set a coarse scale of 1,000 m.\n\n```js\nvar projection = elevation.projection();  \nvar scale = elevation.projection().nominalScale();  \n  \nvar elevationVector = zones.reduceToVectors({  \n   geometry: colombia.geometry(),  \n   crs: projection,  \n   scale: 1000, // scale   geometryType: 'polygon',  \n   eightConnected: false,  \n   labelProperty: 'zone',  \n   bestEffort: true,  \n   maxPixels: 1e13,  \n   tileScale: 3 // In case of error.  \n});  \n  \nprint(elevationVector.limit(10));  \n  \nvar elevationDrawn = elevationVector.draw({  \n   color: 'black',  \n   strokeWidth: 1  \n});  \nMap.addLayer(elevationDrawn, {}, 'Elevation zone polygon');\n```\n\n![](F5/image50.png)\n\n![](F5/image33.png)\n\n![](F5/image36.png)\n\n![Fig. F5.1.1 Raster-based elevation (top left) and zones (top right), vectorized elevation zones overlaid on the raster (bottom-left) and vectorized elevation zones only (bottom-right)](F5/image7.png)\n\n\nYou may have realized that polygons consist of complex lines, including some small polygons with just one pixel. That happens when there are no surrounding pixels of the same elevation zone. You may not need a vector map with such details—if, for instance, you want to produce a regional or global map. We can use a morphological reducer focalMode to simplify the shape by defining a neighborhood size around a pixel. In this example, we will set the kernel radius as four pixels. This operation makes the resulting polygons look much smoother, but less precise (Fig. F5.1.2).\n\n```js\nvar zonesSmooth = zones.focalMode(4, 'square');  \n  \nzonesSmooth = zonesSmooth.reproject(projection.atScale(scale));  \n  \nMap.addLayer(zonesSmooth, {  \n   min: 1,  \n   max: 3,  \n   palette: ['yellow', 'lime', 'green'],  \n   opacity: 0.7}, 'Elevation zones (smooth)');  \n  \nvar elevationVectorSmooth = zonesSmooth.reduceToVectors({  \n   geometry: colombia.geometry(),  \n   crs: projection,  \n   scale: scale,  \n   geometryType: 'polygon',  \n   eightConnected: false,  \n   labelProperty: 'zone',  \n   bestEffort: true,  \n   maxPixels: 1e13,  \n   tileScale: 3  \n});  \n  \nvar smoothDrawn = elevationVectorSmooth.draw({  \n   color: 'black',  \n   strokeWidth: 1  \n});  \nMap.addLayer(smoothDrawn, {}, 'Elevation zone polygon (smooth)');\n\n```\n\nWe can see now that the polygons have more distinct shapes with many fewer small polygons in the new map (Fig. F5.1.2). It is important to note that when you use methods like focalMode (or other, similar methods such as connectedComponents and connectedPixelCount), you need to reproject according to the original image in order to display properly with zoom using the interactive Code Editor.\n\n![](F5/image20.png)\n\n![Fig. F5.1.2 Before (left) and after (right) applying focalMode](F5/image37.png)\n\n\n#### Raster to Points\n\nLastly, we will convert a small part of this elevation image into a point vector dataset. For this exercise, we will use the same example and build on the code from the previous subsection. This might be useful when you want to use geospatial data in a tabular format in combination with other conventional datasets such as economic indicators (Fig. F5.1.3).\n\n![](F5/image24.png)\n\n![Fig. F5.1.3 Elevation point values with latitude and longitude](F5/image11.png)\n\n\nThe easiest way to do this is to use sample while activating the geometries parameter. This will extract the points at the centroid of the elevation pixel.\n\n```js\nvar geometry = ee.Geometry.Polygon([  \n   [-89.553, -0.929],  \n   [-89.436, -0.929],  \n   [-89.436, -0.866],  \n   [-89.553, -0.866],  \n   [-89.553, -0.929]  \n]);  \n  \n\n// To zoom into the area, un-comment and run below  \n// Map.centerObject(geometry,12);  \nMap.addLayer(geometry, {}, 'Areas to extract points');  \n  \nvar elevationSamples = elevation.sample({  \n   region: geometry,  \n   projection: projection,  \n   scale: scale,  \n   geometries: true,  \n});  \n  \nMap.addLayer(elevationSamples, {}, 'Points extracted');  \n  \n// Add three properties to the output table:  \n// 'Elevation', 'Longitude', and 'Latitude'.  \nelevationSamples = elevationSamples.map(function(feature) {   var geom = feature.geometry().coordinates();   return ee.Feature(null, {       'Elevation': ee.Number(feature.get(           'elevation')),       'Long': ee.Number(geom.get(0)),       'Lat': ee.Number(geom.get(1))  \n   });  \n});  \n  \n// Export as CSV.  \nExport.table.toDrive({  \n   collection: elevationSamples,  \n   description: 'extracted_points',  \n   fileFormat: 'CSV'  \n});\n\n```\n\nWe can also extract sample points per elevation zone. Below is an example of extracting 10 randomly selected points per elevation zone (Fig. F5.1.4). You can also set different values for each zone using classValues and classPoints parameters to modify the sampling intensity in each class. This may be useful, for instance, to generate point samples for a validation effort.\n\n```js\nvar elevationSamplesStratified = zones.stratifiedSample({  \n   numPoints: 10,  \n   classBand: 'zone',  \n   region: geometry,  \n   scale: scale,  \n   projection: projection,  \n   geometries: true  \n});  \n  \nMap.addLayer(elevationSamplesStratified, {}, 'Stratified samples');\n\n```\n\n![Fig. F5.1.4 Stratified sampling over different elevation zones](F5/image23.png)\n\n\n:::{.callout-note}\nCode Checkpoint F51a. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### 3. A More Complex Example\n\nIn this section we’ll use two global datasets, one to represent raster formats and the other vectors:\n\n*   The Global Forest Change (GFC) dataset: a raster dataset describing global tree cover and change for 2001–present.\n*   The World Protected Areas Database: a vector database of global protected areas.\n\nThe objective will be to combine these two datasets to quantify rates of deforestation in protected areas in the “arc of deforestation” of the Colombian Amazon. The datasets can be loaded into Earth Engine with the following code:\n\n```js\n// Read input data.  \n// Note: these datasets are periodically updated.  \n// Consider searching the Data Catalog for newer versions.  \nvar gfc = ee.Image('UMD/hansen/global_forest_change_2020_v1_8');  \nvar wdpa = ee.FeatureCollection('WCMC/WDPA/current/polygons');  \n  \n// Print assets to show available layers and properties.  \nprint(gfc);  \nprint(wdpa.limit(10)); // Show first 10 records.\n\nThe GFC dataset (first presented in detail in Chap. F1.1) is a global set of rasters that quantify tree cover and change for the period beginning in 2001. We’ll use a single image from this dataset:\n\n*   'lossyear': a categorical raster of forest loss (1–20, corresponding to deforestation for the period 2001–2020), and 0 for no change\n\nThe World Database on Protected Areas (WDPA) is a harmonized dataset of global terrestrial and marine protected area locations, along with details on the classification and management of each. In addition to protected area outlines, we’ll use two fields from this database:\n\n*   'NAME'’: the name of each protected area\n*   ‘WDPA_PID’: a unique numerical ID for each protected area\n\nTo begin with, we’ll focus on forest change dynamics in ‘La Paya’, a small protected area in the Colombian Amazon. We’ll first visualize these data using the paint command, which is discussed in more detail in Chap. F5.3:\n\n// Display deforestation.  \nvar deforestation = gfc.select('lossyear');  \n  \nMap.addLayer(deforestation, {  \n   min: 1,  \n   max: 20,  \n   palette: ['yellow', 'orange', 'red']  \n}, 'Deforestation raster');  \n  \n// Display WDPA data.  \nvar protectedArea = wdpa.filter(ee.Filter.equals('NAME', 'La Paya'));  \n  \n// Display protected area as an outline (see F5.3 for paint()).  \nvar protectedAreaOutline = ee.Image().byte().paint({  \n   featureCollection: protectedArea,  \n   color: 1,  \n   width: 3  \n});  \n  \nMap.addLayer(protectedAreaOutline, {  \n   palette: 'white'}, 'Protected area');  \n  \n// Set up map display.  \nMap.centerObject(protectedArea);  \nMap.setOptions('SATELLITE');\n\n```\nThis will display the boundary of the La Paya protected area and deforestation in the region (Fig. F5.1.5).\n\n![Fig. F5.1.5 View of the La Paya protected area in the Colombian Amazon (in white), and deforestation over the period 2001–2020 (in yellows and reds, with darker colors indicating more recent changes)](F5/image55.png)\n\n\nWe can use Earth Engine to convert the deforestation raster to a set of polygons. The deforestation data are appropriate for this transformation as each deforestation event is labeled categorically by year, and change events are spatially contiguous. This is performed in Earth Engine using the ee.Image.reduceToVectors method, as described earlier in this section.  \n\n```js\n// Convert from a deforestation raster to vector.  \nvar deforestationVector = deforestation.reduceToVectors({  \n   scale: deforestation.projection().nominalScale(),  \n   geometry: protectedArea.geometry(),  \n   labelProperty: 'lossyear', // Label polygons with a change year.   maxPixels: 1e13  \n});  \n  \n// Count the number of individual change events  \nprint('Number of change events:', deforestationVector.size());  \n  \n// Display deforestation polygons. Color outline by change year.  \nvar deforestationVectorOutline = ee.Image().byte().paint({  \n   featureCollection: deforestationVector,  \n   color: 'lossyear',  \n   width: 1  \n});  \n  \nMap.addLayer(deforestationVectorOutline, {  \n   palette: ['yellow', 'orange', 'red'],  \n   min: 1,  \n   max: 20}, 'Deforestation vector');\n\n```\nFig. F5.1.6 shows a comparison of the raster versus vector representations of deforestation within the protected area.\n\n![](F5/image42.png)\n\n![Fig. F5.1.6 Raster (left) versus vector (right) representations of deforestation data of the La Paya protected area](F5/image13.png)\n\n\nHaving converted from raster to vector, a new set of operations becomes available for post-processing the deforestation data. We might, for instance, be interested in the number of individual change events each year (Fig. F5.1.7):\n\n```js\nvar chart = ui.Chart.feature  \n   .histogram({  \n       features: deforestationVector,  \n       property: 'lossyear'   })  \n   .setOptions({  \n       hAxis: {  \n           title: 'Year'       },  \n       vAxis: {  \n           title: 'Number of deforestation events'       },  \n       legend: {  \n           position: 'none'       }  \n   });print(chart);\n```\n\n![Fig. F5.1.7 Plot of the number of deforestation events in La Paya for the years 2001–2020](F5/image15.png)\n\n\nThere might also be interest in generating point locations for individual change events (e.g., to aid a field campaign):\n\n```js\n// Generate deforestation point locations.  \nvar deforestationCentroids = deforestationVector.map(function(feat) {   return feat.centroid();  \n});  \n  \nMap.addLayer(deforestationCentroids, {  \n   color: 'darkblue'}, 'Deforestation centroids');\n\n```\nThe vector format allows for easy filtering to only deforestation events of interest, such as only the largest deforestation events:\n\n```js\n// Add a new property to the deforestation FeatureCollection  \n// describing the area of the change polygon.  \ndeforestationVector = deforestationVector.map(function(feat) {   return feat.set('area', feat.geometry().area({  \n       maxError: 10   }).divide(10000)); // Convert m^2 to hectare.  \n});  \n  \n// Filter the deforestation FeatureCollection for only large-scale (>10 ha) changes  \nvar deforestationLarge = deforestationVector.filter(ee.Filter.gt(   'area', 10));  \n  \n// Display deforestation area outline by year.  \nvar deforestationLargeOutline = ee.Image().byte().paint({  \n   featureCollection: deforestationLarge,  \n   color: 'lossyear',  \n   width: 1  \n});  \n  \nMap.addLayer(deforestationLargeOutline, {  \n   palette: ['yellow', 'orange', 'red'],  \n   min: 1,  \n   max: 20}, 'Deforestation (>10 ha)');\n\n```\n:::{.callout-note}\nCode Checkpoint F51b. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n#### Raster Properties to Vector Fields\n\nSometimes we want to extract information from a raster to be included in an existing vector dataset. An example might be estimating a deforestation rate for a set of protected areas. Rather than perform this task on a case-by-case basis, we can attach information generated from an image as a property of a feature.\n\nThe following script shows how this can be used to quantify a deforestation rate for a set of protected areas in the Colombian Amazon.\n\n```js\n// Load required datasets.  \nvar gfc = ee.Image('UMD/hansen/global_forest_change_2020_v1_8');  \nvar wdpa = ee.FeatureCollection('WCMC/WDPA/current/polygons');  \n  \n// Display deforestation.  \nvar deforestation = gfc.select('lossyear');  \n  \nMap.addLayer(deforestation, {  \n   min: 1,  \n   max: 20,  \n   palette: ['yellow', 'orange', 'red']  \n}, 'Deforestation raster');  \n  \n// Select protected areas in the Colombian Amazon.  \nvar amazonianProtectedAreas = [   'Cordillera de los Picachos', 'La Paya', 'Nukak',   'Serrania de Chiribiquete',   'Sierra de la Macarena', 'Tinigua'  \n];  \n  \nvar wdpaSubset = wdpa.filter(ee.Filter.inList('NAME',  \n   amazonianProtectedAreas));  \n  \n// Display protected areas as an outline.  \nvar protectedAreasOutline = ee.Image().byte().paint({  \n   featureCollection: wdpaSubset,  \n   color: 1,  \n   width: 1  \n});  \n  \nMap.addLayer(protectedAreasOutline, {  \n   palette: 'white'}, 'Amazonian protected areas');  \n  \n// Set up map display.  \nMap.centerObject(wdpaSubset);  \nMap.setOptions('SATELLITE');  \n  \nvar scale = deforestation.projection().nominalScale();  \n  \n// Use 'reduceRegions' to sum together pixel areas in each protected area.  \nwdpaSubset = deforestation.gte(1)  \n   .multiply(ee.Image.pixelArea().divide(10000)).reduceRegions({  \n       collection: wdpaSubset,  \n       reducer: ee.Reducer.sum().setOutputs([           'deforestation_area']),  \n       scale: scale  \n   });  \n  \nprint(wdpaSubset); // Note the new 'deforestation_area' property.\n```\nThe output of this script is an estimate of deforested area in hectares for each reserve. However, as reserve sizes vary substantially by area, we can normalize by the total area of each reserve to quantify rates of change.\n\n```js\n// Normalize by area.  \nwdpaSubset = wdpaSubset.map(   function(feat) {       return feat.set('deforestation_rate',           ee.Number(feat.get('deforestation_area'))  \n           .divide(feat.area().divide(10000)) // m2 to ha           .divide(20) // number of years           .multiply(100)); // to percentage points   });// Print to identify rates of change per protected area.  \n// Which has the fastest rate of loss?  \nprint(wdpaSubset.reduceColumns({  \n   reducer: ee.Reducer.toList().repeat(2),  \n   selectors: ['NAME', 'deforestation_rate']  \n}));\n\n```\n\n:::{.callout-note}\nCode Checkpoint F51c. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Vector-to-Raster Conversion\n\nIn Sect. 1, we used the protected area feature collection as its original vector format. In this section, we will rasterize the protected area polygons to produce a mask and use this to assess rates of forest change.\n\n#### Polygons to a Mask\n\nThe most common operation to convert from vector to raster is the production of binary image masks, describing whether a pixel intersects a line or falls within a polygon. To convert from vector to a raster mask, we can use the ee.FeatureCollection.reduceToImage method. Let’s continue with our example of the WDPA database and Global Forest Change data from the previous section:\n\n```js\n// Load required datasets.  \nvar gfc = ee.Image('UMD/hansen/global_forest_change_2020_v1_8');  \nvar wdpa = ee.FeatureCollection('WCMC/WDPA/current/polygons');  \n  \n// Get deforestation.  \nvar deforestation = gfc.select('lossyear');  \n  \n// Generate a new property called 'protected' to apply to the output mask.  \nvar wdpa = wdpa.map(function(feat) {   return feat.set('protected', 1);  \n});  \n  \n// Rasterize using the new property.  \n// unmask() sets areas outside protected area polygons to 0.  \nvar wdpaMask = wdpa.reduceToImage(['protected'], ee.Reducer.first())  \n   .unmask();  \n  \n// Center on Colombia.  \nMap.setCenter(-75, 3, 6);  \n  \n// Display on map.  \nMap.addLayer(wdpaMask, {  \n   min: 0,  \n   max: 1}, 'Protected areas (mask)');\n\n```\nWe can use this mask to, for example, highlight only deforestation that occurs within a protected area using logical operations:\n\n```js\n// Set the deforestation layer to 0 where outside a protected area.  \nvar deforestationProtected = deforestation.where(wdpaMask.eq(0), 0);  \n  \n// Update mask to hide where deforestation layer = 0  \nvar deforestationProtected = deforestationProtected  \n   .updateMask(deforestationProtected.gt(0));  \n  \n// Display deforestation in protected areas  \nMap.addLayer(deforestationProtected, {  \n   min: 1,  \n   max: 20,  \n   palette: ['yellow', 'orange', 'red']  \n}, 'Deforestation protected');\n\n```\nIn the above example we generated a simple binary mask, but reduceToImage can also preserve a numerical property of the input polygons. For example, we might want to be able to determine which protected area each pixel represents. In this case, we can produce an image with the unique ID of each protected area:\n\n```js\n// Produce an image with unique ID of protected areas.  \nvar wdpaId = wdpa.reduceToImage(['WDPAID'], ee.Reducer.first());  \n  \nMap.addLayer(wdpaId, {  \n   min: 1,  \n   max: 100000}, 'Protected area ID');\n\n```\nThis output can be useful when performing large-scale raster operations, such as efficiently calculating deforestation rates for multiple protected areas.\n\n:::{.callout-note}\nCode Checkpoint F51d. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n#### A More Complex Example\n\nThe reduceToImage method is not the only way to convert a feature collection to an image. We will create a distance image layer from the boundary of the protected area using distance. For this example, we return to the La Paya protected area explored in Sect. 1.\n\n```js\n// Load required datasets.  \nvar gfc = ee.Image('UMD/hansen/global_forest_change_2020_v1_8');  \nvar wdpa = ee.FeatureCollection('WCMC/WDPA/current/polygons');  \n  \n// Select a single protected area.  \nvar protectedArea = wdpa.filter(ee.Filter.equals('NAME', 'La Paya'));  \n  \n// Maximum distance in meters is set in the brackets.  \nvar distance = protectedArea.distance(1000000);  \n  \nMap.addLayer(distance, {  \n   min: 0,  \n   max: 20000,  \n   palette: ['white', 'grey', 'black'],  \n   opacity: 0.6}, 'Distance');  \n  \nMap.centerObject(protectedArea);\n\n```\nWe can also show the distance inside and outside of the boundary by using the rasterized protected area (Fig. F5.1.8).\n\n```js\n// Produce a raster of inside/outside the protected area.  \nvar protectedAreaRaster = protectedArea.map(function(feat) {   return feat.set('protected', 1);  \n}).reduceToImage(['protected'], ee.Reducer.first());  \n  \nMap.addLayer(distance.updateMask(protectedAreaRaster), {  \n   min: 0,  \n   max: 20000}, 'Distance inside protected area');  \n  \nMap.addLayer(distance.updateMask(protectedAreaRaster.unmask()  \n.not()), {  \n   min: 0,  \n   max: 20000}, 'Distance outside protected area');\n\n```\n![](F5/image56.png)\n\n![](F5/image9.png)\n\n![Fig. F5.1.8 Distance from the La Paya boundary (left), distance within the La Paya (middle), and distance outside the La Paya (right)](F5/image25.png)\n\n\nSometimes it makes sense to work with objects in raster imagery. This is an unusual case of vector-like operations conducted with raster data. There is a good reason for this where the vector equivalent would be computationally burdensome.\n\nAn example of this is estimating deforestation rates by distance to the edge of the protected area, as it is common that rates of change will be higher at the boundary of a protected area. We will create a distance raster with three zones from the La Paya boundary (>1 km, >2 km, >3 km, and >4 km) and to estimate the deforestation by distance from the boundary (Fig. F5.1.9).\n\n```js\nvar distanceZones = ee.Image(0)  \n   .where(distance.gt(0), 1)  \n   .where(distance.gt(1000), 2)  \n   .where(distance.gt(3000), 3)  \n   .updateMask(distance.lte(5000));  \n  \nMap.addLayer(distanceZones, {}, 'Distance zones');  \n  \nvar deforestation = gfc.select('loss');  \nvar deforestation1km = deforestation.updateMask(distanceZones.eq(1));  \nvar deforestation3km = deforestation.updateMask(distanceZones.lte(2));  \nvar deforestation5km = deforestation.updateMask(distanceZones.lte(3));  \n  \nMap.addLayer(deforestation1km, {  \n   min: 0,  \n   max: 1}, 'Deforestation within a 1km buffer');  \nMap.addLayer(deforestation3km, {  \n   min: 0,  \n   max: 1,  \n   opacity: 0.5}, 'Deforestation within a 3km buffer');  \nMap.addLayer(deforestation5km, {  \n   min: 0,  \n   max: 1,  \n   opacity: 0.5}, 'Deforestation within a 5km buffer');\n\n```\n\n![](F5/image22.png)\n\n![](F5/image6.png)\n\n![](F5/image21.png)\n\n![Fig. F5.1.9 Distance zones (top left) and deforestation by zone (<1 km, <3 km, and <5 km)](F5/image26.png)\n\n\nLastly, we can estimate the deforestation area within 1 km of the protected area but only outside of the boundary.\n\n```js\n\nvar deforestation1kmOutside = deforestation1km  \n   .updateMask(protectedAreaRaster.unmask().not());  \n  \n// Get the value of each pixel in square meters  \n// and divide by 10000 to convert to hectares.  \nvar deforestation1kmOutsideArea = deforestation1kmOutside.eq(1)  \n   .multiply(ee.Image.pixelArea()).divide(10000);  \n  \n// We need to set a larger geometry than the protected area  \n// for the geometry parameter in reduceRegion().  \nvar deforestationEstimate = deforestation1kmOutsideArea  \n   .reduceRegion({  \n       reducer: ee.Reducer.sum(),  \n       geometry: protectedArea.geometry().buffer(1000),  \n       scale: deforestation.projection().nominalScale()  \n   });  \n  \nprint('Deforestation within a 1km buffer outside the protected area (ha)',  \n   deforestationEstimate);\n\n```\n:::{.callout-note}\nCode Checkpoint F51e. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Conclusion {.unnumbered}\n\nIn this chapter, you learned how to convert raster to vector and vice versa. More importantly, you now have a better understanding of why and when such conversions are useful. Our examples should give you practical applications and ideas for using these techniques.\n\n\n\n## Zonal Statistics\n\n:::{.callout-tip}\n## Chapter Information\n\n#### Author {.unlisted .unnumbered}\n\n \n\nSara Winsemius and Justin Braaten\n\n\n\n#### Overview {.unlisted .unnumbered}\n\n\nThe purpose of this chapter is to extract values from rasters for intersecting points or polygons. We will lay out the process and a function to calculate zonal statistics, which includes optional parameters to modify the function, and then apply the process to three examples using different raster datasets and combinations of parameters.\n\n#### Learning Outcomes {.unlisted .unnumbered}\n \n\n*   Buffering points as square or circular regions.\n*   Writing and applying functions with optional parameters.\n*   Learning what zonal statistics are and how to use reducers.\n*   Exporting computation results to a table.\n*   Copying properties from one image to another.\n\n#### Assumes you know how to:{.unlisted .unnumbered}\n\n\n*   Recognize similarities and differences among Landsat 5, 7, and 8 spectral bands (Part F1, Part F2, Part F3).\n*   Understand distinctions among Image, ImageCollection, Feature and FeatureCollection Earth Engine objects (Part F1, Part F2, Part F5).\n*   Use drawing tools to create points, lines, and polygons (Chap. F2.1).\n*   Write a function and map it over an ImageCollection (Chap. F4.0).\n*   Mask cloud, cloud shadow, snow/ice, and other undesired pixels (Chap. F4.3).\n*   Export calculated data to tables with Tasks (Chap. F5.0).\n*   Understand the differences between raster and vector data (Chap. F5.0, Chap. F5.1).\n*   Write a function and map it over a FeatureCollection (Chap. F5.1).\n:::\n\n### Introduction {.unlisted .unnumbered}\n\n\nAnyone working with field data collected at plots will likely need to summarize raster-based data associated with those plots. For instance, they need to know the Normalized Difference Vegetation Index (NDVI), precipitation, or elevation for each plot (or surrounding region). Calculating statistics from a raster within given regions is called zonal statistics. Zonal statistics were calculated in Chaps. F5.0 and F5.1 using ee.Image.ReduceRegions. Here, we present a more general approach to calculating zonal statistics with a custom function that works for both ee.Image and ee.ImageCollection objects. In addition to its flexibility, the reduction method used here is less prone to “Computed value is too large” errors that can occur when using ReduceRegions with very large or complex ee.FeatureCollection object inputs.\n\nThe zonal statistics function in this chapter works for an Image or an ImageCollection. Running the function over an ImageCollection will produce a table with values from each image in the collection per point. Image collections can be processed before extraction as needed—for example, by masking clouds from satellite imagery or by constraining the dates needed for a particular research question. In this tutorial, the data extracted from rasters are exported to a table for analysis, where each row of the table corresponds to a unique point-image combination.\n\nIn fieldwork, researchers often work with plots, which are commonly recorded as polygon files or as a center point with a set radius. It is rare that plots will be set directly in the center of pixels from your desired raster dataset, and many field GPS units have positioning errors. Because of these issues, it may be important to use a statistic of adjacent pixels (as described in Chap. F3.2) to estimate the central value in what’s often called a neighborhood mean or focal mean (Cansler and McKenzie 2012, Miller and Thode 2007).\n\nTo choose the size of your neighborhood, you will need to consider your research questions, the spatial resolution of the dataset, the size of your field plot, and the error from your GPS. For example, the raster value extracted for randomly placed 20 m diameter plots would likely merit use of a neighborhood mean when using Sentinel-2 or Landsat 8—at 10 m and 30 m spatial resolution, respectively—while using a thermal band from MODIS (Moderate Resolution Imaging Spectroradiometer) at 1000 m may not. While much of this tutorial is written with plot points and buffers in mind, a polygon asset with predefined regions will serve the same purpose.\n\n\n### Functions\n\nTwo functions are provided; copy and paste them into your script:\n\n*   A function to generate circular or square regions from buffered points\n*   A function to extract image pixel neighborhood statistics for a given region\n\n#### Function: bufferPoints(radius, bounds)\n\nOur first function, bufferPoints, returns a function for adding a buffer to points and optionally transforming to rectangular bounds \n   \n```js\nfunction bufferPoints(radius, bounds) {\n    return function(pt) {\n        pt = ee.Feature(pt);\n        return bounds ? pt.buffer(radius).bounds() : pt.buffer(\n            radius);\n    };\n}\n```\n\n#### Function: zonalStats(fc, params)\n\nThe second function, zonalStats, reduces images in an ImageCollection by regions defined in a FeatureCollection. Note that reductions can return null statistics that you might want to filter out of the resulting feature collection. Null statistics occur when there are no valid pixels intersecting the region being reduced. This situation can be caused by points that are outside of an image or in regions that are masked for quality or clouds.\n\nThis function is written to include many optional parameters (see Table F5.2.2). Look at the function carefully and note how it is written to include defaults that make it easy to apply the basic function while allowing customization.\n\nThe desired datetime format. Use ISO 8601 data string standards. The datetime string is derived from the 'system:time_start' value of the ee.Image being reduced. Optional.\n\n```js\nfunction zonalStats(ic, fc, params) {\n    // Initialize internal params dictionary.\n    var _params = {\n        reducer: ee.Reducer.mean(),\n        scale: null,\n        crs: null,\n        bands: null,\n        bandsRename: null,\n        imgProps: null,\n        imgPropsRename: null,\n        datetimeName: 'datetime',\n        datetimeFormat: 'YYYY-MM-dd HH:mm:ss'\n    };\n\n    // Replace initialized params with provided params.\n    if (params) {\n        for (var param in params) {\n            _params[param] = params[param] || _params[param];\n        }\n    }\n\n    // Set default parameters based on an image representative.\n    var imgRep = ic.first();\n    var nonSystemImgProps = ee.Feature(null)\n        .copyProperties(imgRep).propertyNames();\n    if (!_params.bands) _params.bands = imgRep.bandNames();\n    if (!_params.bandsRename) _params.bandsRename = _params.bands;\n    if (!_params.imgProps) _params.imgProps = nonSystemImgProps;\n    if (!_params.imgPropsRename) _params.imgPropsRename = _params\n        .imgProps;\n\n    // Map the reduceRegions function over the image collection.\n    var results = ic.map(function(img) {\n        // Select bands (optionally rename), set a datetime & timestamp property.\n        img = ee.Image(img.select(_params.bands, _params\n                .bandsRename))\n            // Add datetime and timestamp features.\n            .set(_params.datetimeName, img.date().format(\n                _params.datetimeFormat))\n            .set('timestamp', img.get('system:time_start'));\n\n        // Define final image property dictionary to set in output features.\n        var propsFrom = ee.List(_params.imgProps)\n            .cat(ee.List([_params.datetimeName,\n            'timestamp']));\n        var propsTo = ee.List(_params.imgPropsRename)\n            .cat(ee.List([_params.datetimeName,\n            'timestamp']));\n        var imgProps = img.toDictionary(propsFrom).rename(\n            propsFrom, propsTo);\n\n        // Subset points that intersect the given image.\n        var fcSub = fc.filterBounds(img.geometry());\n\n        // Reduce the image by regions.\n        return img.reduceRegions({\n                collection: fcSub,\n                reducer: _params.reducer,\n                scale: _params.scale,\n                crs: _params.crs\n            })\n            // Add metadata to each feature.\n            .map(function(f) {\n                return f.set(imgProps);\n            });\n\n        // Converts the feature collection of feature collections to a single\n        //feature collection.\n    }).flatten();\n\n    return results;\n}\n```\n\n\n### Point Collection Creation\n\nBelow, we create a set of points that form the basis of the zonal statistics calculations. Note that a unique plot_id property is added to each point. A unique plot or point ID is important to include in your vector dataset for future filtering and joining.\n\n```js\nvar pts = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([-118.6010, 37.0777]), {\n        plot_id: 1\n    }),\n    ee.Feature(ee.Geometry.Point([-118.5896, 37.0778]), {\n        plot_id: 2\n    }),\n    ee.Feature(ee.Geometry.Point([-118.5842, 37.0805]), {\n        plot_id: 3\n    }),\n    ee.Feature(ee.Geometry.Point([-118.5994, 37.0936]), {\n        plot_id: 4\n    }),\n    ee.Feature(ee.Geometry.Point([-118.5861, 37.0567]), {\n        plot_id: 5\n    })\n]);\n\nprint('Points of interest', pts);\n```\n\n:::{.callout-note}\nCode Checkpoint F52a. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Neighborhood Statistic Examples\n\nThe following examples demonstrate extracting raster neighborhood statistics for the following:\n\n*   A single raster with elevation and slope bands\n*   A multiband MODIS time series\n*   A multiband Landsat time series\n\nIn each example, the points created in the previous section will be buffered and then used as regions to extract zonal statistics for each image in the image collection.\n\n#### Topographic Variables\n\nThis example demonstrates how to calculate zonal statistics for a single multiband image. This Digital Elevation Model (DEM) contains a single topographic band representing elevation.\n\n####Buffer the Points\n\nNex, we will apply a 45 m radius buffer to the points defined previously by mapping the bufferPoints function over the feature collection. The radius is set to 45 m to correspond to the 90 m pixel resolution of the DEM. In this case, circles are used instead of squares (set the second argument as false, i.e., do not use bounds).\n\n```js\n// Buffer the points.  \nvar ptsTopo = pts.map(bufferPoints(45, false));\n\n```\n####Calculate Zonal Statistics\n\nThere are two important things to note about the zonalStats function that this example addresses:\n\n*   It accepts only an ee.ImageCollection, not an ee.Image; single images must be wrapped in an ImageCollection.\n*   It expects every image in the input image collection to have a timestamp property named 'system:time_start' with values representing milliseconds from 00:00:00 UTC on 1 January 1970. Most datasets should have this property, if not, one should be added.\n\n```js\n// Import the MERIT global elevation dataset.  \nvar elev = ee.Image('MERIT/DEM/v1_0_3');  \n  \n// Calculate slope from the DEM.  \nvar slope = ee.Terrain.slope(elev);  \n  \n// Concatenate elevation and slope as two bands of an image.  \nvar topo = ee.Image.cat(elev, slope)  \n    // Computed images do not have a 'system:time_start' property; add one based  \n    // on when the data were collected.   .set('system:time_start', ee.Date('2000-01-01').millis());  \n  \n// Wrap the single image in an ImageCollection for use in the  \n// zonalStats function.  \nvar topoCol = ee.ImageCollection([topo]);\n\n```\nDefine arguments for the zonalStats function and then run it. Note that we are accepting defaults for the reducer, scale, Coordinate Reference System (CRS), and image properties to copy over to the resulting feature collection. Refer to the function definition above for defaults.\n\n```js\n// Define parameters for the zonalStats function.  \nvar params = {  \n   bands: [0, 1],  \n   bandsRename: ['elevation', 'slope']  \n};  \n  \n// Extract zonal statistics per point per image.  \nvar ptsTopoStats = zonalStats(topoCol, ptsTopo, params);print('Topo zonal stats table', ptsTopoStats);  \n  \n// Display the layers on the map.  \nMap.setCenter(-118.5957, 37.0775, 13);  \nMap.addLayer(topoCol.select(0), {  \n   min: 2400,  \n   max: 4200}, 'Elevation');  \nMap.addLayer(topoCol.select(1), {  \n   min: 0,  \n   max: 60}, 'Slope');  \nMap.addLayer(pts, {  \n   color: 'purple'}, 'Points');  \nMap.addLayer(ptsTopo, {  \n   color: 'yellow'}, 'Points w/ buffer');\n\n```\nThe result is a copy of the buffered point feature collection with new properties added for the region reduction of each selected image band according to the given reducer. A part of the FeatureCollection is shown in Fig. F5.2.1. The data in that FeatureCollection corresponds to a table containing the information of Table F5.2.3. See Fig. F5.2.2 for a graphical representation of the points and the topographic data being summarized.\n\n![Fig. F5.2.1 A part of the FeatureCollection produced by calculating the zonal statistics](F5/image29.png)\n\n\n![Fig. F5.2.2 Sample points and topographic slope. Elevation and slope values for regions intersecting each buffered point are reduced and attached as properties of the points.](F5/image5.png)\n\n\nTable F5.2.3 Example output from zonalStats organized as a table. Rows correspond to collection features and columns are feature properties. Note that elevation and slope values in this table are rounded to the nearest tenth for brevity.\n\n| plot_id | timestamp    | datetime            | elevation | slope |\n|---------|--------------|---------------------|-----------|-------|\n| 1       | 946684800000 | 2000-01-01 00:00:00 | 2648.1    | 29.7  |\n| 2       | 946684800000 | 2000-01-01 00:00:00 | 2888.2    | 33.9  |\n| 3       | 946684800000 | 2000-01-01 00:00:00 | 3267.8    | 35.8  |\n| 4       | 946684800000 | 2000-01-01 00:00:00 | 2790.7    | 25.1  |\n| 5       | 946684800000 | 2000-01-01 00:00:00 | 2559.4    | 29.4  |\n\n#### MODIS Time Series\n\nA time series of MODIS eight-day surface reflectance composites demonstrates how to calculate zonal statistics for a multiband ImageCollection that requires no preprocessing, such as cloud masking or computation. Note that there is no built-in function for performing region reductions on ImageCollection objects. The zonalStats function that we are using for reduction is mapping the reduceRegions function over an ImageCollection.\n\n#### Buffer the Points\n\nIn this example, suppose the point collection represents center points for field plots that are 100 m x 100 m, and apply a 50 m radius buffer to the points to match the size of the plot. Since we want zonal statistics for square plots, set the second argument of the bufferPoints function to true, so that the bounds of the buffered points are returned.\n\nvar ptsModis = pts.map(bufferPoints(50, true));\n\n#### Calculate Zonal Statistic\n\nImport the MODIS 500 m global eight-day surface reflectance composite collection and filter the collection to include data for July, August, and September from 2015 through 2019.\n\n```js\nvar modisCol = ee.ImageCollection('MODIS/006/MOD09A1')  \n   .filterDate('2015-01-01', '2020-01-01')  \n   .filter(ee.Filter.calendarRange(183, 245, 'DAY_OF_YEAR'));\n```\n\nReduce each image in the collection by each plot according to the following parameters. Note that this time the reducer is defined as the neighborhood median (ee.Reducer.median) instead of the default mean, and that scale, CRS, and properties for the datetime are explicitly defined.\n\n```js\n// Define parameters for the zonalStats function.  \nvar params = {  \n   reducer: ee.Reducer.median(),  \n   scale: 500,  \n   crs: 'EPSG:5070',  \n   bands: ['sur_refl_b01', 'sur_refl_b02', 'sur_refl_b06'],  \n   bandsRename: ['modis_red', 'modis_nir', 'modis_swir'],  \n   datetimeName: 'date',  \n   datetimeFormat: 'YYYY-MM-dd'  \n};  \n  \n// Extract zonal statistics per point per image.  \nvar ptsModisStats = zonalStats(modisCol, ptsModis, params);print('Limited MODIS zonal stats table', ptsModisStats.limit(50));\n\n```\nThe result is a feature collection with a feature for all combinations of plots and images. Interpreted as a table, the result has 200 rows (5 plots times 40 images) and as many columns as there are feature properties. Feature properties include those from the plot asset and the image, and any associated non-system image properties. Note that the printed results are limited to the first 50 features for brevity.\n\n#### Landsat Time Series\n\nThis example combines Landsat surface reflectance imagery across three instruments: Thematic Mapper (TM) from Landsat 5, Enhanced Thematic Mapper Plus (ETM+) from Landsat 7, and Operational Land Imager (OLI) from Landsat 8.\n\nThe following section prepares these collections so that band names are consistent and cloud masks are applied. Reflectance among corresponding bands are roughly congruent for the three sensors when using the surface reflectance product; therefore the processing steps that follow do not address inter-sensor harmonization. Review the current literature on inter-sensor harmonization practices if you'd like to apply a correction.\n\n#### Prepare the Landsat Image Collection\n\nFirst, define the function to mask cloud and shadow pixels (See Chap. F4.3 for more detail on cloud masking).\n\n```js\n// Mask clouds from images and apply scaling factors.\nfunction maskScale(img) {\n    var qaMask = img.select('QA_PIXEL').bitwiseAnd(parseInt('11111',\n        2)).eq(0);\n    var saturationMask = img.select('QA_RADSAT').eq(0);\n\n    // Apply the scaling factors to the appropriate bands.\n    var getFactorImg = function(factorNames) {\n        var factorList = img.toDictionary().select(factorNames)\n            .values();\n        return ee.Image.constant(factorList);\n    };\n    var scaleImg = getFactorImg(['REFLECTANCE_MULT_BAND_.']);\n    var offsetImg = getFactorImg(['REFLECTANCE_ADD_BAND_.']);\n    var scaled = img.select('SR_B.').multiply(scaleImg).add(\n    offsetImg);\n\n    // Replace the original bands with the scaled ones and apply the masks.\n    return img.addBands(scaled, null, true)\n        .updateMask(qaMask)\n        .updateMask(saturationMask);\n}\n```\n\nNext, define functions to select and rename the bands of interest for the Operational Land Imager (OLI) aboard Landsat 8, and for the TM/ETM+ imagers aboard earlier Landsats. This is important because the band numbers are different for OLI and TM/ETM+, and it will make future index calculations easier.\n\n```js\n// Selects and renames bands of interest for Landsat OLI.  \nfunction renameOli(img) {   return img.select(  \n       ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'],  \n       ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']);  \n}  \n  \n// Selects and renames bands of interest for TM/ETM+.  \nfunction renameEtm(img) {   return img.select(  \n       ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7'],  \n       ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']);  \n}\n```\nCombine the cloud mask and band renaming functions into preparation functions for OLI and TM/ETM+. Add any other sensor-specific preprocessing steps that you’d like to the functions below.\n\n```js\n// Prepares (cloud masks and renames) OLI images.  \nfunction prepOli(img) {  \n   img = maskScale(img);  \n   img = renameOli(img);   return img;  \n}// Prepares (cloud masks and renames) TM/ETM+ images.  \nfunction prepEtm(img) {  \n   img = maskScale(img);  \n   img = renameEtm(img);   return img;  \n}\n```\nGet the Landsat surface reflectance collections for OLI, ETM+, and TM sensors. Filter them by the bounds of the point feature collection and apply the relevant image preparation function.\n\n```js\nvar ptsLandsat = pts.map(bufferPoints(15, true));  \n  \nvar oliCol = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')  \n   .filterBounds(ptsLandsat)  \n   .map(prepOli);  \n  \nvar etmCol = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2')  \n   .filterBounds(ptsLandsat)  \n   .map(prepEtm);  \n  \nvar tmCol = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')  \n   .filterBounds(ptsLandsat)  \n   .map(prepEtm);\n```\nMerge the prepared sensor collections.\n\n```js\nvar landsatCol = oliCol.merge(etmCol).merge(tmCol);\n```\n\n#### Calculate Zonal Statistics\n\nReduce each image in the collection by each plot according to the following parameters. Note that this example defines the imgProps and imgPropsRename parameters to copy over and rename just two selected image properties: Landsat image ID and the satellite that collected the data. It also uses the max reducer, which, as an unweighted reducer, will return the maximum value from pixels that have their centroid within the buffer (see Sect. 4.1 below for more details).\n\n```js\n// Define parameters for the zonalStats function.  \nvar params = {  \n   reducer: ee.Reducer.max(),  \n   scale: 30,  \n   crs: 'EPSG:5070',  \n   bands: ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2'],  \n   bandsRename: ['ls_blue', 'ls_green', 'ls_red', 'ls_nir',       'ls_swir1', 'ls_swir2'   ],  \n   imgProps: ['SENSOR_ID', 'SPACECRAFT_ID'],  \n   imgPropsRename: ['img_id', 'satellite'],  \n   datetimeName: 'date',  \n   datetimeFormat: 'YYYY-MM-dd'  \n};  \n  \n// Extract zonal statistics per point per image.  \nvar ptsLandsatStats = zonalStats(landsatCol, ptsLandsat, params)   // Filter out observations where image pixels were all masked.   .filter(ee.Filter.notNull(params.bandsRename));  \nprint('Limited Landsat zonal stats table', ptsLandsatStats.limit(50));\n\n```\nThe result is a feature collection with a feature for all combinations of plots and images.\n\n#### Dealing with Large Collections\n\nIf your browser times out, try exporting the results (as described in Chap. F6.2). It’s likely that point feature collections that cover a large area or contain many points (point-image observations) will need to be exported as a batch task by either exporting the final feature collection as an asset or as a CSV/shapefile/GeoJSON to Google Drive or GCS.\n\nHere is how you would export the above Landsat image-point feature collection to an asset and to Google Drive. Run the following code, activate the Code Editor Tasks tab, and then click the Run button. If you don’t specify your own existing folder in Drive, the folder “EEFA_outputs” will be created.\n\n```js\nExport.table.toAsset({  \n   collection: ptsLandsatStats,  \n   description: 'EEFA_export_Landsat_to_points',  \n   assetId: 'EEFA_export_values_to_points'  \n});  \n  \nExport.table.toDrive({  \n   collection: ptsLandsatStats,  \n   folder: 'EEFA_outputs', // this will create a new folder if it doesn't exist   description: 'EEFA_export_values_to_points',  \n   fileFormat: 'CSV'  \n});\n```\n\n:::{.callout-note}\nCode Checkpoint F52b. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Additional Notes\n\n#### Weighted Versus Unweighted Region Reduction\n\nA region used for calculation of zonal statistics often bisects multiple pixels. Should partial pixels be included in zonal statistics? Earth Engine lets you decide by allowing you to define a reducer as either weighted or unweighted (or you can provide per-pixel weight specification as an image band). A weighted reducer will include partial pixels in the zonal statistic calculation by weighting each pixel's contribution according to the fraction of the area intersecting the region. An unweighted reducer, on the other hand, gives equal weight to all pixels whose cell center intersects the region; all other pixels are excluded from calculation of the statistic.\n\nFor aggregate reducers like ee.Reducer.mean and ee.Reducer.median, the default mode is weighted, while identifier reducers such as ee.Reducer.min and ee.Reducer.max are unweighted. You can adjust the behavior of weighted reducers by calling unweighted on them, as in ee.Reducer.mean.unweighted. You may also specify the weights by modifying the reducer with splitWeights; however, that is beyond the scope of this book.\n\n#### Copy Properties to Computed Images\n\nDerived, computed images do not retain the properties of their source image, so be sure to copy properties to computed images if you want them included in the region reduction table. For instance, consider the simple computation of unscaling Landsat SR data:\n\n```js\n// Define a Landsat image.  \nvar img = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').first();  \n  \n// Print its properties.  \nprint('All image properties', img.propertyNames());  \n  \n// Subset the reflectance bands and unscale them.  \nvar computedImg = img.select('SR_B.').multiply(0.0000275).add(-0.2);  \n  \n// Print the unscaled image's properties.  \nprint('Lost original image properties', computedImg.propertyNames());\n\n```\nNotice how the computed image does not have the source image's properties and only retains the bands information. To fix this, use the copyProperties function to add desired source properties to the derived image. It is best practice to copy only the properties you really need because some properties, such as those containing geometry objects, lists, or feature collections, can significantly increase the computational burden for large collections.\n\n```js\n// Subset the reflectance bands and unscale them, keeping selected  \n// source properties.  \nvar computedImg = img.select('SR_B.').multiply(0.0000275).add(-0.2)  \n   .copyProperties(img, ['system:time_start', 'LANDSAT_PRODUCT_ID']);  \n  \n// Print the unscaled image's properties.  \nprint('Selected image properties retained', computedImg  \n.propertyNames());\n\n```\nNow selected properties are included. Use this technique when returning computed, derived images in a mapped function, and in single-image operations.\n\n#### Understanding Which Pixels are Included in Polygon Statistics\n\nIf you want to visualize what pixels are included in a polygon for a region reducer, you can adapt the following code to use your own region (by replacing geometry), dataset, desired scale, and CRS parameters. The important part to note is that the image data you are adding to the map is reprojected using the same scale and CRS as that used in your region reduction (see Fig. F5.2.3).\n\n```js\n// Define polygon geometry.  \nvar geometry = ee.Geometry.Polygon(  \n   [  \n       [  \n           [-118.6019835717645, 37.079867782687884],  \n           [-118.6019835717645, 37.07838698844939],  \n           [-118.60036351751951, 37.07838698844939],  \n           [-118.60036351751951, 37.079867782687884]  \n       ]  \n   ], null, false);  \n  \n// Import the MERIT global elevation dataset.  \nvar elev = ee.Image('MERIT/DEM/v1_0_3');  \n  \n// Define desired scale and crs for region reduction (for image display too).  \nvar proj = {  \n   scale: 90,  \n   crs: 'EPSG:5070'  \n};\n\n```\nThe count reducer will return how many pixel centers are overlapped by the polygon region, which would be the number of pixels included in any unweighted reducer statistic. You can also visualize which pixels will be included in the reduction by using the toCollection reducer on a latitude/longitude image and adding resulting coordinates as feature geometry. Be sure to specify CRS and scale for both the region reducers and the reprojected layer added to the map (see bullet list below for more details).\n\n```js\n// A count reducer will return how many pixel centers are overlapped by the  \n// polygon region.  \nvar count = elev.select(0).reduceRegion({  \n   reducer: ee.Reducer.count(),  \n   geometry: geometry,  \n   scale: proj.scale,   crs: proj.crs  \n});  \nprint('n pixels in the reduction', count.get('dem'));  \n  \n// Make a feature collection of pixel center points for those that are  \n// included in the reduction.  \nvar pixels = ee.Image.pixelLonLat().reduceRegion({  \n   reducer: ee.Reducer.toCollection(['lon', 'lat']),  \n   geometry: geometry,  \n    scale: proj.scale,   crs: proj.crs  \n});  \nvar pixelsFc = ee.FeatureCollection(pixels.get('features')).map(   function(f) {       return f.setGeometry(ee.Geometry.Point([f.get('lon'), f  \n           .get('lat')  \n       ]));  \n   });  \n  \n// Display layers on the map.  \nMap.centerObject(geometry, 18);  \nMap.addLayer(  \n   elev.reproject({  \n       crs: proj.crs,  \n       scale: proj.scale   }),  \n   {  \n       min: 2500,  \n       max: 3000,  \n       palette: ['blue', 'white', 'red']  \n   }, 'Image');  \nMap.addLayer(geometry, {  \n   color: 'white'}, 'Geometry');  \nMap.addLayer(pixelsFc, {  \n   color: 'purple'}, 'Pixels in reduction');\n\n```\n\n![Fig. F5.2.3 Identifying pixels used in zonal statistics. By mapping the image and vector together, you can see which pixels are included in the unweighted statistic. For this example, three pixels would be included in the statistic because the polygon covers the center point of three pixels.](F5/image44.png)\n\n\n:::{.callout-note}\nCode Checkpoint F52c. The book’s repository contains a script that shows what your code should look like at this point.\n:::\nFinally, here are some notes on CRS and scale:\n\n*   Earth Engine runs reduceRegion using the projection of the image's first band if the CRS is unspecified in the function. For imagery spanning multiple UTM zones, for example, this would lead to different origins. For some functions Earth Engine uses the default EPSG:4326. Therefore, when the opportunity is presented, such as by the reduceRegion function, it is important to specify the scale and CRS explicitly.\n*   The Map default CRS is EPSG:3857. When looking closely at pixels on the map, the data layer scale and CRS should also be set explicitly. Note that zooming out after setting a relatively small scale when reprojecting may result in memory and/or timeout errors because optimized pyramid layers for each zoom level will not be used.\n*   Specifying the CRS and scale in both the reduceRegion and addLayer functions allows the map visualization to align with the information printed in the Console.\n*   The Earth Engine default, WGS 84 lat long (EPSG:4326), is a generic CRS that works worldwide. The code above reprojects to EPSG:5070, North American Equal Albers, which is a CRS that preserves area for North American locations. Use the CRS that is best for your use case when adapting this to your own project, or maintain (and specify) the CRS of the image using, for example, crs: 'img.projection().crs()'.\n\n### Conclusion {.unnumbered}\n\nIn this chapter, you used functions containing optional parameters to extract raster values for collocated points. You also learned how to buffer points, and apply weighted and unweighted reducers to get different types of zonal statistics. These functions were applied to three examples that differed by raster dataset, reducer, spatial resolution, and scale. Lastly, you covered related topics like weighting of reducers and buffer visualization. Now you’re ready to apply these ideas to your own work!\n\n### References {.unnumbered}\n\nCansler CA, McKenzie D (2012) How robust are burn severity indices when applied in a new region? Evaluation of alternate field-based and remote-sensing methods. Remote Sens 4:456–483. https://doi.org/10.3390/rs4020456\n\nMiller JD, Thode AE (2007) Quantifying burn severity in a heterogeneous landscape with a relative version of the delta Normalized Burn Ratio (dNBR). Remote Sens Environ 109:66–80. https://doi.org/10.1016/j.rse.2006.12.006\n\n\n\n## Advanced Vector Operations\n\n:::{.callout-tip}\n## Chapter Information\n\n#### Author {.unlisted .unnumbered}\n\n \n\nUjaval Gandhi\n\n\n\n#### Overview {.unlisted .unnumbered}\n \n\nThis chapter covers advanced techniques for visualizing and analyzing vector data in Earth Engine. There are many ways to visualize feature collections, and you will learn how to pick the appropriate method to create visualizations, such as a choropleth map. We will also cover geoprocessing techniques involving multiple vector layers, such as selecting features in one layer by their proximity to features in another layer and performing spatial joins.\n\n#### Learning Outcomes {.unlisted .unnumbered}\n\n\n*   Visualizing any vector dataset and creating a thematic map.\n*   Understanding joins in Earth Engine.\n*   Carrying out geoprocessing tasks with vector layers in Earth Engine.\n\n#### Assumes you know how to:{.unlisted .unnumbered}\n\n*   Filter a FeatureCollection to obtain a subset (Chap. F5.0, Chap. F5.1).\n*   Write a function and map it over a FeatureCollection (Chap. F5.1, Chap. F5.2).\n:::\n\n### Visualizing Feature Collections\n\nThere is a distinct difference between how rasters and vectors are visualized. While images are typically visualized based on pixel values, vector layers use feature properties (i.e., attributes) to create a visualization. Vector layers are rendered on the Map by assigning a value to the red, green, and blue channels for each pixel on the screen based on the geometry and attributes of the features. The functions used for vector data visualization in Earth Engine are listed below in increasing order of complexity.\n\n*   Map.addLayer: As with raster layers, you can add a FeatureCollection to the Map by specifying visualization parameters. This method supports only one visualization parameter: color. All features are rendered with the specified color.\n*   draw: This function supports the parameters pointRadius and strokeWidth in addition to color. It renders all features of the layer with the specified parameters.\n*   paint: This is a more powerful function that can render each feature with a different color and width based on the values in the specified property.\n*   style: This is the most versatile function. It can apply a different style to each feature, including color, pointSize, pointShape, width, fillColor, and lineType.\n\nIn the exercises below, we will learn how to use each of these functions and see how they can generate different types of maps.\n\n#### Creating a Choropleth Map\n\nWe will use the TIGER: US Census Blocks layer, which stores census block boundaries and their characteristics within the United States, along with the San Francisco neighborhoods layer from Chap. F5.0 to create a population density map for the city of San Francisco.\n\nWe start by loading the census blocks and San Francisco neighborhoods layers. We use ee.Filter.bounds to filter the census blocks layer to the San Francisco boundary.\n```js\nvar blocks = ee.FeatureCollection('TIGER/2010/Blocks');  \nvar roads = ee.FeatureCollection('TIGER/2016/Roads');  \nvar sfNeighborhoods = ee.FeatureCollection(   'projects/gee-book/assets/F5-0/SFneighborhoods');  \n  \nvar geometry = sfNeighborhoods.geometry();  \nMap.centerObject(geometry);  \n  \n// Filter blocks to the San Francisco boundary.  \nvar sfBlocks = blocks.filter(ee.Filter.bounds(geometry));\n\n```\nThe simplest way to visualize this layer is to use Map.addLayer (Fig. F5.3.1). We can specify a color value in the visParams parameter of the function. Each census block polygon will be rendered with stroke and fill of the specified color. The fill color is the same as the stroke color but has a 66% opacity.\n\n```js\n// Visualize with a single color.  \nMap.addLayer(sfBlocks, {  \n   color: '#de2d26'}, 'Census Blocks (single color)');\n\n```\n![Fig. F5.3.1 San Francisco census blocks](F5/image34.png)\n\n\nThe census blocks table has a property named 'pop10' containing the population totals as of the 2010 census. We can use this to create a choropleth map showing population density. We first need to compute the population density for each feature and add it as a property. To add a new property to each feature, we can map a function over the FeatureCollection and calculate the new property called 'pop_density'. Earth Engine provides the area function, which can calculate the area of a feature in square meters. We convert it to square miles and calculate the population density per square mile.\n\n```js\n// Add a pop_density column.  \nvar sfBlocks = sfBlocks.map(function(f) {   // Get the polygon area in square miles.   \nvar area_sqmi = f.area().divide(2.59e6);   \nvar population = f.get('pop10');   // Calculate population density.   \nvar density = ee.Number(population).divide(area_sqmi);   \nreturn f.set({       \n    'area_sqmi': area_sqmi,       \n    'pop_density': density  \n   });  \n});\n\n```\nNow we can use the paint function to create an image from this FeatureCollection using the pop_density property. The paint function needs an empty image that needs to be cast to the appropriate data type. Let’s use the aggregate_stats function to calculate basic statistics for the given column of a FeatureCollection.\n\n```js\n// Calculate the statistics of the newly computed column.  \nvar stats = sfBlocks.aggregate_stats('pop_density');  \nprint(stats);\n\n```\nYou will see that the population density values have a large range. We also have values that are greater than 100,000, so we need to make sure we select a data type that can store values of this size. We create an empty image and cast it to int32, which is able to hold large integer values.\n\nThe result is an image with pixel values representing the population density of the polygons. We can now use the standard image visualization method to add this layer to the Map (Fig. F5.3.2). Then, we need to determine minimum and maximum values for the visualization parameters.A reliable technique to produce a good visualization is to find minimum and maximum values that are within one standard deviation. From the statistics that we calculated earlier, we can estimate good minimum and maximum values to be 0 and 50000, respectively.\n\n```js\nvar palette = ['fee5d9', 'fcae91', 'fb6a4a', 'de2d26', 'a50f15'];  \nvar visParams = {  \n   min: 0,  \n   max: 50000,  \n   palette: palette  \n};  \nMap.addLayer(sfBlocksPaint.clip(geometry), visParams,   'Population Density');\n```\n\n![Fig. F5.3.2 San Francisco population density](F5/image41.png)\n\n\n#### Creating a Categorical Map\n\nContinuing the exploration of styling methods, we will now learn about draw and style. These are the preferred methods of styling for points and line layers. Let’s see how we can visualize the TIGER: US Census Roads layer to create a categorical map.\n\nWe start by filtering the roads layer to the San Francisco boundary and using Map.addLayer to visualize it.\n\n```js\n// Filter roads to San Francisco boundary.  \nvar sfRoads = roads.filter(ee.Filter.bounds(geometry));  \n  \nMap.addLayer(sfRoads, {  \n   color: 'blue'}, 'Roads (default)');\n\n```\nThe default visualization renders each line using a width of 2 pixels. The draw function provides a way to specify a different line width. Let’s use it to render the layer with the same color as before but with a line width of 1 pixel (Fig. F5.3.3).\n\n```js\n// Visualize with draw().  \nvar sfRoadsDraw = sfRoads.draw({  \n   color: 'blue',  \n   strokeWidth: 1  \n});  \nMap.addLayer(sfRoadsDraw, {}, 'Roads (Draw)');\n\n```\n![](F5/image28.png)\n\n![Fig. F5.3.3 San Francisco roads rendered with a line width of 2 pixels (left) and and a line width of 1 pixel (right)](F5/image31.png)\n\n\nThe road layer has a column called “MTFCC” (standing for the MAF/TIGER Feature Class Code). This contains the road priority codes, representing the various types of roads, such as primary and secondary. We can use this information to render each road segment according to its priority. The draw function doesn’t allow us to specify different styles for each feature. Instead, we need to make use of the style function.\n\nThe column contains string values indicating different road types as indicated in at the MAF/TIGER Feature Class Code Definitions page on the US Census Bureau website. Let’s say we want to create a map with rules based on the MTFCC values.\n\nLet’s define a dictionary containing the styling information.\n\n```js\nvar styles = ee.Dictionary({   'S1100': {       'color': 'blue',       'width': 3   },   'S1200': {       'color': 'green',       'width': 2   },   'S1400': {       'color': 'orange',       'width': 1   }  \n});var defaultStyle = {  \n   color: 'gray',   'width': 1  \n};\n```\n\nThe style function needs a property in the FeatureCollection that contains a dictionary with the style parameters. This allows you to specify a different style for each feature. To create a new property, we map a function over the FeatureCollection and assign an appropriate style dictionary to a new property named 'style'. Note the use of the get function, which allows us to fetch the value for a key in the dictionary. It also takes a default value in case the specified key does not exist. We make use of this to assign different styles to the three road classes specified in Table 5.3.2 and a default style to all others.\n\n```js\nvar sfRoads = sfRoads.map(function(f) {   var classcode = f.get('mtfcc');   var style = styles.get(classcode, defaultStyle);   return f.set('style', style);  \n});\n\n```\n\nOur collection is now ready to be styled. We call the style function to specify the property that contains the dictionary of style parameters. The output of the style function is an RGB image rendered from the FeatureCollection (Fig. F5.3.4).\n\n```js\nvar sfRoadsStyle = sfRoads.style({  \n   styleProperty: 'style'  \n});  \nMap.addLayer(sfRoadsStyle.clip(geometry), {}, 'Roads (Style)');\n```\n\n![Fig. F5.3.4 San Francisco roads rendered according to road priority](F5/image46.png)\n\n\n:::{.callout-note}\nCode Checkpoint F53a. The book’s repository contains a script that shows what your code should look like at this point.\n:::\nSave your script for your own future use, as outlined in Chap. F1.0. Then, refresh the Code Editor to begin with a new script for the next section.\n\n### Joins with Feature Collections\n\nEarth Engine was designed as a platform for processing raster data, and that is where it shines. Over the years, it has acquired advanced vector data processing capabilities, and users are now able to carry out complex geoprocessing tasks within Earth Engine. You can leverage the distributed processing power of Earth Engine to process large vector layers in parallel.\n\nThis section shows how you can do spatial queries and spatial joins using multiple large feature collections. This requires the use of joins. As described for Image Collections in Chap. F4.9, a join allows you to match every item in a collection with items in another collection based on certain conditions. While you can achieve similar results using map and filter, joins perform better and give you more flexibility. We need to define the following items to perform a join on two collections.\n\n1.  Filter: A filter defines the condition used to select the features from the two collections. There is a suite of filters in the ee.Filters module that work on two collections, such as ee.Filter.equals and ee.Filter.withinDistance.\n2.  Join type: While the filter determines which features will be joined, the join type determines how they will be joined. There are many join types, including simple join, inner join, and save-all join.\n\nJoins are one of the harder skills to master, but doing so will help you perform many complex analysis tasks within Earth Engine. We will go through practical examples that will help you understand these concepts and the workflow better.\n\n#### Selecting by Location\n\nIn this section, we will learn how to select features from one layer that are within a specified distance from features in another layer. We will continue to work with the San Francisco census blocks and roads datasets from the previous section. We will implement a join to select all blocks in San Francisco that are within 1 km of an interstate highway.\n\nWe start by loading the census blocks and roads collections and filtering the roads layer to the San Francisco boundary.\n\n```js\nvar blocks = ee.FeatureCollection('TIGER/2010/Blocks');  \nvar roads = ee.FeatureCollection('TIGER/2016/Roads');  \nvar sfNeighborhoods = ee.FeatureCollection(   'projects/gee-book/assets/F5-0/SFneighborhoods');  \n  \nvar geometry = sfNeighborhoods.geometry();  \nMap.centerObject(geometry);  \n  \n// Filter blocks and roads to San Francisco boundary.  \nvar sfBlocks = blocks.filter(ee.Filter.bounds(geometry));  \nvar sfRoads = roads.filter(ee.Filter.bounds(geometry));\n\n```\nAs we want to select all blocks within 1 km of an interstate highway, we first filter the sfRoads collection to select all segments with the rttyp property value of I.\n\n```js\nvar interstateRoads = sfRoads.filter(ee.Filter.eq('rttyp', 'I'));\n```\n\nWe use the draw function to visualize the sfBlocks and interstateRoads layers (Fig. F5.3.5).\n\n```js\nvar sfBlocksDrawn = sfBlocks.draw({  \n       color: 'gray',  \n       strokeWidth: 1   })  \n   .clip(geometry);  \nMap.addLayer(sfBlocksDrawn, {}, 'All Blocks');  \nvar interstateRoadsDrawn = interstateRoads.draw({  \n       color: 'blue',  \n       strokeWidth: 3   })  \n   .clip(geometry);  \nMap.addLayer(interstateRoadsDrawn, {}, 'Interstate Roads');\n```\n\n![Fig. F5.3.5 San Francisco blocks and interstate highways](F5/image2.png)\n\n\nLet’s define a join that will select all the features from the sfBlocks layer that are within 1 km of any feature from the interstateRoads layer. We start by defining a filter using the ee.Filter.withinDistance filter. We want to compare the geometries of features in both layers, so we use a special property called '.geo' to compare the collections. By default, the filter will work with exact distances between the geometries. If your analysis does not require a very precise tolerance of spatial uncertainty, specifying a small non-zero maxError distance value will help speed up the spatial operations. A larger tolerance also helps when testing or debugging code so you can get the result quickly instead of waiting longer for a more precise output.\n\n```js\nvar joinFilter = ee.Filter.withinDistance({  \n   distance: 1000,  \n   leftField: '.geo',  \n   rightField: '.geo',  \n   maxError: 10  \n});\n```\n\nWe will use a simple join as we just want features from the first (primary) collection that match the features from the other (secondary) collection.\n\n```js\nvar closeBlocks = ee.Join.simple().apply({  \n   primary: sfBlocks,  \n   secondary: interstateRoads,  \n   condition: joinFilter  \n});\n```\n\nWe can visualize the results in a different color and verify that the join worked as expected (Fig. F5.3.6).\n\n```js\nvar closeBlocksDrawn = closeBlocks.draw({  \n       color: 'orange',  \n       strokeWidth: 1   })  \n   .clip(geometry);  \nMap.addLayer(closeBlocksDrawn, {}, 'Blocks within 1km');\n```\n\n![Fig. F5.3.6 Selected blocks within 1 km of an interstate highway](F5/image40.png)\n\n\n#### Spatial Joins\n\nA spatial join allows you to query two collections based on the spatial relationship. We will now implement a spatial join to count points in polygons. We will work with a dataset of tree locations in San Francisco and polygons of neighborhoods to produce a CSV file with the total number of trees in each neighborhood.\n\nThe San Francisco Open Data Portal maintains a street tree map dataset that has a list of street trees with their latitude and longitude. We will also use the San Francisco neighborhood dataset from the same portal. We downloaded, processed, and uploaded these layers as Earth Engine assets for use in this exercise. We start by loading both layers and using the paint and style functions, covered in Sect. 1, to visualize them (Fig. F5.3.7).\n\nvar sfNeighborhoods = ee.FeatureCollection(   'projects/gee-book/assets/F5-0/SFneighborhoods');  \nvar sfTrees = ee.FeatureCollection(   'projects/gee-book/assets/F5-3/SFTrees');  \n  \n```js\n// Use paint() to visualize the polygons with only outline  \nvar sfNeighborhoodsOutline = ee.Image().byte().paint({  \n   featureCollection: sfNeighborhoods,  \n   color: 1,  \n   width: 3  \n});  \nMap.addLayer(sfNeighborhoodsOutline, {  \n       palette: ['blue']  \n   },   'SF Neighborhoods');  \n  \n// Use style() to visualize the points  \nvar sfTreesStyled = sfTrees.style({  \n   color: 'green',  \n   pointSize: 2,  \n   pointShape: 'triangle',  \n   width: 2  \n});  \nMap.addLayer(sfTreesStyled, {}, 'SF Trees');\n\n```\n![Fig. F5.3.7 San Francisco neighborhoods and trees](F5/image35.png)\n\n\nTo find the tree points in each neighborhood polygon, we will use an ee.Filter.intersects filter.\n\n```js\nvar intersectFilter = ee.Filter.intersects({  \n   leftField: '.geo',  \n   rightField: '.geo',  \n   maxError: 10  \n});\n```\n\nWe need a join that can give us a list of all tree features that intersect each neighborhood polygon, so we need to use a saving join. A saving join will find all the features from the secondary collection that match the filter and store them in a property in the primary collection. Once you apply this join, you will get a version of the primary collection with an additional property that has the matching features from the secondary collection. Here we use the ee.Join.saveAll join, since we want to store all matching features. We specify the matchesKey property that will be added to each feature with the results.\n\n```js\nvar saveAllJoin = ee.Join.saveAll({  \n   matchesKey: 'trees',  \n});\n```\n\nLet’s apply the join and print the first feature of the resulting collection to verify (Fig. F5.3.8).\n\n```js\nvar joined = saveAllJoin  \n   .apply(sfNeighborhoods, sfTrees, intersectFilter);  \nprint(joined.first());\n```\n\n\n![Fig. F5.3.8 Result of the save-all join](F5/image1.png)\n\n\nYou will see that each feature of the sfNeighborhoods collection now has an additional property called trees. This contains all the features from the sfTrees collection that were matched using the intersectFilter. We can now map a function over the results and post-process the collection. As our analysis requires the computation of the total number of trees in each neighborhood, we extract the matching features and use the size function to get the count (Fig. F5.3.9).\n\n```js\n// Calculate total number of trees within each feature.  \nvar sfNeighborhoods = joined.map(function(f) {   var treesWithin = ee.List(f.get('trees'));   var totalTrees = ee.FeatureCollection(treesWithin).size();   return f.set('total_trees', totalTrees);  \n});  \n  \nprint(sfNeighborhoods.first());\n\n```\n![Fig. F5.3.9 Final FeatureCollection with the new property](F5/image18.png)\n\n\nThe results now have a property called total_trees containing the count of intersecting trees in each neighborhood polygon.\n\nThe final step in the analysis is to export the results as a CSV file using the Export.table.toDrive function. Note that as described in detail in F6.2, you should output only the columns you need to the CSV file. Suppose we do not need all the properties to appear in the output; imagine that wedo not need the trees property, for example, in the output. In that case, we can create only those columns we want in the manner below, by specifying the other selectors parameters with the list of properties to export.\n\n```js\n// Export the results as a CSV.  \nExport.table.toDrive({  \n   collection: sfNeighborhoods,  \n   description: 'SF_Neighborhood_Tree_Count',  \n   folder: 'earthengine',  \n   fileNamePrefix: 'tree_count',  \n   fileFormat: 'CSV',  \n   selectors: ['nhood', 'total_trees']  \n});\n\n```\nThe final result is a CSV file with the neighborhood names and total numbers of trees counted using the join (Fig. F5.3.10).\n\n![Fig. F5.3.10 Exported CSV file with tree counts for San Francisco neighborhoods](F5/image3.png)\n\n\n:::{.callout-note}\nCode Checkpoint F53b. The book’s repository contains a script that shows what your code should look like at this point.\n:::\n### Conclusion {.unnumbered}\n\nThis chapter covered visualization and analysis using vector data in Earth Engine. You should now understand different functions for FeatureCollection visualization and be able to create thematic maps with vector layers. You also learned techniques for doing spatial queries and spatial joins within Earth Engine. Earth Engine is capable of handling large feature collections and can be effectively used for many spatial analysis tasks.","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"monokai.theme","output-file":"F5.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","bibliography":["references.bib"],"theme":{"dark":"darkly","light":"cosmo"},"code-copy":true,"linkcolor":"#34a832"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"F5.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt","header-includes":["\\makeatletter","\\@addtoreset{chapter}{part}","\\makeatother"]},"extensions":{"book":{"selfContainedOutput":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","output-file":"F5.epub"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"bibliography":["references.bib"],"cover-image":"cover.png"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf","epub"]}